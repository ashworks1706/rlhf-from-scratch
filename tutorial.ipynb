{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffd4d83",
   "metadata": {},
   "source": [
    "# RLHF From Scratch on LLMs\n",
    "\n",
    "In this notebook, I will start with history of RLHF, the importance of RLHF in LLMs, then go into the architectures TRPO, PPO, GRPO and DPO. Each of the technique's explanation will have the math, code and explanations on how it's done, finally in the end we'll experiment these techniques on one of prebuilt LLMs (the llms are not built from scratch, since i've already done that in my [llm-from-scratch repository](https://github.com/ashworks1706/llm-from-scratch)). If you're new to RL, check out [dqn-from-scratch](https://github.com/ashworks1706/dqn-from-scratch) where i've explained RL indepth from the core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b667bd4",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a4fea",
   "metadata": {},
   "source": [
    "## Brief History of RLHF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d9008",
   "metadata": {},
   "source": [
    "\n",
    "RLHF emerged around 2017-2018 when researchers at OpenAI developed techniques to incorporate human preferences into reinforcement learning systems. The seminal paper \"Deep Reinforcement Learning from Human Preferences\" by Christiano et al. (2017) introduced the core concept of using human comparisons between pairs of outputs to train a reward model that could guide RL agents toward preferred behaviors. While initially applied to simpler tasks and robotics, the technique remained relatively specialized until recent years. The technique gained mainstream attention in 2022 when OpenAI used it to create ChatGPT from GPT-3.5, dramatically improving output quality by aligning the model with human preferences. This breakthrough demonstrated RLHF's potential to transform raw language model capabilities into systems that better align with human intent and values. Since then, RLHF has become a standard component in developing advanced language models like GPT-4, Claude, and Llama 2, with each iteration refining the techniques to achieve better alignment.\n",
    "\n",
    "#### Why RLHF Matters for LLMs\n",
    "\n",
    "<img src=\"assets/rlhf-vs-finetune.png\" width=300>\n",
    "\n",
    "Large Language Models trained solely on next-token prediction are just models with knowledge, they don't know how to answer properly. You have model trained on shakespear work, great! but how do you make it to answer questions in the way we want (in the way humans talk)? These models optimize for predicting the next token based on training data distribution, which doesn't necessarily correlate with producing helpful, harmless, or honest responses. Traditional LLMs may generate toxic, harmful, or misleading content because they're simply trying to produce statistically likely continuations without understanding human values or preferences. They lack an inherent mechanism to distinguish between content that is statistically probable and content that is actually desirable according to human standards. RLHF addresses these issues by creating a feedback loop where human preferences explicitly guide the model's learning process, steering it toward outputs that humans find more helpful, honest, and aligned with their intent. This alignment process transforms a powerful but directionless prediction engine into a system that can better understand and respect nuanced human values and follow complex instructions in ways that maximize utility for users.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5616c",
   "metadata": {},
   "source": [
    "## The Birds Eye View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4df866",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow.png\" >\n",
    "\n",
    "\n",
    "Before delving into the complexities of RLHF, it's essential to understand the overal workflow of what actually happens in a typical RLHF based projects. When a large language model is initially trained on vast internet text corpora, it develops remarkable capabilities to predict text and acquire factual knowledge, but this training alone doesn't prepare it to be helpful in specific contexts or respond appropriately to human instructions. Consider a language model trained on extensive educational materials, including university canvas modules, academic papers, and textbooks. This model would possess substantial knowledge about various academic subjects, pedagogical approaches, and educational concepts. However, if asked to \"Explain the concept of photosynthesis to a 10-year-old,\" it might produce a technically accurate but overly complex explanation filled with academic jargon that would confuse rather than enlighten a young student. The model hasn't been optimized to serve as an effective tutor - it simply predicts what text might follow in educational materials. \n",
    "\n",
    "The Supervised Fine-Tuning stage addresses this gap by training the model on demonstrations of desired behavior. For our hypothetical educational assistant, SFT would involve collecting thousands of examples showing how skilled human tutors respond to student questions: simplifying complex concepts, using age-appropriate language, providing relevant examples, checking for understanding, and offering encouragement. These demonstrations are formatted as input-output pairs (prompt and ideal response), and the model is fine-tuned to minimize the difference between its outputs and these human-generated \"gold standard\" responses. Through this process, the model learns the patterns that characterize helpful tutoring: breaking down complex concepts into simpler components, using analogies relevant to younger audiences, avoiding unnecessary technical terms, and adopting a supportive tone. After SFT, when asked to explain photosynthesis to a 10-year-old, the model is much more likely to respond with an explanation involving plants \"eating sunlight\" and \"breathing in carbon dioxide to make food,\" rather than discussing electron transport chains and ATP synthesis. The model hasn't gained new knowledge, but it has learned a new way to present its existing knowledge that better aligns with the specific goal of being an effective tutor for younger students. However, SFT alone has significant limitations. First, it can only learn from the specific examples it's shown, leaving gaps in how to handle the infinite variety of possible user requests. Second, the demonstrations might not cover the full range of desirable behaviors or edge cases where special handling is needed. Third, the quality of the SFT model depends entirely on the quality and consistency of the demonstration data. Finally, there's no mechanism for the model to understand why certain responses are better than others - it simply learns to mimic patterns without a deeper understanding of the preferences that make one response superior. These limitations are precisely what RLHF is designed to address in the subsequent stages of the alignment process.\n",
    "\n",
    "Following Supervised Fine-Tuning, the RLHF workflow progresses to Human Preference Collection - a crucial stage that fundamentally changes how model improvement occurs. In this phase, rather than providing gold-standard demonstrations, human evaluators compare and rank different model responses to the same prompt. For our educational assistant, this might involve presenting evaluators with pairs of explanations for the same scientific concept and asking them which better achieves the goal of teaching a young student. One explanation might be more engaging and use more appropriate analogies, while another might be technically accurate but still too complex. By explicitly choosing the better response, humans provide preference signals that capture nuanced quality distinctions beyond what demonstration data alone can convey. These comparisons generate valuable datasets where each entry contains a prompt and two responses, with a label indicating which response humans preferred. The collection process typically gathers thousands or even millions of such comparative judgments, creating a rich dataset that embodies human preferences about what constitutes a high-quality response across diverse scenarios.\n",
    "\n",
    "The third stage, Reward Model Training, transforms these human preferences into a quantifiable reward function that can guide further optimization. This reward model takes a prompt and response as input and outputs a scalar score representing how well the response aligns with human preferences. Technically, it's trained to predict which of two responses humans would prefer by maximizing the likelihood of the observed preference data. For our educational tutor, the reward model learns to assign higher scores to explanations that successfully simplify complex concepts without sacrificing accuracy, use age-appropriate analogies, maintain an encouraging tone, and check for understanding. This model becomes a computational proxy for human judgment, capable of evaluating millions of potential responses far beyond what human evaluators could manually assess. The quality of this reward model is critical, as it effectively defines what \"good\" means for all subsequent optimization.\n",
    "\n",
    "With a trained reward model in place, the final stage applies Reinforcement Learning techniques to optimize the language model toward maximizing the predicted reward. The most common approach is Proximal Policy Optimization (PPO), which iteratively improves the model by adjusting its parameters to generate responses that receive higher reward scores. However, simply maximizing reward can lead to degenerate outputs that exploit loopholes in the reward model or diverge too far from natural language patterns. To prevent this, the optimization includes a \"KL divergence\" penalty that constrains how much the optimized model can deviate from the SFT model, preserving fluency and knowledge while improving alignment. For our educational tutor, this process might result in a model that maintains scientific accuracy while consistently finding creative, age-appropriate analogies and explanations across a much broader range of topics than were covered in the original demonstration data. The entire RLHF pipeline is often iterative, with new preference data collected from the improved model, leading to refined reward models and further optimization cycles. This continuous feedback loop progressively aligns the language model with human values and preferences, addressing the fundamental limitations of training on prediction alone or even on demonstration data without comparative preference signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df33e7",
   "metadata": {},
   "source": [
    "## 1. Getting a pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776fee7",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55f9c3",
   "metadata": {},
   "source": [
    "Now the first step is to have a fresh pretrained LLM right off the top. We'll be using huggingface library transformers library for our transformer components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers peft datasets tqdm wandb rouge-score\n",
    "# PEFT is a technique to fine tune LLMs without modifying all of their parameters. it's efficient for our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from library and setup the model class \n",
    " \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "class PretrainedLLM:\n",
    "    def __init__(self, model_name=\"facebook/opt-350m\", device=None):\n",
    "        \"\"\"\n",
    "        Initializing a (No SFT RLHF) pretrained language model for RLHF experiment\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier (default: OPT-350M, a relatively small but capable model)\n",
    "            device: Computing device (will auto-detect if None)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # this code is just for detecting if you have Nvidia CUDA driver or not\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Loading {model_name} on {self.device}...\")\n",
    "        \n",
    "        # Load model and tokenizer (for full guide on implementing llm from scratch check out https://github.com/ashworks1706/llm-from-scratch\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # distributed training for better GPU utilization\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded successfully with {sum(p.numel() for p in self.model.parameters())/1e6:.1f}M parameters\")\n",
    "        \n",
    "    def generate(self, prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate text from the model given a prompt (no RLHF)\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input text to generate from\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature (lower = more deterministic)\n",
    "            top_p: Nucleus sampling parameter (lower = more focused)\n",
    "            \n",
    "        Returns:\n",
    "            Generated text as string\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate with sampling, no fancy tuning required\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode and remove the prompt from the generated text\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_text = full_text[len(self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):]\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def save_checkpoint(self, path):\n",
    "        \"\"\"Save model checkpoint to the specified path\"\"\"\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "        \n",
    "    def load_adapter(self, adapter_path):\n",
    "        \"\"\"Load a PEFT adapter for efficient fine-tuning\"\"\"\n",
    "        self.model = PeftModel.from_pretrained(\n",
    "            self.model,\n",
    "            adapter_path,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        print(f\"Loaded adapter from {adapter_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with a small model for experimentation\n",
    "    llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = \"Explain quantum computing to a 10-year-old:\"\n",
    "    print(f\"(No SFT RLHF) Prompt: {prompt}\")\n",
    "    print(f\"(NO SFT RLHF) Response: {llm.generate(prompt, max_new_tokens=150)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e849",
   "metadata": {},
   "source": [
    "---\n",
    "(No SFT RLHF) *Prompt*: Explain quantum computing to a 10-year-old:\n",
    "\n",
    "(No SFT RLHF) *Response*: Quantum computing is a type of computation that harnesses the collective properties of quantum states, such as superposition, interference, and entanglement, to perform calculations. The basic unit of quantum computation is the quantum bit or qubit. Unlike classical bits, which can only be in states of 0 or 1, qubits can exist in a superposition of states, meaning they can represent both 0 and 1 simultaneously. This property allows quantum computers to process a vast number of possibilities simultaneously.\n",
    "\n",
    "The field emerged from quantum mechanics, which is a fundamental theory in physics that provides a description of the physical properties of nature at the atomic and subatomic scales. Quantum computing leverages quantum mechanical phenomena to perform operations on data. This can potentially solve certain problems much faster than classical computers.\n",
    "\n",
    "For instance, Shor's algorithm demonstrates that a quantum computer can factor large integers exponentially faster than the best known algorithm running on a classical computer. This has significant implications for cryptography systems that rely on the difficulty of factoring large numbers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650836aa",
   "metadata": {},
   "source": [
    "In this example, the user anticipates the model to give explanation releavant to a 10 year old. Instead, the model produces a very technical response with an academic tone without any examples. A better response could have been (with SFT):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948df58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "(No RLHF) *Prompt*: Explain quantum computing to a 10-year-old:\n",
    "\n",
    "(No RLHF) *Response*: Imagine your regular computer is like playing a board game where you can only be on one space at a time. You have to move step by step to get to the end.\n",
    "\n",
    "Quantum computers are like magical game pieces that can be in lots of different spaces at the same time! They use special bits called \"qubits\" instead of regular computer bits.\n",
    "\n",
    "Regular computer bits are like light switches - they can only be ON or OFF. But qubits are special because they can be ON and OFF at the same time, kind of like how a spinning coin is neither heads nor tails until it stops.\n",
    "\n",
    "This special power lets quantum computers solve some really tricky puzzles much faster than regular computers. Scientists are excited about them because they might help us discover new medicines, understand weather better, and solve problems that would take regular computers thousands of years!\n",
    "\n",
    "The quantum computers we have today are still small and make mistakes, but engineers are working hard to make them bigger and better.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36869ed0",
   "metadata": {},
   "source": [
    "Here as you can see that the SFT model has learned to use simple language and analogies, breakdown complex concepts, avoid technical jargon like superposition and slightly adatpive to the age level of the audience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241dde7a",
   "metadata": {},
   "source": [
    "This is our goal response with SFT and RLHF :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a685a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "(SFT RLHF) *Prompt*: Explain quantum computing to a 10-year-old:\n",
    "\n",
    "(SFT RLHF) *Response*: Hey there! Imagine you have a super cool magic coin. When you flip a normal coin, it lands on either heads or tails, right? But this magic coin can spin and be a little bit heads and a little bit tails at the same time while it's spinning!\n",
    "\n",
    "Quantum computers use special things called \"qubits\" that work like these magic coins. While normal computers use regular bits that can only be 0 OR 1 (like a coin that's only heads or tails), quantum computers use qubits that can be 0 AND 1 at the same time (like our spinning magic coin).\n",
    "\n",
    "Why is this so awesome? Well, imagine you lost your favorite toy somewhere in your house. A normal computer would have to look in each room one by one. But a quantum computer is like having a bunch of friends who can check all the rooms at the exact same time! That makes solving certain problems MUCH faster.\n",
    "\n",
    "Scientists are still learning how to build better quantum computers because they're tricky to make. The qubits are super sensitive - kind of like trying to keep a hundred coins spinning without letting any of them fall over!\n",
    "\n",
    "Does that make sense? What do you think would be the coolest thing to use a quantum computer for?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db3321",
   "metadata": {},
   "source": [
    "in this example, we can clearly see the difference, it's more conversational, user friendly and uses analogies to make explanation easier like a human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0a8fa",
   "metadata": {},
   "source": [
    "## 2. Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ba3d2",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow2.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69635b",
   "metadata": {},
   "source": [
    "### Understanding the difference between SFT and RLHF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126eac5b",
   "metadata": {},
   "source": [
    "But what if I just keep model with SFT and not RLHF? Or what if I just skip to RLHF instead of SFT?\n",
    "\n",
    "These are excellent questions that get to the heart of why the complete RLHF pipeline exists. Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) serve different but complementary roles in aligning language models with human expectations and preferences.\n",
    "\n",
    "If you only implement SFT without RLHF, you'll have a model that can follow basic patterns demonstrated in your training examples, but it will struggle to generalize beyond them. As we saw in our quantum computing example, SFT can teach a model to use simpler language and appropriate analogies, but it's limited by the specific demonstrations provided. The model learns to mimic patterns without developing a deeper understanding of why certain responses are better than others. When faced with novel queries or edge cases not covered in the training data, an SFT-only model often fails to maintain the same quality of responses. Additionally, SFT can only optimize for whatever patterns exist in your demonstration data - if that data contains subtle biases or inconsistencies, those will be faithfully reproduced by the model.\n",
    "\n",
    "Conversely, if you attempt to skip SFT and go directly to RLHF, you're likely to encounter significant challenges. RLHF works by refining an already somewhat aligned model through preference optimization. Starting with a raw pretrained model would make this process extremely inefficient and potentially unstable. The preference learning and reinforcement stages need a reasonable starting point where the model can already produce somewhat appropriate responses that humans can meaningfully compare and rank. Without SFT, the initial responses might be so far from helpful that the preference signals become too noisy or the optimization process becomes prohibitively difficult. It would be like trying to teach advanced painting techniques to someone who hasn't yet learned to hold a brush - the feedback would be overwhelming and difficult to incorporate.\n",
    "\n",
    "The full RLHF pipeline with SFT followed by preference learning and reinforcement creates a progressively refined alignment. SFT provides the foundation by teaching the model basic response patterns and formats through demonstration. RLHF then builds on this foundation by teaching the model to distinguish between good and better responses through comparative feedback, allowing it to generalize beyond specific examples to broader human preferences. As we observed in our examples, the SFT model improved basic comprehensibility and appropriateness, while the RLHF model further enhanced engagement, conversational tone, and subtle aspects of helpfulness that are difficult to capture through demonstrations alone. This complementary relationship explains why major AI systems like ChatGPT and Claude use both techniques in sequence rather than choosing one over the other. The complete alignment process transforms raw predictive power into carefully balanced helpful assistance that respects complex human values and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa793cf",
   "metadata": {},
   "source": [
    "### Components of SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b7d30",
   "metadata": {},
   "source": [
    "To perform Supervised Fine-Tuning (SFT) on our pretrained LLM, we need high-quality demonstration data consisting of prompt-response pairs showing the desired behavior, typically thousands of examples created by experts. We also need a data preprocessing pipeline to format this data consistently, including tokenization and special tokens to distinguish between prompts and responses. SFT requires careful configuration of hyperparameters like learning rate, batch size, and optimization methods, with techniques such as warmup and decay schedules for training stability. Rather than fine-tuning all parameters, we'll use PEFT methods like LoRA that add small trainable modules while keeping most of the model frozen, making training more efficient. We'll implement a training loop for forward passes, loss calculation, backpropagation, and parameter updates, along with evaluation metrics such as perplexity and ROUGE scores to assess performance. Finally, our existing PretrainedLLM class already supports checkpointing and adapter saving, which we'll use to periodically save the model state during training. This SFT process will transform our raw model into one that can follow instructions and communicate appropriately, serving as the foundation for subsequent RLHF stages.\n",
    "\n",
    "High-quality demonstration data: Thousands of prompt-response pairs created by experts\n",
    "\n",
    "Data preprocessing pipeline: Consistent formatting, tokenization, and special tokens\n",
    "\n",
    "Fine-tuning configuration: Learning rate, batch size, warmup/decay schedules\n",
    "\n",
    "PEFT implementation: Using LoRA to add trainable modules while freezing most parameters\n",
    "\n",
    "Training loop: Forward passes, loss calculation, backpropagation, parameter updates\n",
    "\n",
    "Evaluation metrics: Perplexity, ROUGE scores to assess performance\n",
    "\n",
    "Checkpointing: Saving model state during training using our existing functionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283179e",
   "metadata": {},
   "source": [
    "For dataset, we're using the Databricks Dolly-15k dataset, which is a high-quality instruction-following dataset specifically designed for fine-tuning language models. This dataset contains 15,000 human-generated prompt/response pairs across various instruction categories including creative writing, classification, information extraction, open QA, brainstorming, and summarization. \n",
    "\n",
    "The Dolly dataset was created by Databricks employees who manually wrote both the prompts and high-quality responses, making it particularly valuable for instruction-tuning. Unlike some other datasets which may be generated or filtered from existing sources, Dolly's samples are purpose-built for teaching models to follow instructions in a helpful manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50206b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SupervisedFineTuner:\n",
    "    def __init__(self, base_model, dataset_name=\"databricks/dolly-15k\", max_seq_length=512):\n",
    "        \"\"\"\n",
    "        Initializing SFT framework\n",
    "        \n",
    "        Args:\n",
    "            base_model: The PretrainedLLM instance to fine-tune\n",
    "            dataset_name: HuggingFace dataset identifier containing instruction/response pairs\n",
    "            max_seq_length: Maximum sequence length for inputs\n",
    "        \"\"\"\n",
    "        self.llm = base_model\n",
    "        self.tokenizer = base_model.tokenizer\n",
    "        self.model = base_model.model\n",
    "        self.device = base_model.device\n",
    "        self.max_seq_length = max_seq_length # max length of sequences that model will process\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        # If tokenizer doesn't have padding token, set it to eos token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "        print(f\"Loading dataset {dataset_name}...\")\n",
    "        self.raw_dataset = load_dataset(dataset_name)\n",
    "        print(f\"Dataset loaded with {len(self.raw_dataset['train'])} training examples\")\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Process the dataset into the format needed for instruction fine-tuning\"\"\"\n",
    "        \n",
    "        def format_instruction(example):\n",
    "            \"\"\"Format an example into a prompt-response pair with special tokens\"\"\"\n",
    "            # Different datasets have different column names, they might call different call different labels\n",
    "            if 'instruction' in example and 'response' in example:\n",
    "                prompt = example['instruction']\n",
    "                response = example['response']\n",
    "            elif 'prompt' in example and 'completion' in example:\n",
    "                prompt = example['prompt']\n",
    "                response = example['completion']\n",
    "            else:\n",
    "                # Fallback for other dataset formats\n",
    "                prompt = str(example['input']) if 'input' in example else \"\"\n",
    "                response = str(example['output']) if 'output' in example else \"\"\n",
    "            \n",
    "            # Format with special tokens\n",
    "            formatted_text = f\"User: {prompt.strip()}\\n\\nAssistant: {response.strip()}\"\n",
    "            return {\"formatted_text\": formatted_text}\n",
    "        \n",
    "        print(\"Formatting dataset...\")\n",
    "        self.processed_dataset = self.raw_dataset.map(format_instruction)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"Tokenize the examples and prepare for training\"\"\"\n",
    "            texts = examples[\"formatted_text\"]\n",
    "            \n",
    "            # Tokenize with padding and truncation\n",
    "            tokenized = self.tokenizer(\n",
    "                texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Create labels (for causal LM, labels are the same as input_ids)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "            \n",
    "            # Mask padding tokens in the labels to -100 so they're not included in loss\n",
    "            tokenized[\"labels\"][tokenized[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "            \n",
    "            return tokenized\n",
    "        \n",
    "        print(\"Tokenizing dataset...\")\n",
    "        self.tokenized_dataset = self.processed_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.processed_dataset[\"train\"].column_names\n",
    "        )\n",
    "        \n",
    "        return self.tokenized_dataset\n",
    "    \n",
    "    def setup_peft(self, r=16, lora_alpha=32, lora_dropout=0.05):\n",
    "        \"\"\"Set up Parameter-Efficient Fine-Tuning using LoRA\"\"\"\n",
    "        \n",
    "        print(\"Setting up LoRA for efficient fine-tuning...\")\n",
    "        # Configure LoRA\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=r,  # Rank of the update matrices\n",
    "            lora_alpha=lora_alpha,  # Scaling factor\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],  # Which modules to apply LoRA to\n",
    "            bias=\"none\",\n",
    "            inference_mode=False\n",
    "        )\n",
    "        \n",
    "        # Prepare model for training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        # Apply LoRA\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Display trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, output_dir=\"sft_model\", num_epochs=3, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train the model using the prepared dataset\"\"\"\n",
    "        \n",
    "        print(\"Setting up training arguments...\")\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,                           # Directory to save model checkpoints\n",
    "            num_train_epochs=num_epochs,                     # Number of times to iterate through the dataset\n",
    "            per_device_train_batch_size=batch_size,          # Batch size per GPU/CPU for training\n",
    "            gradient_accumulation_steps=4,                   # Number of updates steps to accumulate gradients for\n",
    "            warmup_ratio=0.1,                               # Percentage of steps for learning rate warmup\n",
    "            weight_decay=0.01,                              # L2 regularization weight\n",
    "            learning_rate=learning_rate,                     # Initial learning rate\n",
    "            logging_steps=10,                               # How often to log training metrics\n",
    "            save_steps=200,                                 # How often to save model checkpoints\n",
    "            save_total_limit=3,                             # Maximum number of checkpoints to keep\n",
    "            fp16=True if self.device == \"cuda\" else False,   # Whether to use 16-bit floating point precision\n",
    "            report_to=\"none\"                                # Disable external reporting services\n",
    "        )\n",
    "        \n",
    "        # Create data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        print(\"Creating trainer...\")\n",
    "        # Initialize the trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.tokenized_dataset[\"train\"],\n",
    "            eval_dataset=self.tokenized_dataset[\"test\"] if \"test\" in self.tokenized_dataset else None,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        # Finally Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the adapter\n",
    "        adapter_path = os.path.join(output_dir, \"adapter\")\n",
    "        self.model.save_pretrained(adapter_path) # save the model to the path\n",
    "        self.tokenizer.save_pretrained(adapter_path)\n",
    "        print(f\"Saved LoRA adapter to {adapter_path}\")\n",
    "        \n",
    "        return adapter_path\n",
    "    \n",
    "    def evaluate(self, evaluation_prompts):\n",
    "        \"\"\"Evaluate the model on a list of prompts\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        for prompt in evaluation_prompts:\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            \n",
    "            # Generate with the original model\n",
    "            base_response = self.llm.generate(prompt, max_new_tokens=200)\n",
    "            print(f\"Base model response: {base_response}\\n\")\n",
    "            \n",
    "            # Format prompt for the fine-tuned model\n",
    "            formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Generate with the fine-tuned model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=200,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                \n",
    "            # Decode and display\n",
    "            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            sft_response = full_text[len(formatted_prompt):]\n",
    "            print(f\"SFT model response: {sft_response}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c49e95",
   "metadata": {},
   "source": [
    "As we can see, we have first a base model (our pretrained LLM), a dataset name, and a maximum sequence length parameter that controls how much text the model processes at once. It handles tokenizer configuration by ensuring the padding token is properly set, which is crucial for consistent batch processing during training. Next, the prepare_data method loads the Dolly-15k dataset and transforms each example by extracting the prompt-response pairs and formatting them with special tokens that help the model distinguish between user input and expected output. The formatting includes adding \"User:\" and \"Assistant:\" prefixes that teach the model the proper conversation structure. After formatting, it tokenizes all examples, handling padding and truncation to ensure consistent lengths, and sets up special label handling where padding tokens are masked from loss calculations.\n",
    "\n",
    "The setup_peft method is particularly innovative, implementing Parameter-Efficient Fine-Tuning using Low-Rank Adaptation (LoRA). Rather than updating all model weights—which would be computationally expensive—LoRA adds small trainable matrices to key attention components while keeping most parameters frozen. The method configures LoRA with appropriate rank and scaling parameters, then applies it to the query and value projection matrices of the transformer architecture. This approach dramatically reduces the number of trainable parameters to often less than 1% of the total, making fine-tuning feasible on hardware. At the core of LoRA's efficiency is its mathematical insight about weight updates. It first decomposes weight updates into products of two smaller matrices by leveraging the observation that during fine-tuning, the weight updates often have a low \"intrinsic rank\" - meaning they can be approximated by low-rank matrices without significant loss of information. For example, in a transformer model where a weight matrix W might be `768×768` (containing `589,824` parameters), LoRA replaces the full update with two matrices B `(768×16)` and A `(16×768)`, requiring only `24,576` parameters - a 96% reduction. These matrices are initialized with careful scaling: B starts with random Gaussian values while A begins at zero, ensuring training begins from the original model's behavior. The implementation in the code uses `r=16` for the rank hyperparameter, which determines this compression ratio. The `lora_alpha=32` parameter controls scaling during inference, effectively determining how strongly the adaptation affects the original weights. The `target_modules=[\"q_proj\", \"v_proj\"]` parameter specifically targets the query and value projection matrices in the attention mechanism, which are particularly influential for language understanding and generation while leaving other components untouched.\n",
    "\n",
    "The train method handles the actual training process by configuring optimization parameters including learning rate, batch size, and gradient accumulation steps. It sets up a training pipeline with appropriate arguments for supervised learning, including warmup schedules and weight decay for regularization. After training completes, it saves just the LoRA adapter rather than the full model, making the fine-tuned version extremely portable at just a fraction of the full model size. Finally, the evaluation method provides a convenient way to compare the base model against the fine-tuned version using the same prompts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Initialize base model\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Create SFT trainer\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    \n",
    "    # Prepare data\n",
    "    processed_data = sft.prepare_data()\n",
    "    \n",
    "    # Setup PEFT\n",
    "    peft_model = sft.setup_peft()\n",
    "    \n",
    "    # Train the model\n",
    "    adapter_path = sft.train(output_dir=\"sft_model\", num_epochs=1)  # Reduced for demo\n",
    "    \n",
    "    # Load the adapter into the base model\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    # Test the model\n",
    "    evaluation_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old:\",\n",
    "        \"Write a short story about a robot learning to feel emotions:\",\n",
    "        \"How do I bake a chocolate cake?\"\n",
    "    ]\n",
    "    \n",
    "    sft.evaluate(evaluation_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23542e",
   "metadata": {},
   "source": [
    "EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ea9ed",
   "metadata": {},
   "source": [
    "and there we go! we have a successfully supervised fine tuned llm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a89dba",
   "metadata": {},
   "source": [
    "## 3. Reinforcement Learning with Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dc63b",
   "metadata": {},
   "source": [
    "#### Understand the relation between LLMs and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3fd7b",
   "metadata": {},
   "source": [
    "Before diving into the human feedback mechanisms, it's crucial to understand how Reinforcement Learning fundamentals apply to language models. This connection isn't immediately obvious, as LLMs seem quite different from traditional RL scenarios like game-playing agents or robotic control systems.\n",
    "\n",
    "\n",
    "Reinforcement Learning is a machine learning paradigm where an **agent** learns to make optimal decisions by interacting with an **environment** to maximize cumulative rewards over time. The mathematical foundation rests on the Markov Decision Process (MDP), formally defined as a tuple $(S, A, P, R, \\gamma)$ where:\n",
    "\n",
    "**Core RL Components:**\n",
    "- **State Space (S)**: The set of all possible situations the agent can observe\n",
    "- **Action Space (A)**: The set of all possible decisions the agent can make  \n",
    "- **Transition Probabilities (P)**: $P(s'|s,a)$ - probability of reaching state $s'$ from state $s$ taking action $a$\n",
    "- **Reward Function (R)**: $R(s,a,s')$ - immediate feedback signal for taking action $a$ in state $s$\n",
    "- **Policy (π)**: $\\pi(a|s)$ - strategy mapping states to action probabilities\n",
    "- **Value Function (V)**: $V^\\pi(s) = \\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s]$ - expected cumulative reward\n",
    "- **Discount Factor (γ)**: Controls the importance of future vs. immediate rewards\n",
    "\n",
    "Let's return to our quantum computing explanation scenario to see how these RL concepts map to language models. Imagine our model is tasked with explaining quantum computing to a 10-year-old:\n",
    "\n",
    "**Traditional RL Scenario**: A robot learning to navigate a maze\n",
    "- **Environment**: Physical maze with walls and pathways\n",
    "- **State**: Robot's current position and orientation\n",
    "- **Actions**: Move forward, turn left, turn right, etc.\n",
    "- **Reward**: +10 for reaching the goal, -1 for hitting walls\n",
    "- **Policy**: Strategy for choosing movements based on current position\n",
    "\n",
    "**LLM RL Scenario**: Model explaining quantum computing to a child\n",
    "- **Environment**: The conversational context and task requirements\n",
    "- **State**: Current prompt + all previously generated tokens\n",
    "- **Actions**: Choosing the next token from vocabulary (~50,000 possibilities)\n",
    "- **Reward**: Human preference scores for the complete explanation\n",
    "- **Policy**: Strategy for choosing tokens to maximize helpfulness\n",
    "\n",
    "At each generation step, the model is in a specific state (the prompt plus all previously generated tokens) and must choose an action (the next token). For example:\n",
    "\n",
    "- **State 1**: \"Explain quantum computing to a 10-year-old:\"\n",
    "- **Action 1**: Choose token \"Hey\" (starting conversationally)\n",
    "- **State 2**: \"Explain quantum computing to a 10-year-old: Hey\"\n",
    "- **Action 2**: Choose token \"there!\" (continuing the friendly tone)\n",
    "- **State 3**: \"Explain quantum computing to a 10-year-old: Hey there!\"\n",
    "- **Action 3**: Choose token \"Imagine\" (starting an analogy)\n",
    "\n",
    "And so forth, until the model generates an end-of-sequence token, completing the trajectory.\n",
    "\n",
    "Let's formalize this mapping with concrete mathematical examples:\n",
    "\n",
    "**State Representation in LLMs:**\n",
    "In traditional RL, a state might be discrete coordinates: $s = (x, y, \\theta)$\n",
    "In LLMs, the state is the sequence of tokens generated so far:\n",
    "$$s_t = (w_1, w_2, \\ldots, w_t)$$\n",
    "\n",
    "For our quantum example:\n",
    "- $s_0$: \"Explain quantum computing to a 10-year-old:\"\n",
    "- $s_1$: \"Explain quantum computing to a 10-year-old: Hey\"  \n",
    "- $s_2$: \"Explain quantum computing to a 10-year-old: Hey there!\"\n",
    "- $s_3$: \"Explain quantum computing to a 10-year-old: Hey there! Imagine\"\n",
    "\n",
    "**Action Space in LLMs:**\n",
    "The action space is the model's vocabulary $\\mathcal{V}$. At each step $t$, the model chooses action $a_t \\in \\mathcal{V}$:\n",
    "$$a_t = \\text{NextToken}(s_t)$$\n",
    "\n",
    "The probability of choosing action $a$ in state $s$ is given by the softmax over logits:\n",
    "$$\\pi_\\theta(a|s) = \\frac{\\exp(f_\\theta(s,a))}{\\sum_{a' \\in \\mathcal{V}} \\exp(f_\\theta(s,a'))}$$\n",
    "where $f_\\theta(s,a)$ represents the logit score for token $a$ given context $s$.\n",
    "\n",
    "**Logit Scores**: These are the raw, unnormalized output values from the neural network before applying softmax. The logit score indicates how strongly the model believes a particular token should follow the current context - higher logits mean higher probability after softmax normalization. These scores directly impact which tokens the model considers most likely to continue the sequence.\n",
    "\n",
    "**Policy Evolution:**\n",
    "The model's policy starts as next-token prediction trained on internet text:\n",
    "$$\\pi_{\\text{pretrained}}(a_t|s_t) \\propto P_{\\text{internet}}(a_t|s_t)$$\n",
    "\n",
    "Through RLHF, this evolves to optimize for human preferences:\n",
    "$$\\pi_{\\text{RLHF}}(a_t|s_t) = \\arg\\max_\\pi \\mathbb{E}_{s,a \\sim \\pi}[R_{\\text{human}}(s,a)]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_{\\text{RLHF}}$ is the optimized policy after reinforcement learning\n",
    "- $a_t$ is the next token to generate\n",
    "- $s_t$ is the current context (all tokens so far)\n",
    "- $\\arg\\max_\\pi$ means \"find the policy that maximizes\"\n",
    "- $\\mathbb{E}_{s,a \\sim \\pi}$ is the expected value when sampling states and actions from policy $\\pi$\n",
    "- $R_{\\text{human}}(s,a)$ is the human preference reward for taking action $a$ in state $s$\n",
    "\n",
    "\n",
    "A **trajectory** $\\tau$ in RL is a sequence of state-action-reward tuples:\n",
    "$$\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots, s_T, a_T, r_T)$$\n",
    "\n",
    "In our quantum explanation scenario:\n",
    "- $s_0$: \"Explain quantum computing to a 10-year-old:\"\n",
    "- $a_0$: \"Hey\" (choosing conversational start)\n",
    "- $s_1$: \"Explain quantum computing to a 10-year-old: Hey\"\n",
    "- $a_1$: \"there!\" (continuing friendly tone)\n",
    "- $s_2$: \"Explain quantum computing to a 10-year-old: Hey there!\"\n",
    "- $a_2$: \"Imagine\" (starting an analogy)\n",
    "- ...\n",
    "- $r_T$: Human preference score for complete response\n",
    "\n",
    "**The Sparse Reward Challenge:**\n",
    "Unlike traditional RL where rewards can be immediate (hitting a wall = -1), language model rewards are typically **sparse** - only available at the end of generation when humans evaluate the complete response. This creates the credit assignment problem: which early token choices led to the eventual positive feedback?\n",
    "\n",
    "$$R(s_t, a_t) = \\begin{cases} \n",
    "0 & \\text{if } t < T \\\\\n",
    "R_{\\text{human}}(\\text{full response}) & \\text{if } t = T\n",
    "\\end{cases}$$\n",
    "\n",
    "The genius of applying RL to language models lies in reframing text generation as a sequential decision-making process. Instead of just predicting the statistically most likely next token (as in standard pretraining), we can now optimize for much more sophisticated objectives:\n",
    "\n",
    "**Traditional Approach**: \n",
    "$$\\text{Objective: } \\max_\\theta \\mathbb{E}_{(x,y) \\sim D}[\\log P_\\theta(y|x)]$$\n",
    "\"What token typically comes next in internet text?\"\n",
    "\n",
    "where:\n",
    "- $\\theta$ represents the model parameters being optimized\n",
    "- $\\mathbb{E}_{(x,y) \\sim D}$ is the expected value over examples from dataset D\n",
    "- $(x,y)$ are input-output pairs (prompt and completion)\n",
    "- $P_\\theta(y|x)$ is the probability the model assigns to output y given input x\n",
    "- The objective maximizes log probability of correct completions in the training data\n",
    "\n",
    "**RL Approach**:\n",
    "$$\\text{Objective: } \\max_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\"What token choice will lead to responses humans find helpful, harmless, and honest?\"\n",
    "\n",
    "where:\n",
    "- $\\theta$ represents the model parameters being optimized\n",
    "- $\\mathbb{E}_{\\tau \\sim \\pi_\\theta}$ is the expected value over trajectories sampled from policy $\\pi_\\theta$\n",
    "- $\\tau$ is a complete trajectory (sequence of tokens) generated by the model\n",
    "- $R(\\tau)$ is the reward (human preference score) assigned to the complete response\n",
    "- The objective maximizes the expected reward across all possible response trajectories\n",
    "\n",
    "In our quantum explanation example, the traditional model might continue with \"superposition\" and \"entanglement\" because they frequently appear after \"quantum computing\" in training data. But an RL-optimized model learns that choosing words like \"imagine\" or \"think of it like\" leads to better human ratings for child-appropriate explanations.\n",
    "\n",
    "**State Space Complexity**: Unlike board games where states are well-defined, language model states live in a vast combinatorial space. The same prompt can lead to exponentially many possible conversation paths:\n",
    "\n",
    "- Path 1: \"Hey there! Imagine you have a magic coin...\"\n",
    "- Path 2: \"So, quantum computers are like super special computers...\"  \n",
    "- Path 3: \"You know how regular computers work with...\"\n",
    "\n",
    "Each path represents a different trajectory through the state space, leading to different human preference scores.\n",
    "\n",
    "This is the exact reason why having same prompt to LLM most likely won't give you the same response \n",
    "\n",
    "**The Value Function in Language Models:**\n",
    "The value function estimates expected future rewards from any given state:\n",
    "$$V^\\pi(s) = \\mathbb{E}_{\\pi}[\\sum_{t'=t}^T \\gamma^{t'-t} R_{t'}|S_t = s]$$\n",
    "\n",
    "where:\n",
    "- $V^\\pi(s)$: Expected total reward when following policy $\\pi$ from state $s$ (a particular sequence of tokens)\n",
    "- $\\mathbb{E}_{\\pi}$: Expected value when following token selection policy $\\pi$\n",
    "- $\\sum_{t'=t}^T$: Sum of rewards from current time $t$ to end of generation $T$\n",
    "- $\\gamma^{t'-t}$: Discount factor giving less weight to distant rewards\n",
    "- $R_{t'}$: Reward received at time $t'$ (usually 0 until final human feedback)\n",
    "- $S_t = s$: Starting from the current state (current prompt + generated tokens so far)\n",
    "\n",
    "In our quantum explanation context:\n",
    "- High value states: Contexts that lead to engaging, age-appropriate explanations\n",
    "- Low value states: Contexts that lead to technical jargon or confusion\n",
    "\n",
    "For example:\n",
    "- $V(\"Explain quantum computing: Hey there! Imagine\")$ = High (leads to good analogies)\n",
    "- $V(\"Explain quantum computing: Quantum superposition involves\")$ = Low (too technical)\n",
    "\n",
    "\n",
    "This reframing transforms language model training from pattern matching to goal-oriented behavior, enabling models to optimize for nuanced human values rather than just statistical likelihood. The mathematical framework provides:\n",
    "\n",
    "1. **Credit Assignment**: Understanding which early tokens contribute to eventual success\n",
    "2. **Policy Optimization**: Systematic improvement toward human preferences  \n",
    "3. **Value Estimation**: Predicting which conversation paths lead to better outcomes\n",
    "4. **Exploration-Exploitation**: Balancing trying new approaches vs. using known good patterns\n",
    "\n",
    "The subsequent RLHF stages (preference collection, reward modeling, and policy optimization) all build on this fundamental RL foundation to create AI systems that are not just knowledgeable, but genuinely helpful and aligned with human intentions. This mathematical rigor is what enables the sophisticated preference learning and optimization algorithms we'll explore in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce026f20",
   "metadata": {},
   "source": [
    "### Human Preference Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e40e5",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow3.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b4b3e",
   "metadata": {},
   "source": [
    "#### But Why Do We Even Need Human Preference Collection?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883ebdc",
   "metadata": {},
   "source": [
    "You might wonder: \"We already have an SFT model that can explain quantum computing nicely to children. Why do we need this additional step?\" This is a fundamental question that gets to the heart of what makes RLHF so powerful.\n",
    "\n",
    "The limitation of SFT becomes clear when we consider what it actually teaches the model. SFT is essentially sophisticated pattern matching - it learns to mimic the style and structure of the demonstration data, but it doesn't develop a deeper understanding of *why* certain responses are better than others. It's like teaching someone to paint by having them copy masterpieces stroke by stroke - they might reproduce the paintings accurately, but they haven't learned the principles that make great art.\n",
    "\n",
    "Consider our quantum computing example. An SFT model might learn that responses for children should use simple language and analogies, but it lacks nuanced understanding of what makes one analogy better than another. It might consistently use the \"spinning coin\" analogy because it appeared in training data, but it won't know that sometimes a \"magic treasure hunt\" analogy might be more engaging for a particular child, or that asking follow-up questions creates better learning experiences.\n",
    "\n",
    "Human Preference Collection addresses these limitations by teaching the model to understand *relative quality* - not just how to generate appropriate responses, but how to distinguish between good, better, and best responses. This comparative learning is fundamentally different from demonstration learning and unlocks much more sophisticated behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3e1a6",
   "metadata": {},
   "source": [
    "Human Preference Collection is essentially a massive, systematic comparison exercise where human evaluators help the model learn what \"better\" means in countless different scenarios. Here's how it works in practice:\n",
    "\n",
    "#### Step 1: Response Generation\n",
    "First, we use our SFT model to generate multiple responses to the same prompt. For our quantum computing example, we might generate several different explanations:\n",
    "\n",
    "**Prompt**: \"Explain quantum computing to a 10-year-old\"\n",
    "\n",
    "**Response A**: \"Quantum computers use qubits instead of regular bits. Think of regular bits like light switches that can only be on or off. Qubits are special because they can be on and off at the same time, like a spinning coin that's both heads and tails until it stops spinning.\"\n",
    "\n",
    "**Response B**: \"Hey there! Imagine you have a magic coin that can be heads and tails at the same time while it's spinning! Quantum computers use these special 'qubits' that work just like our magic coin. This lets them solve puzzles much faster than regular computers. What's your favorite kind of puzzle?\"\n",
    "\n",
    "**Response C**: \"Quantum computing is like having a super-fast helper that can check every room in your house for your lost toy at the same time, instead of checking one room after another like a regular computer would do.\"\n",
    "\n",
    "#### Step 2: Human Evaluation and the Mathematics of Preference\n",
    "\n",
    "Human evaluators (often experts in education, communication, or the relevant domain) are presented with pairs of these responses and asked to choose which one better fulfills the criteria. The evaluation interface might look like:\n",
    "\n",
    "```\n",
    "Prompt: Explain quantum computing to a 10-year-old\n",
    "\n",
    "Response A: [Response A text]\n",
    "Response B: [Response B text]\n",
    "\n",
    "Which response better explains quantum computing to a 10-year-old?\n",
    "□ Response A is significantly better\n",
    "□ Response A is slightly better  \n",
    "□ Response B is slightly better\n",
    "□ Response B is significantly better\n",
    "□ They're about the same quality\n",
    "```\n",
    "\n",
    "The evaluators consider multiple factors:\n",
    "- **Age-appropriateness**: Does it use language a 10-year-old would understand?\n",
    "- **Engagement**: Would this capture and maintain a child's interest?\n",
    "- **Accuracy**: Is the explanation scientifically sound (even if simplified)?\n",
    "- **Clarity**: Would a child actually understand this after reading it?\n",
    "- **Interactivity**: Does it encourage questions or further learning?\n",
    "\n",
    "#### The Bradley-Terry Model: Mathematical Foundation of Preference\n",
    "\n",
    "Behind this seemingly simple comparison process lies sophisticated mathematical modeling. The **Bradley-Terry model** provides the theoretical foundation for how we convert human preferences into trainable signals. This model, originally developed for analyzing sports competitions, perfectly captures the essence of pairwise comparisons.\n",
    "\n",
    "**The Core Mathematical Insight:**\n",
    "The Bradley-Terry model assumes that each response has an underlying \"quality score\" or \"strength\" that determines how likely it is to be preferred over another response. If response $y_w$ (the \"winner\") has strength $\\pi_w$ and response $y_l$ (the \"loser\") has strength $\\pi_l$, then the probability that humans prefer $y_w$ over $y_l$ is:\n",
    "\n",
    "$$P(y_w \\succ y_l) = \\frac{\\pi_w}{\\pi_w + \\pi_l}$$\n",
    "\n",
    "In our quantum computing example, if Response B consistently beats Response A in human evaluations, the model learns that Response B has higher intrinsic quality for this type of explanation.\n",
    "\n",
    "**Connecting to Language Models:**\n",
    "In RLHF, we don't observe these strength values $\\pi$ directly. Instead, we model them using our language model's probability of generating each response:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\frac{\\exp(r_\\theta(x, y_w))}{\\exp(r_\\theta(x, y_w)) + \\exp(r_\\theta(x, y_l))}$$\n",
    "\n",
    "where:\n",
    "- $x$ is the prompt (\"Explain quantum computing to a 10-year-old\")\n",
    "- $y_w$ and $y_l$ are the competing responses\n",
    "- $r_\\theta(x, y)$ is a reward function that scores how good response $y$ is for prompt $x$\n",
    "\n",
    "This can be simplified using the logistic function:\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, which transforms any input into a value between 0 and 1, essentially converting the difference in reward scores into a probability.\n",
    "\n",
    "**Why This Mathematical Foundation Matters:**\n",
    "The Bradley-Terry model provides several crucial advantages:\n",
    "\n",
    "1. **Transitivity**: If Response B beats Response A, and Response C beats Response B, then Response C should beat Response A. The model ensures these relationships are mathematically consistent.\n",
    "\n",
    "2. **Calibrated Probabilities**: The model provides well-calibrated probability estimates. If the model says Response B has a 0.8 probability of being preferred over Response A, then in practice, humans should prefer B about 80% of the time.\n",
    "\n",
    "3. **Handling Uncertainty**: The model can represent cases where responses are very close in quality (probability near 0.5) versus clear winners (probability near 0 or 1).\n",
    "\n",
    "#### Step 3: Preference Data Creation and Loss Functions\n",
    "\n",
    "Each comparison creates a preference data point. If evaluators consistently prefer Response B over Response A, this creates a training signal that Response B's approach (conversational tone, asking questions, using engaging analogies) should be valued more highly than Response A's approach (more technical, less interactive).\n",
    "\n",
    "**The Preference Dataset:**\n",
    "Mathematically, our preference dataset $\\mathcal{D}$ consists of tuples:\n",
    "$$\\mathcal{D} = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\}_{i=1}^N$$\n",
    "\n",
    "where for each prompt $x^{(i)}$, we have identified a preferred response $y_w^{(i)}$ and a less preferred response $y_l^{(i)}$.\n",
    "\n",
    "**The Loss Function:**\n",
    "To train models on this preference data, we use the Bradley-Terry loss:\n",
    "$$\\mathcal{L}_{BT} = -\\sum_{i=1}^N \\log \\sigma(r_\\theta(x^{(i)}, y_w^{(i)}) - r_\\theta(x^{(i)}, y_l^{(i)}))$$\n",
    "\n",
    "This loss function represents the Bradley-Terry loss for preference learning, where:\n",
    "- $\\mathcal{L}_{BT}$ is the total loss we're minimizing\n",
    "- $\\sigma$ is the sigmoid function that converts score differences into probabilities\n",
    "- $r_\\theta(x^{(i)}, y_w^{(i)})$ is the reward score for the preferred response\n",
    "- $r_\\theta(x^{(i)}, y_l^{(i)})$ is the reward score for the less preferred response\n",
    "- The negative log ensures that maximizing the probability of correct preferences minimizes the loss\n",
    "- We sum over all N preference pairs in our dataset\n",
    "\n",
    "This loss function encourages the reward model to assign higher scores to preferred responses and lower scores to dispreferred ones.\n",
    "\n",
    "**Practical Example with Our Quantum Responses:**\n",
    "Let's say human evaluators prefer Response B over Response A with a probability of 0.85. The Bradley-Terry model would encode this as:\n",
    "\n",
    "$$P(\\text{Response B} \\succ \\text{Response A}) = \\sigma(r_\\theta(\\text{prompt}, \\text{Response B}) - r_\\theta(\\text{prompt}, \\text{Response A})) = 0.85$$\n",
    "\n",
    "This means:\n",
    "$$r_\\theta(\\text{prompt}, \\text{Response B}) - r_\\theta(\\text{prompt}, \\text{Response A}) = \\sigma^{-1}(0.85) \\approx 1.73$$\n",
    "\n",
    "The reward model learns that Response B's engaging, interactive approach should score about 1.73 points higher than Response A's more technical approach.\n",
    "\n",
    "\n",
    "Let's extend our quantum computing analogy to understand preference collection itself:\n",
    "\n",
    "Imagine you're teaching a young student not just about quantum computers, but about how to *mathematically evaluate* different explanations. Instead of just showing them one \"correct\" way to explain it (like SFT), you show them pairs of explanations and ask: \"Which of these would help you understand better, and how confident are you?\"\n",
    "\n",
    "**Comparison 1**: Technical explanation vs. Magic coin analogy\n",
    "→ Student strongly prefers: Magic coin analogy (Bradley-Terry probability: 0.9)\n",
    "\n",
    "**Comparison 2**: Magic coin analogy vs. Treasure hunt analogy  \n",
    "→ Student slightly prefers: Treasure hunt analogy (Bradley-Terry probability: 0.6)\n",
    "\n",
    "**Comparison 3**: Treasure hunt analogy vs. Interactive treasure hunt with questions\n",
    "→ Student strongly prefers: Interactive version (Bradley-Terry probability: 0.95)\n",
    "\n",
    "The mathematical model learns not just the ranking (interactive > treasure hunt > magic coin > technical), but also the *strength* of these preferences. The large gap between technical and magic coin explanations tells the model that engagement is critically important. The smaller gap between magic coin and treasure hunt analogies suggests that while both work well, the specific analogy choice is less crucial than the overall approach.\n",
    "\n",
    "\n",
    "In real RLHF implementations, this process happens at massive scale:\n",
    "\n",
    "- **Thousands of prompts** across diverse topics and scenarios\n",
    "- **Multiple responses** generated for each prompt (typically 2-4)\n",
    "- **Multiple evaluators** rating each comparison (to ensure reliability)\n",
    "- **Hundreds of thousands** of preference comparisons collected\n",
    "- **Quality control** measures to ensure consistent evaluation standards\n",
    "\n",
    "**Inter-Annotator Agreement:**\n",
    "With multiple human evaluators, we need to measure consistency. If evaluators frequently disagree, the preference signal becomes noisy. We typically use metrics like:\n",
    "\n",
    "- **Fleiss' Kappa**: Measures agreement across multiple evaluators\n",
    "- **Krippendorff's Alpha**: Handles different scales of preference (slight vs. strong)\n",
    "\n",
    "**Confidence Intervals:**\n",
    "The Bradley-Terry model also provides uncertainty estimates. If only 55% of evaluators prefer Response B over Response A, the confidence interval for the preference probability might be [0.45, 0.65], indicating high uncertainty. The training process can weight these uncertain comparisons less heavily.\n",
    "\n",
    "For our quantum explanation example, the preference data might reveal patterns like:\n",
    "- Responses with questions score higher than statements (BT probability: 0.75)\n",
    "- Analogies to familiar objects beat abstract concepts (BT probability: 0.88)\n",
    "- Conversational tone (\"Hey there!\") beats formal tone (BT probability: 0.82)\n",
    "- Interactive explanations beat passive ones (BT probability: 0.91)\n",
    "\n",
    "These probabilities encode not just the preferences, but their relative importance for training the reward model.\n",
    "\n",
    "\n",
    "**Handling Ties and Ambiguous Preferences:**\n",
    "Sometimes evaluators can't decide between responses, or they're genuinely equivalent. The Bradley-Terry model handles this by allowing preference probabilities near 0.5. We can extend the model to explicitly handle ties:\n",
    "\n",
    "$$P(\\text{tie}) = \\frac{\\nu}{\\pi_w + \\pi_l + \\nu}$$\n",
    "\n",
    "where $\\nu$ represents the \"tie strength.\"\n",
    "\n",
    "**Multiple Response Comparisons:**\n",
    "While pairwise comparisons are most common, we can extend to ranking multiple responses simultaneously using the **Plackett-Luce model**, which generalizes Bradley-Terry to full rankings:\n",
    "\n",
    "$$P(\\text{ranking: } y_1 \\succ y_2 \\succ \\ldots \\succ y_k) = \\prod_{i=1}^k \\frac{\\pi_{y_i}}{\\sum_{j=i}^k \\pi_{y_j}}$$\n",
    "\n",
    "This equation represents the Plackett-Luce model for comparing multiple responses simultaneously. Here:\n",
    "- $y_1 \\succ y_2 \\succ \\ldots \\succ y_k$ is a complete ranking of k responses\n",
    "- $\\pi_{y_i}$ is the quality/strength score of response $y_i$\n",
    "- The equation calculates the probability of observing a specific ranking by multiplying the conditional probabilities of each response being selected as best among the remaining options\n",
    "- At each step i, we calculate the probability of choosing response $y_i$ as the best among all remaining responses ($y_i$ through $y_k$)\n",
    "\n",
    "**Temporal and Context Dependencies:**\n",
    "Advanced implementations consider that preferences might change over time or depend on context. The model can be extended to:\n",
    "\n",
    "$$P(y_w \\succ y_l | x, t, c) = \\sigma(r_\\theta(x, y_w, t, c) - r_\\theta(x, y_l, t, c))$$\n",
    "\n",
    "This extended equation models preferences that change over time or depend on context. Here:\n",
    "- $t$ represents the time component (preferences might evolve as language norms change)\n",
    "- $c$ represents additional context factors (user demographics, device type, previous interactions)\n",
    "- $r_\\theta(x, y, t, c)$ is the reward function that scores response $y$ for prompt $x$, considering time $t$ and context $c$\n",
    "- $\\sigma$ is the sigmoid function that converts the difference in reward scores into a probability\n",
    "- The equation gives the probability that response $y_w$ is preferred over $y_l$ given all these factors\n",
    "\n",
    "This enables more personalized and contextually appropriate preference modeling beyond static comparisons.\n",
    "\n",
    "where $t$ represents time and $c$ represents additional context.\n",
    "\n",
    "#### Why This Works Better Than More SFT Data\n",
    "\n",
    "You might think: \"Why not just collect more demonstration data instead?\" The key insight is that preference collection captures information that demonstration data cannot:\n",
    "\n",
    "**Demonstration data tells us**: \"This is a good response\" (provides one point in quality space)\n",
    "\n",
    "**Preference data tells us**: \"This response is better than that response *because*...\" (provides relative quality differences)\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "SFT learns to maximize likelihood:\n",
    "$$\\mathcal{L}_{SFT} = \\sum_{i=1}^N \\log P_\\theta(y^{(i)} | x^{(i)})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{SFT}$ is the supervised fine-tuning loss function we're maximizing\n",
    "- $P_\\theta(y^{(i)} | x^{(i)})$ is the probability that our model with parameters $\\theta$ assigns to the demonstration response $y^{(i)}$ given prompt $x^{(i)}$\n",
    "- We sum over all N examples in our demonstration dataset\n",
    "- This function encourages the model to assign high probability to human-written demonstration responses, but doesn't distinguish between different quality levels among acceptable responses\n",
    "\n",
    "This doesn't distinguish between \"good\" and \"better\" - it treats all demonstration responses as equally optimal.\n",
    "\n",
    "Preference learning optimizes for relative quality:\n",
    "$$\\mathcal{L}_{Pref} = \\sum_{i=1}^N \\log P(y_w^{(i)} \\succ y_l^{(i)} | x^{(i)})$$\n",
    "\n",
    "This explicitly learns the quality differences that matter to humans. In this equation:\n",
    "- $\\mathcal{L}_{Pref}$ is the preference learning loss function we're maximizing\n",
    "- $P(y_w^{(i)} \\succ y_l^{(i)} | x^{(i)})$ is the probability that the preferred response $y_w^{(i)}$ is indeed better than the less preferred response $y_l^{(i)}$ for prompt $x^{(i)}$\n",
    "- We sum over all N preference pairs in our dataset\n",
    "- This function encourages the model to correctly predict human preferences between responses, learning the relative quality differences humans care about\n",
    "\n",
    "The \"because\" part is crucial. Through comparative evaluation, the model learns the underlying principles that make responses effective, not just specific examples of effective responses. This enables much better generalization to new scenarios and more nuanced quality judgments.\n",
    "\n",
    "In our quantum computing scenario, instead of learning \"use the magic coin analogy,\" the model learns \"use analogies that relate to a child's everyday experience, and frame them in an engaging, interactive way\" with a quantified strength of preference (Bradley-Terry probability of 0.83 for interactive vs. passive explanations).\n",
    "\n",
    "This preference data becomes the foundation for the next stage: training a reward model that can automatically evaluate response quality and guide the final optimization process. The human preferences, collected at scale and mathematically modeled through the Bradley-Terry framework, teach the AI system not just what to say, but what makes some ways of saying things better than others - and exactly how much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd636c7b",
   "metadata": {},
   "source": [
    "### Reward Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f9112",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow4.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3115b",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have collected thousands of human preference comparisons, we face a critical challenge: how do we scale this human judgment to evaluate millions of potential responses? This is where reward models become essential - they serve as computational proxies for human preferences, transforming our carefully collected preference data into an automated evaluation system.\n",
    "\n",
    "#### Why Do We Need Reward Models?\n",
    "\n",
    "You might wonder: \"Why can't we just ask humans to evaluate every response during training?\" The scale problem becomes apparent when we consider the numbers:\n",
    "\n",
    "- **During preference collection**: Humans evaluate ~100,000 preference pairs over weeks or months\n",
    "- **During RL optimization**: The model generates millions of responses during training, each needing evaluation\n",
    "- **Real-time constraints**: Policy optimization requires immediate feedback for each generated token sequence\n",
    "\n",
    "Having humans evaluate every single response would be impossibly slow and expensive. We need a way to \"distill\" human judgment into a fast, automated system that can predict human preferences at scale.\n",
    "\n",
    "\n",
    "Remember our Bradley-Terry model from preference collection? The reward model is where that mathematical framework becomes operational. We're essentially training a neural network to predict the outcome of human preference comparisons.\n",
    "\n",
    "**The Core Insight:**\n",
    "If humans prefer response $y_w$ over $y_l$ for prompt $x$, then there exists some underlying \"reward\" or \"quality score\" such that:\n",
    "\n",
    "$$r(x, y_w) > r(x, y_l)$$\n",
    "\n",
    "The Bradley-Terry model tells us the probability of this preference should be:\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r(x, y_w) - r(x, y_l))$$\n",
    "\n",
    "**Training Objective:**\n",
    "Our reward model $r_\\theta$ is trained to minimize the Bradley-Terry loss:\n",
    "$$\\mathcal{L}_{RM} = -\\sum_{i=1}^N \\log \\sigma(r_\\theta(x^{(i)}, y_w^{(i)}) - r_\\theta(x^{(i)}, y_l^{(i)}))$$\n",
    "\n",
    "This loss function encourages the reward model to assign higher scores to preferred responses and lower scores to dispreferred ones.\n",
    "\n",
    "\n",
    "Let's continue with our educational tutor scenario. Imagine we've collected preference data and found these patterns:\n",
    "\n",
    "**Preference Pattern 1**: Interactive explanations consistently beat passive ones (85% preference rate)\n",
    "**Preference Pattern 2**: Age-appropriate analogies beat technical terms (92% preference rate)  \n",
    "**Preference Pattern 3**: Encouraging tone beats neutral tone (73% preference rate)\n",
    "\n",
    "Instead of memorizing these specific comparisons, we want to train a \"preference predictor\" (our reward model) that can:\n",
    "\n",
    "1. **Generalize**: Evaluate new quantum explanations it has never seen before\n",
    "2. **Combine factors**: Understand that an explanation that is both interactive AND uses good analogies should score higher than one with just good analogies\n",
    "3. **Quantify quality**: Assign numerical scores that reflect the strength of human preferences\n",
    "\n",
    "For our quantum computing examples:\n",
    "\n",
    "- **Response A** (technical): \"Quantum computers use qubits that exist in superposition...\"\n",
    "  - Reward score: 2.1 (low due to technical language)\n",
    "\n",
    "- **Response B** (good analogy): \"Think of qubits like spinning coins that can be heads and tails...\"\n",
    "  - Reward score: 6.8 (higher due to good analogy)\n",
    "\n",
    "- **Response C** (interactive + analogy): \"Hey there! Imagine you have a magic coin... What do you think would happen if...?\"\n",
    "  - Reward score: 8.9 (highest due to combining engagement, analogy, and interactivity)\n",
    "\n",
    "#### Architecture \n",
    "\n",
    "**Model Architecture:**\n",
    "The reward model typically shares the same transformer architecture as our language model, but with a crucial difference in the output layer:\n",
    "\n",
    "```\n",
    "Input: [Prompt + Response tokens]\n",
    "     ↓\n",
    "Transformer Layers (shared with LLM)\n",
    "     ↓\n",
    "Final Hidden State\n",
    "     ↓\n",
    "Linear Projection → Single Scalar Score\n",
    "```\n",
    "\n",
    "**Key Architectural Decisions:**\n",
    "\n",
    "1. **Shared Backbone**: Using the same transformer architecture ensures the reward model understands language with the same sophistication as the policy model\n",
    "\n",
    "2. **Scalar Output**: Unlike language models that output probability distributions over vocabulary, reward models output a single number representing quality\n",
    "\n",
    "3. **Prompt-Response Processing**: The model sees the entire conversation context (prompt + response) and learns to evaluate the appropriateness of the response given the specific prompt\n",
    "\n",
    "\n",
    "**Dataset Preparation:**\n",
    "Our preference dataset becomes a set of training examples:\n",
    "$$\\mathcal{D}_{RM} = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\\}_{i=1}^N$$\n",
    "\n",
    "Each example consists of:\n",
    "- $x^{(i)}$: A prompt (\"Explain quantum computing to a 10-year-old\")\n",
    "- $y_w^{(i)}$: The preferred response (interactive explanation with analogies)\n",
    "- $y_l^{(i)}$: The less preferred response (technical explanation)\n",
    "\n",
    "**Forward Pass:**\n",
    "For each training example, we compute:\n",
    "\n",
    "$r_w = r_\\theta(x^{(i)}, y_w^{(i)})$ (reward for preferred response)\n",
    "\n",
    "$r_l = r_\\theta(x^{(i)}, y_l^{(i)})$ (reward for less preferred response)\n",
    "\n",
    "In this forward pass, we're computing reward scores for both responses in a preference pair. The reward model $r_\\theta$ with parameters $\\theta$ processes each prompt-response pair independently. It assigns a scalar score $r_w$ to the preferred response $y_w^{(i)}$ and a score $r_l$ to the less preferred response $y_l^{(i)}$, both conditioned on the same prompt $x^{(i)}$. This forward pass transforms text into quantitative preference signals that can be used to calculate the loss and update model parameters.\n",
    "\n",
    "**Loss Calculation:**\n",
    "The Bradley-Terry loss becomes:\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\sigma(r_w - r_l) = -\\log \\frac{1}{1 + e^{-(r_w - r_l)}}$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$\\mathcal{L} = \\log(1 + e^{-(r_w - r_l)}) = \\log(1 + e^{r_l - r_w})$$\n",
    "\n",
    "The Bradley-Terry loss function mathematically quantifies how well our reward model predicts human preferences. This equation represents the negative log likelihood of the probability that response $y_w$ is preferred over response $y_l$. It penalizes the model when it assigns similar or incorrect scores to the preference pairs. When the preferred response has a much higher reward score than the less preferred one ($r_w \\gg r_l$), the loss approaches zero. Conversely, if the model incorrectly scores the less preferred response higher, the loss grows substantially. This elegant formulation ensures the reward model learns to align its numerical scores with human judgments across diverse responses.\n",
    "\n",
    "**Gradient Flow:**\n",
    "The gradients encourage:\n",
    "- **Increasing $r_w$**: Making preferred responses score higher\n",
    "- **Decreasing $r_l$**: Making dispreferred responses score lower\n",
    "- **Maximizing the gap**: The sigmoid function creates stronger gradients when the difference is small, encouraging clear separation\n",
    "\n",
    "\n",
    "**Preference Strength Modeling:**\n",
    "Not all preferences are equally strong. We can extend the model to handle preference strength:\n",
    "\n",
    "$$P(y_w \\succ y_l | x, s) = \\sigma(s \\cdot (r_\\theta(x, y_w) - r_\\theta(x, y_l)))$$\n",
    "\n",
    "where $s$ represents preference strength:\n",
    "- $s = 1$: Slight preference  \n",
    "- $s = 2$: Strong preference\n",
    "- $s = 3$: Very strong preference\n",
    "\n",
    "**Regularization and Calibration:**\n",
    "To prevent overfitting and ensure well-calibrated probabilities, we often add regularization terms:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{BT} + \\lambda_1 \\|\\theta\\|_2^2 + \\lambda_2 \\mathcal{L}_{calibration}$$\n",
    "\n",
    "where:\n",
    "- $\\|\\theta\\|_2^2$ is L2 regularization preventing large weights\n",
    "- $\\mathcal{L}_{calibration}$ ensures predicted probabilities match observed frequencies\n",
    "\n",
    "The equation represents the total loss function used to train the reward model, combining the Bradley-Terry preference loss with regularization terms. L2 regularization (the $\\|\\theta\\|_2^2$ term) penalizes large parameter values by adding the sum of squared weights to the loss function, encouraging the model to learn simpler patterns and avoid overfitting. This regularization technique shrinks weights toward zero, improving generalization to unseen data by preventing the model from becoming too complex or putting too much emphasis on any single feature.\n",
    "\n",
    "\n",
    "**Handling Uncertainty:**\n",
    "\n",
    "For ambiguous preferences where humans disagreed, we can model uncertainty:\n",
    "\n",
    "$$\\mathcal{L}_{uncertain} = -\\alpha \\log \\sigma(r_w - r_l) - (1-\\alpha) \\log \\sigma(r_l - r_w)$$\n",
    "\n",
    "where $\\alpha$ represents the fraction of annotators who preferred $y_w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bed03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize reward model using our existing PretrainedLLM class\n",
    "        \n",
    "        Args:\n",
    "            base_model: PretrainedLLM instance (our existing class)\n",
    "            device: Computing device\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Use the transformer from our existing PretrainedLLM class\n",
    "        self.transformer = base_model.model\n",
    "        self.tokenizer = base_model.tokenizer\n",
    "        \n",
    "        # Remove the language modeling head and add reward head\n",
    "        if hasattr(self.transformer, 'lm_head'):\n",
    "            self.transformer.lm_head = None\n",
    "        \n",
    "        # Single scalar output for reward prediction\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        self.reward_head = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Initialize reward head with small weights\n",
    "        nn.init.normal_(self.reward_head.weight, std=0.01)\n",
    "        nn.init.zeros_(self.reward_head.bias)\n",
    "        \n",
    "        print(f\"Reward model initialized with {sum(p.numel() for p in self.parameters())/1e6:.1f}M parameters\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the reward model\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs for [prompt + response]\n",
    "            attention_mask: Mask for padding tokens\n",
    "            \n",
    "        Returns:\n",
    "            reward_score: Scalar reward for the input sequence\n",
    "        \"\"\"\n",
    "        # Get transformer outputs (compatible with our transformer architecture)\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Get the last hidden state\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Use the last non-padding token for reward prediction\n",
    "        if attention_mask is not None:\n",
    "            # Find the last non-padding token for each sequence\n",
    "            last_non_pad_indices = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = input_ids.shape[0]\n",
    "            \n",
    "            # Extract the hidden state at the last non-padding position\n",
    "            last_hidden = last_hidden_state[\n",
    "                torch.arange(batch_size), last_non_pad_indices\n",
    "            ]\n",
    "        else:\n",
    "            # If no mask, use the last token\n",
    "            last_hidden = last_hidden_state[:, -1, :]\n",
    "        \n",
    "        # Project to scalar reward\n",
    "        reward_score = self.reward_head(last_hidden).squeeze(-1)\n",
    "        \n",
    "        return reward_score\n",
    "\n",
    "class RewardModelTrainer:\n",
    "    def __init__(self, reward_model, learning_rate=1e-5):\n",
    "        \"\"\"\n",
    "        Initialize reward model trainer\n",
    "        \n",
    "        Args:\n",
    "            reward_model: RewardModel instance\n",
    "            learning_rate: Learning rate for optimization\n",
    "        \"\"\"\n",
    "        self.model = reward_model\n",
    "        self.tokenizer = reward_model.tokenizer\n",
    "        self.device = reward_model.device\n",
    "        self.optimizer = torch.optim.AdamW(reward_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def create_preference_dataset(self, sft_model, prompts, num_responses_per_prompt=4):\n",
    "        \"\"\"\n",
    "        Create synthetic preference dataset using our SFT model\n",
    "        This simulates the human preference collection process\n",
    "        \n",
    "        Args:\n",
    "            sft_model: SupervisedFineTuner instance (our SFT model)\n",
    "            prompts: List of prompts to generate responses for\n",
    "            num_responses_per_prompt: Number of responses to generate per prompt\n",
    "            \n",
    "        Returns:\n",
    "            preference_data: List of preference tuples (prompt, preferred, dispreferred)\n",
    "        \"\"\"\n",
    "        print(\"Generating responses for preference dataset...\")\n",
    "        preference_data = []\n",
    "        \n",
    "        for prompt in tqdm(prompts, desc=\"Generating preference data\"):\n",
    "            responses = []\n",
    "            \n",
    "            # Generate multiple responses for the same prompt\n",
    "            for _ in range(num_responses_per_prompt):\n",
    "                # Format prompt for SFT model\n",
    "                formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "                inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = sft_model.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        temperature=0.8,  # Higher temperature for diversity\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                # Decode response\n",
    "                full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                response = full_text[len(formatted_prompt):]\n",
    "                responses.append(response.strip())\n",
    "            \n",
    "            # Create preference pairs using simple heuristics\n",
    "            # In practice, this would be done by human evaluators\n",
    "            for i in range(len(responses)):\n",
    "                for j in range(i + 1, len(responses)):\n",
    "                    # Simple heuristic: prefer longer, more engaging responses\n",
    "                    # In reality, humans would judge based on helpfulness, accuracy, etc.\n",
    "                    score_i = self._simple_quality_score(responses[i])\n",
    "                    score_j = self._simple_quality_score(responses[j])\n",
    "                    \n",
    "                    if score_i > score_j:\n",
    "                        preference_data.append((prompt, responses[i], responses[j]))\n",
    "                    elif score_j > score_i:\n",
    "                        preference_data.append((prompt, responses[j], responses[i]))\n",
    "        \n",
    "        print(f\"Created {len(preference_data)} preference pairs\")\n",
    "        return preference_data\n",
    "    \n",
    "    def _simple_quality_score(self, response):\n",
    "        \"\"\"\n",
    "        Simple heuristic to simulate human preference\n",
    "        In practice, this would be actual human evaluation\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Prefer responses with questions (interactive)\n",
    "        if '?' in response:\n",
    "            score += 2\n",
    "        \n",
    "        # Prefer responses with engaging words\n",
    "        engaging_words = ['imagine', 'think', 'consider', 'hey', 'wow', 'amazing']\n",
    "        for word in engaging_words:\n",
    "            if word.lower() in response.lower():\n",
    "                score += 1\n",
    "        \n",
    "        # Prefer moderate length (not too short, not too long)\n",
    "        length = len(response.split())\n",
    "        if 20 <= length <= 100:\n",
    "            score += 1\n",
    "        elif length < 10:\n",
    "            score -= 2\n",
    "        \n",
    "        # Prefer responses with analogies\n",
    "        analogy_words = ['like', 'similar', 'imagine', 'think of']\n",
    "        for word in analogy_words:\n",
    "            if word.lower() in response.lower():\n",
    "                score += 1\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def prepare_preference_batch(self, preference_data, batch_size=8):\n",
    "        \"\"\"\n",
    "        Prepare batches for training the reward model\n",
    "        \n",
    "        Args:\n",
    "            preference_data: List of (prompt, preferred, dispreferred) tuples\n",
    "            batch_size: Size of training batches\n",
    "            \n",
    "        Returns:\n",
    "            batches: List of prepared batches\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, len(preference_data), batch_size):\n",
    "            batch_data = preference_data[i:i + batch_size]\n",
    "            \n",
    "            # Prepare inputs for preferred responses\n",
    "            preferred_texts = []\n",
    "            dispreferred_texts = []\n",
    "            \n",
    "            for prompt, preferred, dispreferred in batch_data:\n",
    "                # Format as conversation for consistency with our SFT format\n",
    "                preferred_text = f\"User: {prompt}\\n\\nAssistant: {preferred}\"\n",
    "                dispreferred_text = f\"User: {prompt}\\n\\nAssistant: {dispreferred}\"\n",
    "                \n",
    "                preferred_texts.append(preferred_text)\n",
    "                dispreferred_texts.append(dispreferred_text)\n",
    "            \n",
    "            # Tokenize both sets\n",
    "            preferred_inputs = self.tokenizer(\n",
    "                preferred_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            dispreferred_inputs = self.tokenizer(\n",
    "                dispreferred_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            batch = {\n",
    "                'preferred_ids': preferred_inputs['input_ids'],\n",
    "                'preferred_mask': preferred_inputs['attention_mask'],\n",
    "                'dispreferred_ids': dispreferred_inputs['input_ids'],\n",
    "                'dispreferred_mask': dispreferred_inputs['attention_mask']\n",
    "            }\n",
    "            \n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def bradley_terry_loss(self, preferred_rewards, dispreferred_rewards):\n",
    "        \"\"\"\n",
    "        Compute Bradley-Terry loss for preference learning\n",
    "        This is the same loss used across TRPO, PPO, and GRPO\n",
    "        \n",
    "        Args:\n",
    "            preferred_rewards: Rewards for preferred responses\n",
    "            dispreferred_rewards: Rewards for dispreferred responses\n",
    "            \n",
    "        Returns:\n",
    "            loss: Bradley-Terry loss encouraging preferred > dispreferred\n",
    "        \"\"\"\n",
    "        # Compute log probability that preferred is better\n",
    "        logits = preferred_rewards - dispreferred_rewards\n",
    "        loss = -F.logsigmoid(logits).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        Single training step on a batch of preference data\n",
    "        \n",
    "        Args:\n",
    "            batch: Dictionary containing preference batch data\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass for preferred responses\n",
    "        preferred_rewards = self.model(\n",
    "            batch['preferred_ids'], \n",
    "            batch['preferred_mask']\n",
    "        )\n",
    "        \n",
    "        # Forward pass for dispreferred responses\n",
    "        dispreferred_rewards = self.model(\n",
    "            batch['dispreferred_ids'],\n",
    "            batch['dispreferred_mask'] \n",
    "        )\n",
    "        \n",
    "        # Compute Bradley-Terry loss (same as used in TRPO/PPO/GRPO)\n",
    "        loss = self.bradley_terry_loss(preferred_rewards, dispreferred_rewards)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy (how often preferred > dispreferred)\n",
    "        accuracy = (preferred_rewards > dispreferred_rewards).float().mean()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'preferred_reward_mean': preferred_rewards.mean().item(),\n",
    "            'dispreferred_reward_mean': dispreferred_rewards.mean().item(),\n",
    "            'reward_gap': (preferred_rewards - dispreferred_rewards).mean().item()\n",
    "        }\n",
    "    \n",
    "    def train(self, preference_data, num_epochs=3, batch_size=8):\n",
    "        \"\"\"\n",
    "        Train the reward model on preference data\n",
    "        \n",
    "        Args:\n",
    "            preference_data: List of preference tuples\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size\n",
    "        \"\"\"\n",
    "        print(f\"Training reward model for {num_epochs} epochs...\")\n",
    "        \n",
    "        # Prepare batches\n",
    "        batches = self.prepare_preference_batch(preference_data, batch_size)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "            \n",
    "            progress_bar = tqdm(batches, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                metrics = self.train_step(batch)\n",
    "                total_loss += metrics['loss']\n",
    "                total_accuracy += metrics['accuracy']\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{metrics['loss']:.4f}\",\n",
    "                    'acc': f\"{metrics['accuracy']:.3f}\",\n",
    "                    'gap': f\"{metrics['reward_gap']:.3f}\"\n",
    "                })\n",
    "            \n",
    "            avg_loss = total_loss / len(batches)\n",
    "            avg_accuracy = total_accuracy / len(batches)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.3f}\")\n",
    "    \n",
    "    def evaluate_responses(self, prompts, responses):\n",
    "        \"\"\"\n",
    "        Evaluate responses using the trained reward model\n",
    "        This method will be used by TRPO, PPO, and GRPO optimizers\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompts\n",
    "            responses: List of responses to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            rewards: Tensor of reward scores\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        rewards = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt, response in zip(prompts, responses):\n",
    "                # Format as conversation\n",
    "                text = f\"User: {prompt}\\n\\nAssistant: {response}\"\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = self.tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Get reward score\n",
    "                reward = self.model(inputs['input_ids'], inputs['attention_mask'])\n",
    "                rewards.append(reward.item())\n",
    "        \n",
    "        return torch.tensor(rewards)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the trained reward model\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.model.state_dict(), os.path.join(path, \"reward_model.pt\"))\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        print(f\"Reward model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a trained reward model\"\"\"\n",
    "        self.model.load_state_dict(torch.load(os.path.join(path, \"reward_model.pt\")))\n",
    "        print(f\"Reward model loaded from {path}\")\n",
    "\n",
    "# Example usage integrating with our existing classes\n",
    "def create_reward_model_example():\n",
    "    \"\"\"\n",
    "    Example of creating and training a reward model using our existing classes\n",
    "    This reward model will be used consistently across TRPO, PPO, GRPO algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use our existing PretrainedLLM class\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Create and train SFT model (using our existing SupervisedFineTuner)\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    processed_data = sft.prepare_data()\n",
    "    peft_model = sft.setup_peft()\n",
    "    adapter_path = sft.train(output_dir=\"sft_model\", num_epochs=1)\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    # Create reward model using the same base architecture\n",
    "    reward_model = RewardModel(base_llm)\n",
    "    reward_trainer = RewardModelTrainer(reward_model)\n",
    "    \n",
    "    # Create preference dataset (simulating human preferences)\n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How do I bake a chocolate cake?\",\n",
    "        \"What is machine learning?\",\n",
    "        \"Write a short story about friendship\",\n",
    "        \"Explain gravity in simple terms\"\n",
    "    ]\n",
    "    \n",
    "    preference_data = reward_trainer.create_preference_dataset(sft, test_prompts)\n",
    "    \n",
    "    # Train the reward model (same training process for TRPO/PPO/GRPO)\n",
    "    reward_trainer.train(preference_data, num_epochs=2)\n",
    "    \n",
    "    # Test the reward model\n",
    "    print(\"\\n=== Reward Model Evaluation ===\")\n",
    "    test_responses = [\n",
    "        \"Quantum computers are like magic calculators that can solve many problems at once!\",\n",
    "        \"Quantum computing involves qubits and superposition states in quantum mechanical systems.\",\n",
    "        \"Hey there! Imagine quantum computers as super-smart puzzle solvers that can try all solutions simultaneously!\"\n",
    "    ]\n",
    "    \n",
    "    prompt = \"Explain quantum computing to a 10-year-old\"\n",
    "    rewards = reward_trainer.evaluate_responses([prompt] * 3, test_responses)\n",
    "    \n",
    "    for i, (response, reward) in enumerate(zip(test_responses, rewards)):\n",
    "        print(f\"\\nResponse {i+1}: {response[:60]}...\")\n",
    "        print(f\"Reward Score: {reward:.3f}\")\n",
    "    \n",
    "    # Save the reward model for use in optimization algorithms\n",
    "    reward_trainer.save_model(\"reward_model\")\n",
    "    \n",
    "    return reward_model, reward_trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018020c",
   "metadata": {},
   "source": [
    "#### Understanding the RewardModel Class\n",
    "\n",
    "The RewardModel class is fundamentally a neural network that takes a conversation (prompt + response) and outputs a single number representing how \"good\" or \"helpful\" that response is according to human preferences. Think of it as an automated judge that has learned to evaluate responses the way humans would. The architecture is brilliant in its simplicity yet powerful in its approach.\n",
    "\n",
    "When we initialize the reward model, we start with an existing pretrained language model from our PretrainedLLM class. This is crucial because we want the reward model to understand language with the same sophistication as the models it will be evaluating. The initialization process does something very interesting - it takes the transformer backbone (all those attention layers that understand language) but removes the language modeling head that normally predicts the next token. Instead, it replaces this with a simple linear layer that outputs just one number - the reward score. This architectural change transforms a text generator into a text evaluator.\n",
    "\n",
    "The reward head initialization is carefully done with small random weights and zero bias. This ensures that initially, the model doesn't have strong preferences and can learn from the human preference data without being biased toward any particular type of response. The total parameter count is printed to show how large the model is - in this case, it includes all the transformer parameters plus the small reward head.\n",
    "\n",
    "The forward pass through the reward model is where the magic happens. When you give it a conversation, it processes the entire sequence through the transformer layers, just like a normal language model would. However, instead of using all the hidden states to predict next tokens, it focuses specifically on the final hidden state at the last meaningful position. This is important because the last token position contains the most comprehensive understanding of the entire conversation - it has \"seen\" and processed all the previous context.\n",
    "\n",
    "The attention mask handling is crucial for practical implementation. In batched processing, sequences have different lengths, so we pad shorter sequences with special tokens. The attention mask tells us which tokens are real content versus padding. The code finds the last non-padding token for each sequence in the batch and extracts the hidden state at that position. This ensures we're always evaluating based on the actual end of the conversation, not padding tokens. Finally, this rich hidden representation is projected through the simple linear reward head to produce a single scalar score.\n",
    "\n",
    "#### The RewardModelTrainer Class Deep Dive\n",
    "\n",
    "The RewardModelTrainer class is where the reward model actually learns human preferences. The initialization is straightforward - it takes the reward model and sets up an optimizer to train it. The choice of AdamW optimizer with a small learning rate (1e-5) is deliberate because we're fine-tuning a large pretrained model and want stable, gradual learning.\n",
    "\n",
    "The create_preference_dataset method is particularly interesting because it simulates the human preference collection process. In real RLHF implementations, humans would manually compare responses and indicate preferences. Here, we automate this by generating multiple responses to the same prompt using our SFT model with high temperature (0.8) to ensure diversity. The higher temperature makes the sampling more random, so we get varied responses that can be meaningfully compared.\n",
    "\n",
    "For each prompt, the method generates several responses, then creates all possible pairwise comparisons between them. The _simple_quality_score function serves as our simulated human evaluator. It implements heuristics that reflect what humans typically prefer: interactive responses with questions, engaging language, appropriate length, and good analogies. While this is simplified compared to real human judgment, it captures important patterns that help train the reward model effectively.\n",
    "\n",
    "The scoring system is quite thoughtful. Responses get points for containing questions (indicating interactivity), using engaging words like \"imagine\" or \"amazing,\" having appropriate length (not too short or too long), and using analogy words that suggest good explanations. Responses that are too short are penalized because they're often unhelpful. This creates a ranking system where more engaging, helpful responses score higher than dry, technical ones.\n",
    "\n",
    "Batch Processing and Training Mechanics\n",
    "The prepare_preference_batch method handles the crucial task of converting preference data into trainable batches. This is where the Bradley-Terry model framework becomes operational. Each preference pair (preferred and dispreferred response) needs to be tokenized and formatted consistently. The method formats both responses as complete conversations, then tokenizes them with padding and truncation to ensure uniform batch sizes.\n",
    "\n",
    "The batch structure is carefully designed with separate tokenized inputs for preferred and dispreferred responses, along with their attention masks. This allows the model to process both responses in the same forward pass, making training efficient. The maximum length of 512 tokens is a practical choice that balances computational efficiency with the ability to handle reasonably long conversations.\n",
    "\n",
    "The bradley_terry_loss function implements the mathematical heart of preference learning. The Bradley-Terry model assumes that if humans prefer response A over response B, then the reward model should assign a higher score to A than to B. The loss function encourages this by computing the difference between preferred and dispreferred rewards, then using log-sigmoid to convert this into a probability. The negative log-sigmoid loss means we're maximizing the probability that preferred responses score higher than dispreferred ones.\n",
    "\n",
    "#### Training Process and Optimization\n",
    "\n",
    "The train_step method orchestrates a single training iteration. It performs forward passes through both preferred and dispreferred responses, computes the Bradley-Terry loss, and updates the model parameters. The accuracy calculation is particularly insightful - it measures how often the model correctly predicts that preferred responses should score higher than dispreferred ones. This gives us an immediate sense of how well the model is learning human preferences.\n",
    "\n",
    "The metrics returned from each training step provide valuable insights into the learning process. The loss tells us how well the model is optimizing the preference ranking objective. The accuracy shows the percentage of correct preference predictions. The reward means for preferred and dispreferred responses help us understand if the model is learning to assign appropriate score ranges. The reward gap is especially important - it measures how well the model can distinguish between good and bad responses.\n",
    "\n",
    "The main training loop in the train method coordinates everything together. It processes the preference data into batches, then iterates through multiple epochs of training. The progress bar provides real-time feedback on training metrics, helping us monitor whether the model is learning effectively. The epoch-level summaries give us a high-level view of training progress.\n",
    "\n",
    "#### Evaluation and Practical Usage\n",
    "\n",
    "The evaluate_responses method demonstrates how the trained reward model will be used in practice. During PPO or other RL optimization, we need to quickly evaluate many potential responses to guide the policy toward better outputs. This method takes prompts and responses, formats them consistently, and returns numerical scores that quantify quality according to learned human preferences.\n",
    "\n",
    "The evaluation process puts the model in eval mode to disable dropout and other training-specific behaviors, ensuring consistent scoring. The no-grad context prevents gradient computation since we're only doing inference. Each prompt-response pair is formatted, tokenized, and passed through the reward model to get a score. These scores will later guide the RL optimization process.\n",
    "\n",
    "The save and load methods handle model persistence, which is crucial for practical deployment. The reward model state dictionary contains all the learned parameters, while the tokenizer must be saved separately to ensure consistent text processing. This modular saving approach allows the reward model to be loaded and used independently of the original training setup.\n",
    "\n",
    "The create_reward_model_example function demonstrates the complete end-to-end workflow. It shows how the reward model integrates with our existing classes - starting from a pretrained LLM, creating an SFT model, generating preference data, training the reward model, and finally evaluating its performance. This integration ensures that all components of our RLHF pipeline work together seamlessly.\n",
    "\n",
    "The example evaluation at the end provides a concrete demonstration of how the reward model distinguishes between different response qualities. Technical responses should score lower than engaging, child-friendly explanations, and the trained model should reflect these learned preferences in its numerical scores. This completes the transformation from human preference data to an automated evaluation system that can guide large-scale optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576dea8c",
   "metadata": {},
   "source": [
    "### Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a308803",
   "metadata": {},
   "source": [
    "#### Why these algorithms were needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271dbc3",
   "metadata": {},
   "source": [
    "Understanding why specialized optimization algorithms like TRPO, PPO, GRPO, and DPO became necessary requires diving deep into the fundamental challenges of policy optimization in reinforcement learning, especially when applied to the complex domain of language model alignment. The journey from basic policy gradient methods to sophisticated RLHF algorithms reveals a fascinating progression of mathematical insights and practical engineering solutions that address some of the most challenging problems in machine learning optimization.\n",
    "\n",
    "\n",
    "At its core, policy optimization in reinforcement learning seeks to find the optimal policy $\\pi^*$ that maximizes expected cumulative reward. The policy gradient theorem, first formalized by Sutton et al., provides the mathematical foundation for this optimization. For a parameterized policy $\\pi_\\theta$, the objective is to maximize:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "where $\\tau$ represents a trajectory (sequence of state-action pairs) and $R(\\tau)$ is the cumulative reward. The policy gradient theorem tells us that:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\hat{A}_t\\right]$$\n",
    "\n",
    "This equation is the policy gradient theorem formula, which is fundamental to policy optimization in reinforcement learning. It states that the gradient of the expected reward objective function equals the expected value of the sum of policy gradients at each time step, weighted by advantage estimates. In simpler terms, it tells us how to adjust the policy parameters to increase the probability of actions that lead to better-than-expected outcomes (positive advantage) and decrease the probability of actions that lead to worse-than-expected outcomes (negative advantage). This provides the mathematical foundation for how reinforcement learning algorithms can systematically improve a policy through gradient-based optimization.\n",
    "\n",
    "This elegant formulation suggests we can optimize policies by following the gradient of expected rewards. However, this seemingly straightforward approach conceals profound computational and statistical challenges that become especially acute when applied to language models.\n",
    "\n",
    "The basic REINFORCE algorithm implements this gradient estimate directly, but it suffers from catastrophically high variance. In language models, where a single trajectory might involve generating hundreds of tokens, each with tens of thousands of possible choices, the variance of gradient estimates becomes so large that learning becomes practically impossible. The fundamental issue lies in the credit assignment problem: when a complete response receives a reward (positive or negative), which of the hundreds of preceding token choices actually contributed to that outcome?\n",
    "\n",
    "Consider our quantum computing explanation scenario. If our model generates: \"Hey there! Imagine quantum computers as magical calculators that can solve many problems simultaneously by existing in multiple states at once, kind of like a spinning coin that's both heads and tails until it stops!\" and receives a high reward score, the basic policy gradient has no principled way to determine whether the reward should be attributed to the engaging opening (\"Hey there!\"), the analogy choice (\"magical calculators\"), the technical accuracy (\"multiple states at once\"), or the relatable comparison (\"spinning coin\"). This attribution ambiguity leads to gradient estimates with enormous variance, making stable learning nearly impossible.\n",
    "\n",
    "##### The Challenge of Sample Efficiency\n",
    "\n",
    "Language model training presents unique sample efficiency challenges that traditional RL algorithms weren't designed to handle. Unlike robotics or game-playing scenarios where millions of interactions might be feasible, generating text samples from large language models is computationally expensive. Each forward pass through a model with billions of parameters requires significant computation, and generating diverse, high-quality responses for preference learning demands even more resources.\n",
    "\n",
    "Furthermore, the evaluation bottleneck compounds this efficiency problem. While traditional RL environments provide immediate reward signals (hitting a wall gives instant negative feedback), language model evaluation requires either expensive human annotation or carefully trained reward models. This sparse reward structure means we must extract maximum learning signal from every expensive sample we generate.\n",
    "\n",
    "The mathematical challenge becomes clear when we examine the variance of policy gradient estimators. For a policy gradient estimate $\\hat{g}$, the variance scales approximately as:\n",
    "\n",
    "$$\\text{Var}[\\hat{g}] \\propto \\frac{1}{N} \\sum_{t=0}^T \\text{Var}[R_t]$$\n",
    "\n",
    "where $N$ is the sample size and $T$ is the trajectory length. In language models, $T$ can be hundreds of tokens, and $\\text{Var}[R_t]$ can be enormous due to the discrete, high-dimensional action space. Traditional variance reduction techniques like baselines help but aren't sufficient for the scale of the problem.\n",
    "\n",
    "##### On-Policy vs. Off-Policy Learning: A Critical Distinction\n",
    "\n",
    "The distinction between on-policy and off-policy learning becomes crucial when we consider the practical constraints of language model training. On-policy methods require that training data comes from the current policy being optimized. Every time we update our model parameters, all previously collected data becomes \"stale\" because it was generated from a different policy. This means we must constantly generate fresh samples, making on-policy methods sample-inefficient but ensuring that our gradient estimates remain unbiased.\n",
    "\n",
    "Mathematically, on-policy methods maintain the condition:\n",
    "\n",
    "$$\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{current}}}}[\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\hat{A}_t]$$\n",
    "\n",
    "This expectation is taken with respect to the current policy, ensuring unbiased gradient estimates but requiring constant sample generation.\n",
    "\n",
    "Off-policy methods, conversely, can reuse data collected from previous policy versions through importance sampling corrections:\n",
    "\n",
    "$$\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\frac{\\pi_{\\theta_{\\text{current}}}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\hat{A}_t\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "The importance sampling ratio $\\frac{\\pi_{\\theta_{\\text{current}}}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ theoretically corrects for the distribution mismatch, but in practice, this ratio can become extremely large or small, leading to high variance that negates the sample efficiency benefits.\n",
    "\n",
    "In language models, this trade-off becomes particularly stark. The computational cost of generating samples is so high that off-policy methods seem attractive, but the high-dimensional discrete action space makes importance sampling corrections notoriously unstable. A single token change can dramatically shift the probability of entire sequences, leading to importance weights that vary by orders of magnitude.\n",
    "\n",
    "##### Why Naive Optimization Fails\n",
    "\n",
    "Perhaps the most fundamental challenge that necessitated specialized algorithms is the stability problem inherent in direct policy optimization. Language models exist in extremely high-dimensional parameter spaces—modern models have billions of parameters—and the loss landscape is notoriously non-convex with many local minima and saddle points.\n",
    "\n",
    "When we apply basic policy gradients to language models, small parameter updates can lead to catastrophic changes in policy behavior. This happens because the probability of generating any specific sequence depends on the product of probabilities across all tokens in that sequence:\n",
    "\n",
    "$$P_\\theta(y|x) = \\prod_{t=1}^T \\pi_\\theta(y_t|x, y_{<t})$$\n",
    "\n",
    "This equation represents the probability of generating a complete response sequence (y) given a prompt (x), which equals the product of probabilities for each individual token choice. It shows that even small changes in individual token probabilities can compound exponentially across the sequence length, potentially causing dramatic shifts in the model's overall behavior. This mathematical reality explains why naive optimization methods can lead to unstable training when applied to language models.\n",
    "\n",
    "Even small changes to individual token probabilities can compound exponentially across the sequence, leading to dramatic shifts in the overall policy. In our quantum computing example, a small parameter change that slightly increases the probability of technical terms might completely alter the model's tendency to use child-friendly language, undermining months of training.\n",
    "\n",
    "This sensitivity is mathematically captured by the condition number of the optimization problem. For policy optimization, we're essentially trying to solve:\n",
    "\n",
    "$$\\theta^* = \\arg\\max_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "The curvature of this objective function—measured by its Hessian—can vary dramatically across the parameter space. Regions where the policy changes rapidly with parameter adjustments correspond to high curvature, making gradient-based optimization unstable without careful step size control.\n",
    "\n",
    "\n",
    "#####  Why This Matters for AI Safety\n",
    "\n",
    "The development of these optimization algorithms isn't just a technical curiosity—it represents crucial progress toward building AI systems that can be reliably aligned with human values and preferences. The mathematical rigor required to make these algorithms work reveals deep insights about the nature of learning from human feedback and the challenges of encoding complex human preferences into computational systems.\n",
    "\n",
    "The stability guarantees provided by trust region methods ensure that AI systems won't suddenly exhibit catastrophic behavioral changes during training. The sample efficiency improvements enable learning from limited human feedback, making it feasible to train large-scale systems without requiring prohibitive amounts of human annotation. The theoretical foundations provide confidence that these systems will behave predictably and safely as they scale to larger models and more complex tasks.\n",
    "\n",
    "As we dive into the specific algorithms in the following sections, we'll see how each method addresses these fundamental challenges through different mathematical approaches and engineering trade-offs. The progression from basic policy gradients to sophisticated RLHF algorithms represents one of the most important developments in modern machine learning, with implications that extend far beyond language model training to the broader challenge of building AI systems that are helpful, harmless, and honest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b0ede",
   "metadata": {},
   "source": [
    "#### Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5afbf",
   "metadata": {},
   "source": [
    "<img src=\"assets/trpo.png\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316d5c6",
   "metadata": {},
   "source": [
    "\n",
    "Trust Region Policy Optimization (TRPO), introduced by Schulman et al. in 2015, represents a pivotal breakthrough in reinforcement learning that directly addresses the fundamental instability problems plaguing vanilla policy gradient methods. To understand TRPO's revolutionary approach, let's revisit our quantum computing explanation scenario and see how it transforms the chaotic landscape of policy optimization into a controlled, mathematically principled learning process.\n",
    "\n",
    "**The Core Problem TRPO Solves**\n",
    "\n",
    "Imagine our language model is like a student learning to explain quantum computing to children. In vanilla policy gradient methods, each update is like giving the student feedback and saying \"adjust your teaching style,\" but without any constraints on how drastically they should change. The student might completely abandon their previous approach overnight—going from using simple analogies to suddenly employing advanced mathematical notation—creating catastrophic instability in learning.\n",
    "\n",
    "The fundamental issue lies in the relationship between policy updates and performance guarantees. When we update our policy from $\\pi_{\\theta_{\\text{old}}}$ to $\\pi_{\\theta}$, vanilla policy gradient methods provide no guarantees about how the new policy will perform. A small change in parameters might lead to dramatically different token probabilities, and since language generation depends on the product of probabilities across all tokens:\n",
    "\n",
    "$$P_\\theta(y|x) = \\prod_{t=1}^T \\pi_\\theta(y_t|x, y_{<t})$$\n",
    "\n",
    "Even tiny parameter changes can compound exponentially, leading to completely different responses. In our quantum explanation example, a small increase in the probability of technical terms early in the sequence might cascade through the entire response, transforming a child-friendly explanation into an incomprehensible academic discourse.\n",
    "\n",
    "**The Trust Region Insight**\n",
    "\n",
    "TRPO's breakthrough insight is remarkably intuitive: policy gradient estimates are only locally valid. Just as a map of your neighborhood won't help you navigate across the country, gradient estimates computed at policy $\\pi_{\\theta_{\\text{old}}}$ are only reliable in a small \"neighborhood\" around that policy. Step too far away, and the gradient information becomes meaningless or even misleading.\n",
    "\n",
    "This leads to TRPO's central principle: constrain policy updates to remain within a \"trust region\" where our gradient estimates are reliable. In our teaching analogy, this means allowing our student to refine their explanation techniques gradually, ensuring they don't abandon effective strategies too quickly while still making steady progress toward better teaching.\n",
    "\n",
    "**Mathematical Foundation: The Surrogate Objective**\n",
    "\n",
    "TRPO begins with the policy gradient theorem but reformulates it using importance sampling to create a surrogate objective that can be optimized using data from the old policy. The standard policy gradient objective:\n",
    "\n",
    "$$\\eta(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^T R_t\\right]$$\n",
    "\n",
    "is transformed into the surrogate objective:\n",
    "\n",
    "$$L^{\\pi_{\\theta_{\\text{old}}}}(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\sum_{t=0}^T \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t)\\right]$$\n",
    "\n",
    "where:\n",
    "- $\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the importance sampling ratio\n",
    "- $A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t)$ is the advantage function under the old policy\n",
    "- The expectation is taken over trajectories from the old policy\n",
    "\n",
    "This formulation is brilliant because it allows us to estimate policy improvements using data we've already collected, rather than requiring new samples after each parameter update.\n",
    "\n",
    "**The Advantage Function in Language Models**\n",
    "\n",
    "The advantage function $A^\\pi(s,a)$ measures how much better taking action $a$ in state $s$ is compared to the average action the policy would take. In our quantum explanation context:\n",
    "\n",
    "$$A^\\pi(\\text{state}, \\text{action}) = Q^\\pi(\\text{state}, \\text{action}) - V^\\pi(\\text{state})$$\n",
    "\n",
    "For example:\n",
    "- **State**: \"Explain quantum computing to a 10-year-old: Think of\"\n",
    "- **Action A**: \"superposition\" (technical term)\n",
    "- **Action B**: \"magic\" (child-friendly term)\n",
    "\n",
    "If $A^\\pi(\\text{state}, \\text{\"superposition\"}) = -2.3$ and $A^\\pi(\\text{state}, \\text{\"magic\"}) = +1.8$, this tells us that choosing \"magic\" leads to significantly better outcomes than the average action, while \"superposition\" performs worse than average.\n",
    "\n",
    "**The Trust Region Constraint**\n",
    "\n",
    "The key innovation of TRPO is constraining the KL divergence between consecutive policies:\n",
    "\n",
    "$$\\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_{\\text{old}}}}}[D_{KL}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s))] \\leq \\delta$$\n",
    "\n",
    "where:\n",
    "- $\\rho_{\\pi_{\\theta_{\\text{old}}}}$ is the state visitation distribution under the old policy\n",
    "- $\\delta$ is the trust region size (typically 0.01 or 0.02)\n",
    "- $D_{KL}$ is the Kullback-Leibler divergence\n",
    "\n",
    "This constraint ensures that the new policy doesn't deviate too dramatically from the old one. In KL divergence terms, if two policies are identical, $D_{KL} = 0$. As policies become more different, KL divergence increases, providing a natural measure of policy similarity.\n",
    "\n",
    "For our quantum explanation model, this means we can't suddenly switch from a child-friendly communication style to academic jargon. The constraint forces gradual refinement: improving word choices, adjusting analogies, and enhancing engagement while maintaining the overall communication approach.\n",
    "\n",
    "**The Constrained Optimization Problem**\n",
    "\n",
    "TRPO formulates policy optimization as a constrained optimization problem:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{maximize}} \\quad \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\sum_{t=0}^T \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t)\\right]$$\n",
    "\n",
    "$$\\text{subject to} \\quad \\mathbb{E}_{s \\sim \\rho_{\\pi_{\\theta_{\\text{old}}}}}[D_{KL}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s))] \\leq \\delta$$\n",
    "\n",
    "This is a constrained optimization problem where we want to maximize expected improvement while staying within the trust region. The mathematical beauty lies in how this formulation provides theoretical guarantees about monotonic improvement.\n",
    "\n",
    "**Theoretical Guarantees: The Policy Performance Bound**\n",
    "\n",
    "TRPO's theoretical foundation rests on a crucial theorem that provides a lower bound on policy performance. The theorem states that:\n",
    "\n",
    "$$\\eta(\\pi) \\geq L^{\\pi_{\\text{old}}}(\\pi) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$$\n",
    "\n",
    "where:\n",
    "- $\\eta(\\pi)$ is the true performance of policy $\\pi$\n",
    "- $L^{\\pi_{\\text{old}}}(\\pi)$ is the surrogate objective\n",
    "- $\\epsilon$ is the maximum advantage magnitude\n",
    "- $\\alpha = \\max_s D_{TV}(\\pi_{\\text{old}}(\\cdot|s) \\| \\pi(\\cdot|s))$ measures policy difference\n",
    "- $D_{TV}$ is the total variation distance\n",
    "\n",
    "This bound guarantees that if we improve the surrogate objective while keeping the policy change small, we're guaranteed to improve the true policy performance. This is the mathematical foundation that makes TRPO's trust region approach theoretically sound.\n",
    "\n",
    "**From Theory to Practice: The Conjugate Gradient Solution**\n",
    "\n",
    "The constrained optimization problem is theoretically elegant but computationally challenging. Solving it exactly would require computing second-order derivatives (the Hessian) of the KL constraint, which is prohibitively expensive for large language models with billions of parameters.\n",
    "\n",
    "TRPO's practical brilliance lies in its approximation method. It linearizes the objective and uses a quadratic approximation of the constraint:\n",
    "\n",
    "1. **Linear approximation of objective**: $L^{\\pi_{\\theta_{\\text{old}}}}(\\theta) \\approx L^{\\pi_{\\theta_{\\text{old}}}}(\\theta_{\\text{old}}) + g^T(\\theta - \\theta_{\\text{old}})$\n",
    "\n",
    "2. **Quadratic approximation of constraint**: $\\mathbb{E}[D_{KL}] \\approx \\frac{1}{2}(\\theta - \\theta_{\\text{old}})^T H (\\theta - \\theta_{\\text{old}})$\n",
    "\n",
    "where $g$ is the policy gradient and $H$ is the Hessian of the KL divergence.\n",
    "\n",
    "This leads to the closed-form solution:\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^T H^{-1} g}} H^{-1} g$$\n",
    "\n",
    "The term $H^{-1} g$ is the \"natural gradient\" direction, and the scaling factor ensures we stay within the trust region.\n",
    "\n",
    "**Conjugate Gradient Algorithm**\n",
    "\n",
    "Computing $H^{-1}$ directly is still too expensive, so TRPO uses the conjugate gradient algorithm to solve $Hx = g$ iteratively. This requires only matrix-vector products $Hv$, which can be computed efficiently using automatic differentiation:\n",
    "\n",
    "$$Hv = \\nabla_\\theta \\left(\\left(\\nabla_\\theta \\mathbb{E}[D_{KL}]\\right)^T v\\right)$$\n",
    "\n",
    "The conjugate gradient algorithm finds the solution $x = H^{-1}g$ without explicitly computing the inverse matrix, making the computation feasible for large neural networks.\n",
    "\n",
    "**Line Search and Backtracking**\n",
    "\n",
    "Even with the trust region constraint, TRPO includes an additional safety mechanism: backtracking line search. After computing the proposed update direction, TRPO tests the actual policy improvement and backs off if necessary:\n",
    "\n",
    "1. Compute proposed update: $\\theta_{\\text{proposed}} = \\theta_{\\text{old}} + \\alpha \\sqrt{\\frac{2\\delta}{g^T H^{-1} g}} H^{-1} g$\n",
    "2. Check KL constraint: $D_{KL}(\\pi_{\\theta_{\\text{old}}}, \\pi_{\\theta_{\\text{proposed}}}) \\leq \\delta$\n",
    "3. Check improvement: $\\eta(\\pi_{\\theta_{\\text{proposed}}}) > \\eta(\\pi_{\\theta_{\\text{old}}})$\n",
    "4. If either fails, reduce $\\alpha$ and try again\n",
    "\n",
    "**TRPO in Language Model Context**\n",
    "\n",
    "When applied to our quantum explanation model, TRPO's process looks like this:\n",
    "\n",
    "1. **Current State**: Model generates technical explanations\n",
    "2. **Collect Trajectories**: Generate multiple quantum explanations using current policy\n",
    "3. **Compute Advantages**: Reward model scores indicate which responses are better\n",
    "4. **Trust Region Update**: Adjust parameters to increase probability of good responses while staying close to current policy\n",
    "5. **Verification**: Ensure new policy hasn't changed too dramatically\n",
    "\n",
    "For example, if the current policy generates: \"Quantum computers utilize quantum superposition...\" and this receives a low reward, TRPO would gradually increase the probability of more engaging openings like \"Hey there! Imagine...\" without completely abandoning the model's existing knowledge.\n",
    "\n",
    "**Why TRPO Was Groundbreaking**\n",
    "\n",
    "TRPO represented a paradigm shift in several key ways:\n",
    "\n",
    "1. **Theoretical Guarantees**: Unlike previous methods, TRPO provides mathematical guarantees about monotonic improvement, ensuring that each update makes the policy better (or at least no worse).\n",
    "\n",
    "2. **Sample Efficiency**: By reusing data from the old policy through importance sampling, TRPO achieved better sample efficiency than on-policy methods that discard old data.\n",
    "\n",
    "3. **Stability**: The trust region constraint prevented the catastrophic policy collapses that plagued vanilla policy gradient methods.\n",
    "\n",
    "4. **Generality**: TRPO worked across diverse domains, from robotics to game playing to (eventually) language modeling.\n",
    "\n",
    "**Limitations and Computational Challenges**\n",
    "\n",
    "Despite its theoretical elegance, TRPO has significant practical limitations:\n",
    "\n",
    "1. **Computational Complexity**: The conjugate gradient algorithm and line search make each update computationally expensive, requiring multiple forward and backward passes.\n",
    "\n",
    "2. **Hyperparameter Sensitivity**: The trust region size $\\delta$ requires careful tuning. Too small, and learning is slow; too large, and the approximations break down.\n",
    "\n",
    "3. **Implementation Complexity**: TRPO requires implementing conjugate gradient, Hessian-vector products, and line search, making it significantly more complex than simpler alternatives.\n",
    "\n",
    "4. **Scalability Issues**: For very large language models, even the conjugate gradient approach can be prohibitively expensive.\n",
    "\n",
    "**The Path to PPO**\n",
    "\n",
    "TRPO's limitations directly motivated the development of Proximal Policy Optimization (PPO), which we'll explore next. PPO achieves similar stability guarantees with a much simpler implementation, making it the preferred choice for most practical applications, including large-scale language model training.\n",
    "\n",
    "However, understanding TRPO is crucial because:\n",
    "- It established the theoretical foundations for trust region methods\n",
    "- It demonstrated the importance of constraining policy updates\n",
    "- It introduced key concepts like surrogate objectives and natural gradients\n",
    "- It provided the mathematical intuition that guides more practical algorithms\n",
    "\n",
    "In our quantum computing education example, TRPO would gradually transform our model from generating technical academic responses to producing engaging, child-friendly explanations through a series of constrained, theoretically guaranteed improvements. Each step would maintain the model's core knowledge while progressively aligning its communication style with human preferences for age-appropriate education.\n",
    "\n",
    "The mathematical rigor of TRPO established trust region methods as a cornerstone of modern reinforcement learning, providing the theoretical foundation upon which practical algorithms like PPO build their simplified yet effective approaches to policy optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29c2a2",
   "metadata": {},
   "source": [
    "In policy gradient algorithms, update steps computed at any specific policy $\\pi_{\\theta}$ are only really \"predictive\" in the neighborhood of $\\theta_t$. That is, it is probable that updates outside of this neighborhood may not contain any predictive value at all. Intuitively, you may then think of constraining updates so that they do stay in the vicinity of our current policy.\n",
    "\n",
    "Since the policy is a probability distribution over (state, action) pairs, we refer to it as the \"policy space\". Trust region methods (Kakade, 2001; Schulman et al., 2015a; 2017) aim to restrict how far successive policies are allowed to deviate. \n",
    "\n",
    "In Trust Region Policy Optimization(Schulman et al., 2015a), this is achieved by minimizing the KL divergence between successive policies on the optimization trajectory. Let $\\hat{A_{\\pi}}(s_t, a_t)$ be the estimated advantage function where the agent picks action $a$ at time $t$ given he is at state $s$ while following a policy $\\pi_{\\theta}$. Let $\\pi(a_t|s_t)$ be the old policy where we take action $a$ given we are in state $s$ at time $t$. $\\pi_{\\theta}(a_t|s_t)$ is our current (parameterized) policy which we seek to update.\n",
    "\n",
    "Define the optimization problem as:\n",
    "$$\\underset{\\theta}{\\max}\\ \\mathbb{E}_{(s_t, a_t)\\sim \\pi} \\left[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi(a_t|s_t)}\\hat{A_{\\pi}}(s_t, a_t)\\right]\\\\ \\\\\n",
    "\\text{subject to } D_{KL}(\\pi_{\\theta}(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))\\leq\\delta, \\ \\forall s\n",
    "$$\n",
    "The \"subject to\" line is essentially an assumption that we will have to improve. It is saying that our optimization problem will abide by the goal of having the KL divergence between the two policies becoming less than some small $\\delta$.\n",
    "\n",
    "This theoretical update is not easy to compute. Thus, TRPO makes approximations by reformulating both the loss function $\\mathcal{L}(\\theta_k, \\theta)$ and the KL divergence $D_KL(\\theta||\\theta_k)$ to give an easier-to-compute approximation of the objective. Furthermore, this approximate objective is then able to be solved using Lagrangian duality to yield the update:\n",
    "$$\\theta_{k+1}=\\theta_k+\\alpha^j\\sqrt{\\frac{2\\delta}{g^TH^{-1}g}}H^{-1}g$$\n",
    "Since the Hessian inverse $H^{-1}$ is expensive to compute, TRPO utilizes the conjugate gradient algorithm to solve $Hx=g$ (or $x=H^{-1}g$) which requires a function for computing $Hx$ instead of computing and storing the entire matrix $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bd4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE IMPLEMENTATION TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f01e99",
   "metadata": {},
   "source": [
    "CODE EXPLANATION TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700002b",
   "metadata": {},
   "source": [
    "#### Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb3bed",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO), introduced by Schulman et al. in 2017, represents one of the most significant practical breakthroughs in reinforcement learning. While TRPO established the theoretical foundations for trust region methods, PPO achieved something equally important: making these powerful ideas simple, efficient, and implementable at scale. To understand PPO's revolutionary impact, let's continue with our quantum computing explanation scenario and see how it transforms TRPO's complex mathematics into an elegant, practical solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c84905",
   "metadata": {},
   "source": [
    "\n",
    "**PPO's Brilliant Insight: First-Order Trust Regions**\n",
    "\n",
    "PPO's breakthrough came from a deceptively simple question: \"What if we could achieve TRPO's stability guarantees using only first-order gradient information?\" Instead of computing complex trust region constraints exactly, PPO approximates them using a much simpler clipping mechanism that achieves similar goals with dramatically reduced computational cost.\n",
    "\n",
    "The key insight is that we don't need to solve the constrained optimization problem exactly—we just need to prevent policy updates from being too large. PPO achieves this through a clever surrogate objective that automatically limits the magnitude of policy changes.\n",
    "\n",
    "**The PPO-Clip Objective**\n",
    "\n",
    "PPO replaces TRPO's complex constrained optimization with a simple clipped surrogate objective. For each state-action pair, PPO computes the probability ratio:\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$$\n",
    "\n",
    "This ratio measures how much the new policy $\\pi_\\theta$ differs from the old policy $\\pi_{\\theta_{\\text{old}}}$ for a specific action. When $r_t(\\theta) = 1$, the policies are identical for that action. When $r_t(\\theta) > 1$, the new policy assigns higher probability to the action, and when $r_t(\\theta) < 1$, it assigns lower probability.\n",
    "\n",
    "The clipped objective is then defined as:\n",
    "\n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "where:\n",
    "- $A_t$ is the advantage function at time $t$\n",
    "- $\\epsilon$ is the clipping parameter (typically 0.1 or 0.2)\n",
    "- $\\text{clip}(x, a, b)$ constrains $x$ to the interval $[a, b]$\n",
    "\n",
    "**Case 1: Positive Advantage ($A_t > 0$)**\n",
    "When an action leads to better-than-expected outcomes, we want to increase its probability. However, PPO prevents us from increasing it too much:\n",
    "\n",
    "- If $r_t(\\theta) < 1 + \\epsilon$: The objective becomes $r_t(\\theta) A_t$, encouraging normal updates\n",
    "- If $r_t(\\theta) > 1 + \\epsilon$: The objective becomes $(1 + \\epsilon) A_t$, preventing excessive increases\n",
    "\n",
    "For example, if our model discovers that starting with \"Hey there!\" leads to much better engagement scores, PPO will increase the probability of this opening, but not so much that it dominates all other possibilities.\n",
    "\n",
    "**Case 2: Negative Advantage ($A_t < 0$)**\n",
    "When an action leads to worse-than-expected outcomes, we want to decrease its probability, but again with limits:\n",
    "\n",
    "- If $r_t(\\theta) > 1 - \\epsilon$: The objective becomes $r_t(\\theta) A_t$, encouraging normal updates\n",
    "- If $r_t(\\theta) < 1 - \\epsilon$: The objective becomes $(1 - \\epsilon) A_t$, preventing excessive decreases\n",
    "\n",
    "If our model finds that using technical jargon like \"superposition\" in child explanations gets poor scores, PPO will decrease its probability, but won't eliminate it entirely in a single update.\n",
    "\n",
    "The clipping mechanism is mathematically elegant because it creates a pessimistic (conservative) objective. The $\\min$ operation always chooses the smaller of the two terms, preventing overoptimistic updates. This conservatism provides implicit trust region enforcement without requiring complex second-order computations.\n",
    "\n",
    "To understand why this works, consider the relationship between the clipped objective and policy performance. When we clip the importance sampling ratios, we're effectively limiting how much the policy can change in a single update. This prevents the policy from making large, potentially destabilizing jumps while still allowing meaningful progress toward better performance.\n",
    "\n",
    "Now let's walk through the complete PPO workflow step by step, following the training pipeline that makes this objective functional:\n",
    "\n",
    "<img src=\"assets/ppo2.png\">\n",
    "\n",
    "<img src=\"assets/ppo.png\">\n",
    "\n",
    "The PPO training process follows a systematic workflow that transforms our quantum explanation model through iterative improvements. Let's trace through each component of this pipeline:\n",
    "\n",
    "##### Step 1: Trajectory Collection with Current Policy (or Current Actor)\n",
    "\n",
    "The first step in each PPO iteration involves generating multiple quantum explanations using our current policy $\\pi_{\\theta_{\\text{old}}}$. This is where our model produces diverse responses that will serve as training data for the next update.\n",
    "\n",
    "**Process:**\n",
    "For our quantum computing explanation task, we present the model with prompts like \"Explain quantum computing to a 10-year-old\" and generate multiple responses:\n",
    "\n",
    "- **Response 1**: \"Quantum computers are special machines that use quantum bits...\"\n",
    "- **Response 2**: \"Hey there! Imagine quantum computers as magical calculators...\"\n",
    "- **Response 3**: \"Think of quantum computing like having a coin that spins...\"\n",
    "\n",
    "**Mathematical Framework:**\n",
    "Each trajectory $\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T, a_T)$ represents a complete conversation where:\n",
    "- $s_t$ is the context (prompt + tokens generated so far)\n",
    "- $a_t$ is the next token chosen\n",
    "- The policy $\\pi_{\\theta_{\\text{old}}}(a_t|s_t)$ determines token selection probabilities\n",
    "\n",
    "**Key Implementation Details:**\n",
    "- We use sampling (not greedy decoding) to ensure diversity\n",
    "- Temperature controls exploration vs exploitation\n",
    "- We collect multiple trajectories per prompt to capture response variety\n",
    "- Each trajectory includes the complete conversation history\n",
    "\n",
    "##### Step 2: Reward Model Evaluation (or Current Actor)\n",
    "\n",
    "Once we have collected trajectories, each complete response is evaluated using our trained reward model to get scalar reward scores.\n",
    "\n",
    "**Reward Assignment Process:**\n",
    "Our reward model $r_\\theta(x, y)$ evaluates each prompt-response pair:\n",
    "\n",
    "For our quantum explanation examples:\n",
    "- **Response 1**: Technical explanation → Reward: 3.2\n",
    "- **Response 2**: Engaging with analogies → Reward: 7.8  \n",
    "- **Response 3**: Simple analogy → Reward: 6.5\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "The reward model outputs scalar scores based on human preferences learned during reward model training:\n",
    "$$r_\\theta(x, y) = \\text{RewardModel}(x \\oplus y)$$\n",
    "\n",
    "where $x \\oplus y$ represents the concatenated prompt and response.\n",
    "\n",
    "**Sparse Reward Challenge:**\n",
    "Unlike traditional RL where rewards occur throughout the trajectory, language models receive rewards only at sequence completion. This creates the credit assignment problem that PPO's advantage estimation must solve.\n",
    "\n",
    "##### Step 3: Value Function (or Critic Function) Estimation and Advantage Computation\n",
    "\n",
    "The value function $V^\\pi(s)$ estimates the expected future reward from any given state, while advantages $A^\\pi(s,a)$ measure how much better a specific action is compared to the average.\n",
    "\n",
    "**Value Function Role:**\n",
    "$$V^\\pi(s_t) = \\mathbb{E}_\\pi\\left[\\sum_{k=t}^T \\gamma^{k-t} r_k \\mid s_t\\right]$$\n",
    "\n",
    "For our quantum explanation:\n",
    "- $V(\"Explain quantum computing: \")$ = 5.2 (expected reward for this starting state)\n",
    "- $V(\"Explain quantum computing: Hey there!\")$ = 7.1 (higher expectation after engaging start)\n",
    "\n",
    "**Generalized Advantage Estimation (GAE)**\n",
    "\n",
    "PPO uses GAE to compute advantages with an optimal bias-variance trade-off:\n",
    "\n",
    "$$\\hat{A}_t^{\\text{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the temporal difference error.\n",
    "\n",
    "**GAE Parameter Impact:**\n",
    "- $\\lambda = 0$: Uses only 1-step TD estimates (low variance, higher bias)\n",
    "- $\\lambda = 1$: Uses full Monte Carlo returns (high variance, no bias)\n",
    "- $\\lambda = 0.95$ (typical): Balances bias and variance effectively\n",
    "\n",
    "**Advantage Interpretation in Language Models:**\n",
    "- Positive advantage: This token choice leads to better outcomes than average\n",
    "- Negative advantage: This token choice leads to worse outcomes than average\n",
    "- Zero advantage: This token choice is about average\n",
    "\n",
    "Example for our quantum explanation:\n",
    "- $A(\"Explain quantum: \", \\text{\"Hey\"}) = +1.8$ (engaging start is better than average)\n",
    "- $A(\"Explain quantum: \", \\text{\"Quantum\"}) = -0.9$ (technical start is worse than average)\n",
    "\n",
    "##### Step 4: Policy Optimization with Clipped Objective\n",
    "\n",
    "Now we reach the heart of PPO: the clipped surrogate objective that enables stable policy improvements.\n",
    "\n",
    "**The Clipped Surrogate Loss:**\n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the probability ratio.\n",
    "\n",
    "**Clipping Mechanism Deep Dive:**\n",
    "\n",
    "**Scenario 1: Positive Advantage (Good Actions)**\n",
    "When $A_t > 0$, we want to increase the probability of this action:\n",
    "\n",
    "```\n",
    "If r_t(θ) ≤ 1 + ε: Use r_t(θ)A_t (normal update)\n",
    "If r_t(θ) > 1 + ε: Use (1 + ε)A_t (clip to prevent over-optimization)\n",
    "```\n",
    "\n",
    "In our quantum example, if \"Hey there!\" receives positive advantage, PPO increases its probability but prevents it from dominating completely.\n",
    "\n",
    "**Scenario 2: Negative Advantage (Bad Actions)**  \n",
    "When $A_t < 0$, we want to decrease the probability of this action:\n",
    "\n",
    "```\n",
    "If r_t(θ) ≥ 1 - ε: Use r_t(θ)A_t (normal update)\n",
    "If r_t(θ) < 1 - ε: Use (1 - ε)A_t (clip to prevent over-penalization)\n",
    "```\n",
    "\n",
    "If \"superposition\" receives negative advantage in child explanations, PPO decreases its probability gradually rather than eliminating it entirely.\n",
    "\n",
    "**Why Clipping Works:**\n",
    "The clipping creates a pessimistic (conservative) objective that prevents policy collapse. The $\\min$ operation always chooses the more conservative update, ensuring stability.\n",
    "\n",
    "##### Step 5: Value Function Training\n",
    "\n",
    "Parallel to policy optimization, PPO trains the value function to better estimate state values for future advantage computations.\n",
    "\n",
    "**Value Function Loss with Clipping:**\n",
    "$$L^{VF}(\\theta) = \\mathbb{E}_t\\left[\\max\\left((V_\\theta(s_t) - V_t^{\\text{targ}})^2, (\\text{clip}(V_\\theta(s_t), V_{\\text{old}} - \\epsilon, V_{\\text{old}} + \\epsilon) - V_t^{\\text{targ}})^2\\right)\\right]$$\n",
    "\n",
    "**Why Clip the Value Function?**\n",
    "Just as we clip policy updates, we clip value function updates to prevent large jumps that could destabilize advantage estimation. This ensures that our critic (value function) learns smoothly alongside our actor (policy).\n",
    "\n",
    "**Target Computation:**\n",
    "The target values $V_t^{\\text{targ}}$ are computed using the GAE returns:\n",
    "$$V_t^{\\text{targ}} = \\hat{A}_t + V(s_t)$$\n",
    "\n",
    "This provides supervised targets for value function regression.\n",
    "\n",
    "##### Step 6: Entropy Regularization\n",
    "\n",
    "The entropy bonus encourages exploration by preventing the policy from becoming too deterministic:\n",
    "\n",
    "$$S[\\pi_\\theta](s) = \\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot|s)}\\left[-\\log \\pi_\\theta(a|s)\\right]$$\n",
    "\n",
    "**Entropy in Language Models:**\n",
    "For our quantum explanation model, entropy ensures response diversity:\n",
    "- High entropy: Model considers many possible continuations\n",
    "- Low entropy: Model strongly prefers specific continuations  \n",
    "\n",
    "**Practical Impact:**\n",
    "Without entropy regularization, our model might converge to generating identical \"optimal\" responses, losing the ability to adapt to different contexts or user preferences.\n",
    "\n",
    "##### Step 7: Combined Loss Optimization\n",
    "\n",
    "All three components combine in the final PPO objective:\n",
    "$$L(\\theta) = L^{\\text{CLIP}}(\\theta) - c_1 L^{VF}(\\theta) + c_2 S[\\pi_\\theta](s)$$\n",
    "\n",
    "**Coefficient Balancing:**\n",
    "- $c_1 = 0.5$: Balances policy and value learning\n",
    "- $c_2 = 0.01$: Provides gentle exploration pressure\n",
    "- These ratios ensure policy optimization dominates while maintaining value function accuracy and exploration\n",
    "\n",
    "**Multiple Epochs per Batch:**\n",
    "PPO performs 3-10 gradient updates on the same batch of data, dramatically improving sample efficiency. The clipping mechanism prevents over-optimization during these multiple passes.\n",
    "\n",
    "##### Step 8: Iterative Improvement\n",
    "\n",
    "The entire process repeats iteratively:\n",
    "\n",
    "**Iteration 1:**\n",
    "- Collect trajectories from current policy\n",
    "- Compute rewards and advantages  \n",
    "- Update policy with clipped objective\n",
    "- Result: Slightly better quantum explanations\n",
    "\n",
    "**Iteration 2:**\n",
    "- Use improved policy to collect new trajectories\n",
    "- Rewards should generally increase due to better responses\n",
    "- Continue optimization\n",
    "- Result: More engaging, child-friendly explanations\n",
    "\n",
    "**Iteration N:**\n",
    "- Policy has learned to generate consistently high-quality responses\n",
    "- Balances scientific accuracy with age-appropriate communication\n",
    "- Maintains diversity while optimizing for human preferences\n",
    "\n",
    "**Convergence Pattern:**\n",
    "Our quantum explanation model progressively evolves:\n",
    "\n",
    "1. **Initial**: Technical, dry explanations\n",
    "2. **Early training**: Begins using simpler language\n",
    "3. **Mid training**: Incorporates analogies and examples\n",
    "4. **Late training**: Masters engaging, interactive communication\n",
    "5. **Converged**: Consistently produces helpful, harmless, honest explanations\n",
    "\n",
    "**Example Evolution Sequence:**\n",
    "\n",
    "**Iteration 1**: \"Quantum computers use qubits in superposition states...\"  \n",
    "**Reward**: 3.1 → **Update**: Increase probability of simpler terms\n",
    "\n",
    "**Iteration 50**: \"Quantum computers are like special calculators that can try multiple solutions...\"  \n",
    "**Reward**: 6.4 → **Update**: Increase probability of interactive elements  \n",
    "\n",
    "**Iteration 200**: \"Hey there! Imagine quantum computers as magical problem-solvers. What's your favorite type of puzzle?\"  \n",
    "**Reward**: 8.8 → **Update**: Fine-tune for even better engagement\n",
    "\n",
    "**In essence:**\n",
    "\n",
    "When applied to our quantum explanation model, PPO's training process involves:\n",
    "\n",
    "1. **Trajectory Collection**: Generate multiple quantum explanations using the current policy\n",
    "2. **Advantage Computation**: Use the reward model and value function to compute advantages for each token choice\n",
    "3. **Policy Updates**: Apply the clipped objective to improve token selection probabilities\n",
    "4. **Value Updates**: Train the critic to better estimate state values\n",
    "5. **Multiple Epochs**: Repeat updates multiple times on the same batch (typically 3-10 epochs)\n",
    "\n",
    "\n",
    "**Why PPO Works So Well**\n",
    "\n",
    "PPO's success stems from several key advantages:\n",
    "\n",
    "**1. Computational Efficiency**\n",
    "PPO requires only first-order gradient information, making it 10-100 times faster than TRPO while achieving comparable performance. This efficiency enables training on large-scale language models.\n",
    "\n",
    "**2. Implementation Simplicity**\n",
    "The clipping mechanism is straightforward to implement and debug, unlike TRPO's complex conjugate gradient procedures. This simplicity has made PPO the de facto standard for many RL applications.\n",
    "\n",
    "**3. Hyperparameter Robustness**\n",
    "PPO works well with a standard set of hyperparameters across diverse tasks:\n",
    "- Clipping parameter $\\epsilon = 0.2$\n",
    "- Value function coefficient $c_1 = 0.5$\n",
    "- Entropy coefficient $c_2 = 0.01$\n",
    "- GAE parameter $\\lambda = 0.95$\n",
    "\n",
    "**4. Sample Efficiency**\n",
    "By performing multiple gradient updates on the same batch of data (typically 3-10 epochs), PPO achieves better sample efficiency than purely on-policy methods while maintaining stability.\n",
    "\n",
    "**PPO Variants and Extensions (Optional)**\n",
    "\n",
    "Since its introduction, PPO has spawned several important variants:\n",
    "\n",
    "**PPO-Penalty**: Instead of clipping, this variant adds a KL penalty term:\n",
    "$$L(\\theta) = \\mathbb{E}_t\\left[r_t(\\theta) A_t - \\beta D_{KL}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s_t) \\| \\pi_\\theta(\\cdot|s_t))\\right]$$\n",
    "\n",
    "**Adaptive PPO**: Dynamically adjusts the clipping parameter based on the observed KL divergence.\n",
    "\n",
    "**PPO with Experience Replay**: Combines PPO with off-policy learning to improve sample efficiency further.\n",
    "\n",
    "**PPO's Impact on RLHF**\n",
    "\n",
    "PPO's efficiency and stability made large-scale RLHF feasible for the first time. Its adoption enabled:\n",
    "\n",
    "- Training ChatGPT and GPT-4 with human feedback\n",
    "- Scaling RLHF to models with hundreds of billions of parameters\n",
    "- Making RLHF accessible to researchers and practitioners worldwide\n",
    "- Establishing the standard pipeline for aligning language models\n",
    "\n",
    "**Limitations and Challenges**\n",
    "\n",
    "Despite its success, PPO has some limitations:\n",
    "\n",
    "**1. Sample Efficiency**: While better than vanilla policy gradients, PPO still requires many samples compared to some off-policy methods.\n",
    "\n",
    "**2. Hyperparameter Sensitivity**: Although robust, PPO's performance can still be sensitive to the choice of clipping parameter and other hyperparameters.\n",
    "\n",
    "**3. Policy Collapse**: In some cases, PPO can lead to mode collapse where the policy becomes too deterministic.\n",
    "\n",
    "**4. Distribution Mismatch**: Like all on-policy methods, PPO suffers when there's a significant mismatch between the data distribution and the current policy.\n",
    "\n",
    "**Monotonic Improvement**: Under certain conditions, PPO can be shown to provide monotonic policy improvement.\n",
    "\n",
    "**Sample Complexity**: PPO achieves $O(1/\\epsilon)$ sample complexity for finding $\\epsilon$-optimal policies in certain settings.\n",
    "\n",
    "**Convergence**: With appropriate step sizes and clipping parameters, PPO converges to local optima of the policy gradient objective.\n",
    "\n",
    "**PPO in Modern AI Systems**\n",
    "\n",
    "Today, PPO remains the backbone of most large-scale RLHF systems:\n",
    "\n",
    "- **OpenAI GPT series**: Used PPO for alignment training\n",
    "- **Anthropic Claude**: Employs PPO-based techniques for constitutional AI\n",
    "- **Google Bard/Gemini**: Utilizes PPO-style optimization for safety training\n",
    "- **Open-source models**: Most open-source RLHF implementations use PPO\n",
    "\n",
    "However, PPO's combination of simplicity, efficiency, and effectiveness has made it remarkably durable. Even as new methods emerge, PPO often serves as the baseline and continues to be competitive with more complex alternatives.\n",
    "\n",
    "In our quantum computing education example, PPO would progressively refine our model through a series of efficient, clipped updates. Each update would cautiously improve the model's ability to engage children while avoiding the catastrophic policy changes that plagued earlier methods. The result would be a model that gradually learns to balance scientific accuracy with age-appropriate communication, creating explanations that are both correct and captivating.\n",
    "\n",
    "PPO's genius lies not in mathematical sophistication but in practical insight: sometimes the best solution is the simplest one that actually works at scale. By replacing TRPO's complex machinery with elegant clipping, PPO democratized trust region methods and made stable policy optimization accessible to the entire machine learning community. This accessibility has been crucial for the rapid development and deployment of aligned AI systems, making PPO one of the most influential algorithms in modern reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb4727",
   "metadata": {},
   "source": [
    "Let's start off by making a simple `PPOModel` class such that we can perform a `forward()` pass. \n",
    "\n",
    "In PPO, we refer to the policy model as the \"actor\" and the value model as the \"critic\".\n",
    "\n",
    "The value model outputs scalar values (scores) just like the reward model. Meanwhile the policy model outputs probability distributions (take the log and you get logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be87083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "from transformers import Trainer\n",
    "\n",
    "class PPOModel(nn.Module):\n",
    "    def __init__(self, actor_model, critic_model):\n",
    "        super().__init__()\n",
    "        self.actor_model = actor_model\n",
    "        self.critic_model = critic_model\n",
    "\n",
    "    def forward(self, sequences, extra_inputs=None):\n",
    "        # fetch logits from actor, fetch scalar reward from reference\n",
    "        actor_logits = self.actor_model(**sequences, return_dict=True).logits\n",
    "        critic_values = self.critic_model(**sequences)[-1]\n",
    "        \n",
    "        if extra_inputs is not None:\n",
    "            extra_loss = self.actor_model(**extra_inputs, return_dict=True).loss\n",
    "        else:\n",
    "            extra_loss = 0.0\n",
    "        return actor_logits, critic_values, extra_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3fd11",
   "metadata": {},
   "source": [
    "Now, navigate to the `src/ppo` folder to find `ppo_trainer.py`. This will contain the main code for our PPO algorithm.\n",
    "\n",
    "We will now go over each component of the `PPOTrainer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b591fd6",
   "metadata": {},
   "source": [
    "The KL divergence cannot be computed in its original form. \n",
    "\n",
    "The action space (token space) is far too vast for us to sum/integrate over all $x$.\n",
    "Furthermore, when we train, we don't store full probability distributions but rather log-probabilities of the tokens.\n",
    "Thus, it doesn't make sense to waste GPU memory on something we can avoid. \n",
    "So, it is necessary to estimate it (with Monte Carlo, for example). However, we have a plethora of KL estimators to choose from.\n",
    "\n",
    "Choosing the most \"optimal\" estimator is out of the scope of this notebook. We only implement the most popular estimators which have been tried and tested. \n",
    "\n",
    "Suppose $r=\\frac{\\pi (\\theta)}{\\pi_{\\text{old}}(\\theta)}$ is the ratio between the current policy and the old policy.\n",
    "Let $k$ denote an ambiguous KL estimator. Then, we have the following:\n",
    "\n",
    "$$k_3 = (r - 1) - \\log r$$\n",
    "$$k_{\\text{abs}}=|\\log r|$$\n",
    "$$k_{\\text{MSE}}=\\frac{1}{2}(\\log r)^2$$\n",
    "\n",
    "This might be up for debate, but the best KL estimator is $k_{\\text{MSE}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards_with_kl_penalty(self, ref_values, actor_log_probs, ref_log_probs, responses_mask):\n",
    "    \"\"\"\n",
    "    Computes rewards with the KL divergence penalty.\n",
    "    Includes implementations of KL estimators since we can't compute it exactly.\n",
    "    - k_3 is a popular KL estimator proposed here: http://joschu.net/blog/kl-approx.html\n",
    "\n",
    "    Args:\n",
    "        ref_values\n",
    "\n",
    "        actor_log_probs: torch.Tensor\n",
    "            log probabilities from our actor model\n",
    "\n",
    "        ref_log_probs: torch.Tensor\n",
    "            log probabilities from our reference model\n",
    "\n",
    "        responses_mask: torch.Tensor\n",
    "    \"\"\"\n",
    "    masks = responses_mask[:, 1:] \n",
    "    rewards_score = self.get_last_reward_score(ref_values, responses_mask)\n",
    "    \n",
    "    batch_size = rewards_score.shape[0]\n",
    "    rewards_with_kl_penalty, kl_penalty_all = [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        mask = masks[i]: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            \n",
    "        eos_mask: torch.Tensor\n",
    "\n",
    "        epsilon: torch.Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # this keeps our value predictions within some epsilon-distance\n",
    "    # mostly for numerical stability before performing regression\n",
    "    clip_value_preds = torch.clamp(value_preds, values - epsilon, values + epsilon)   \n",
    "\n",
    "    # thus, we have two errors, but it doesn't matter because we take the maximum (one)\n",
    "    values_error = (value_preds - returns) ** 2\n",
    "    clip_values_error = (clip_value_preds - returns) ** 2\n",
    "    \n",
    "    # this is essentially the inner sum for one trajectory t \\in D_k where D_k is set of trajectories\n",
    "    loss = 0.5 * masked_mean(torch\n",
    "        lp_a = actor_lo: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            \n",
    "        eos_mask: torch.Tensor\n",
    "\n",
    "        epsilon: torch.Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # this keeps our value predictions within some epsilon-distance\n",
    "    # mostly for numerical stability before performing regression\n",
    "    clip_value_preds = torch.clamp(value_preds, values - epsilon, values + epsilon)   \n",
    "\n",
    "    # thus, we have two errors, but it doesn't matter because we take the maximum (one)\n",
    "    values_error = (value_preds - returns) ** 2\n",
    "    clip_values_error = (clip_value_preds - returns) ** 2\n",
    "    \n",
    "    # this is essentially the inner sum for one trajectory t \\in D_k where D_k is set of trajectories\n",
    "    loss = 0.5 * masked_mean(torchg_probs[i][mask] # masked actor logprobs\n",
    "        lp_r = ref_log_probs[i][mask] # masked reference logprobs\n",
    "\n",
    "\n",
    "        # in my equations below, r is simply the ratio: pi(y) / pi_ref(y)\n",
    "        if self.args.kl_penalty_method == 'k_3': # equation: (r - 1) - log r\n",
    "            lp_diff = lp_a - lp_r\n",
    "            ratio = torch.exp(lp_diff)\n",
    "            kl_est = (ratio - 1.0) - lp_diff\n",
    "\n",
    "        elif self.args.kl_penalty_method == 'abs': # equation: |log r|\n",
    "            kl_est = torch.abs(lp_a - lp_r)\n",
    "\n",
    "        elif self.args.kl_penalty_method == 'mse': # equation: 1/2 * (log r)^2\n",
    "            kl_est = 0.5 * (lp_a - lp_r) ** 2 \n",
    "        else:\n",
    "            raise Value: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            \n",
    "        eos_mask: torch.Tensor\n",
    "\n",
    "        epsilon: torch.Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # this keeps our value predictions within some epsilon-distance\n",
    "    # mostly for numerical stability before performing regression\n",
    "    clip_value_preds = torch.clamp(value_preds, values - epsilon, values + epsilon)   \n",
    "\n",
    "    # thus, we have two errors, but it doesn't matter because we take the maximum (one)\n",
    "    values_error = (value_preds - returns) ** 2\n",
    "    clip_values_error = (clip_value_preds - returns) ** 2\n",
    "    \n",
    "    # this is essentially the inner sum for one trajectory t \\in D_k where D_k is set of trajectories\n",
    "    loss = 0.5 * masked_mean(torchError(f\"Unknown kl_penalty_method: {self.args.kl_penalty_method}\")\n",
    "\n",
    "            \n",
    "        kl_penalty = - self.args.kl_penalty_beta * kl_est\n",
    "        kl_penalty_all.append(kl_penalty)\n",
    "\n",
    "        if self.args.reward_score_clip is not None:\n",
    "            rewards_score[i] = torch.clamp(rewards_score[i], -self.args.reward_score_clip, self.args.reward_score_clip)\n",
    "        \n",
    "        end_index = mask.nonzero()[-1].detach().item()\n",
    "        kl_penalty[end_index] += rewards_score[i]\n",
    "\n",
    "        rewards_with_kl_penalty.append(kl_penalty)\n",
    "    return torch.stack(rewards_with_kl_penalty), torch.stack(kl_penalty_all), rewards_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cf3f9",
   "metadata": {},
   "source": [
    "Now, we cannot compute the advantage function $A_t$ exactly, so we estimate it. \n",
    "\n",
    "The algorithm used in PPO to compute the estimate of the advantage $\\hat{A}_t$ is Generalized Advantage Estimation (GAE).\n",
    "One key thing here is that we do not just naively estimate $A_t$; we have to do it carefully to mitigate a large variance.\n",
    "\n",
    "If $T$ is our trajectory length, then GAE computes this estimate by:\n",
    "\n",
    "$$\\hat{A}_t=\\sum^{T-t-1}_{l=0}(\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\gamma$ is the discount factor, $\\lambda\\in[0,1]$ is the GAE parameter, and $\\delta_t=R(s_t,a_t)+\\gamma V(s_{t+1})-V(s_t)$ which is the Temporal-Difference (TD) error. Here, $R(s_t, a_t)$ is the reward at time $t$, and $V(s_t)$ is the value function at time $t$. \n",
    "\n",
    "Variance reduction is a common theme in reinforcement learning as long-horizon estimates are prone to deviating from our expected value.\n",
    "What GAE does is it performs multi-step \"bootstrapping\" to reduce the variance. Bootstrapping is a term which refers to updating an estimate value using (one or more) of the **same kind of estimate values**.\n",
    "\n",
    "Without bootstrapping, our variance would explode, leading to longer episode trajectories (which we don't want). We want to keep these trajectories as short as possible; that is, to converge as fast as possible. In GAE, this bootstrapping can be seen in the TD error $\\delta_t=R(s_t,a_t)+\\gamma V(s_{t+1})-V(s_t)$.\n",
    "\n",
    "However, we also need to consider bias. Remember the bias-variance tradeoff? Here, as we progress in the trajectory, our estimates accumulate positive bias at each step. This is the price to be paid for variance reduction at each step. There are proposed methods (such as VAPO https://arxiv.org/pdf/2504.05118) which aim to achieve both low variance while mitigating high bias. This is a bit more advanced, though.\n",
    "\n",
    "In the context of applying this to language models, $T$ can be viewed as the maximum sequence length of a model's output. In GAE, the $t$-th step would be the $t$-th token that is currently being sampled from our model. The difference $T-t$ is just how far away this token is from the end of the sentence (EOS). The only nonzero reward $R(s_T)$ is calculated at the `<EOS>` token. Thus, all prior tokens are just propagating it backwards (with discounting). \n",
    "\n",
    "Given this discounting, as we increase the difference $T-t=2, T-t=3,..., T-t=k$, the reward signal grows weaker. Eventually, the last token to obtain this initial reward might obtain a zero value! Yikes. Thankfully, there are methods which attempt to address this decaying reward problem (such as VC-PPO https://arxiv.org/pdf/2503.01491)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f87186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae_advantage_return(self, rewards, values, mask, gamma, lam):\n",
    "    \"\"\"\n",
    "    Computes the Generalized Advantage Estimation via Temporal-Difference with parameter lambda.\n",
    "\n",
    "    Args:\n",
    "        rewards: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        mask: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        gamma: float\n",
    "            discount factor\n",
    "        lam: float\n",
    "            lambda parameter for GAE algorithm\n",
    "    \"\"\"\n",
    "    B, T = rewards.shape # B is batch size, T is response_length\n",
    "\n",
    "    # here, we bootstrap the value model updates with Temporal-Difference (parameter \\lambda)\n",
    "    with torch.no_grad():\n",
    "        advantages_reversed = []\n",
    "        lastgaelam = 0\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # for long sequences with T - t >> 1, discounting reduces the reward signal to near zero\n",
    "            next_values = values[:, t + 1] if t < T - 1 else 0.0\n",
    "            delta = rewards[:, t] + gamma * next_values - values[:, t]\n",
    "            lastgaelam = (delta + gamma * lam * lastgaelam) * mask[:, t] \n",
    "            advantages_reversed.append(lastgaelam)\n",
    "        advantages = torch.stack(advantages_reversed[::-1], dim=1)\n",
    "\n",
    "        returns = advantages + values\n",
    "        # \n",
    "        if use_advantage_norm:\n",
    "            advantages = masked_whiten(advantages, eos_mask)\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf330d7",
   "metadata": {},
   "source": [
    "Implementing PPO (which is an Actor-Critic algorithm) requires a lot of design choices, where you can easily lose yourself trying to read them all. However, the topmost choice deals with the actor and critic themselves. We have two choices: \n",
    "\n",
    "1. Learn two largely separate models. One for the actor and another for the critic.\n",
    "2. Learn one large model, and apply two shallow heads at the end to format our output (one for the actor, another for the critic)\n",
    "\n",
    "The first approach is usually more popular because the singular model we learn is usually a deep transformer which contains rich, hidden representations. As such, our two heads only need to be shallow because they serve as \"mappings\" from our representations to scalar values. The large transformer will have already done the grunt work during its intense training runs (and perhaps, any additional supervised fine-tuning).\n",
    "\n",
    "In usual implementations of PPO (which is an Actor-Critic algorithm), we train a single large model with two heads on top. One is the actor (policy) head and the other is the critic (value) head.\n",
    "We optimize this single model by minimizing a three-part loss function. \n",
    "\n",
    "That is, we add the $L^{CLIP}$ clipped surrogate loss as proposed in the original PPO paper. Next, we subtract an MSE error loss, $L^{VF}$ for computing the value function. Finally, we add the entropy of the policy, $H(\\pi_{\\theta}(\\cdot | s))$ which is multiplied by some constant term, $c_1$. Usually, we also multiply $L^{VF}$ by a constant $c_2$ as well. So we have:\n",
    "$$L=L^{CLIP}-c_2 L^{VF} + c_1 H(\\pi_{\\theta}(\\cdot | s))$$\n",
    "\n",
    "In the following sections, we will go over each term and their theoretical significances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747d43d",
   "metadata": {},
   "source": [
    "The actor has an entropy term which is effectively a regularizer that promotes exploration. Entropy in our case is (very loosely) a measure of our uncertainty in the policy. As such, it can tell us how confident a policy is at choosing an action given that they are in some state. When this entropy $H(\\pi)$ is low, we have a high confidence, whereas a high $H(\\pi)$ implies low confidence. \n",
    "\n",
    "We want our policy to diversify itself instead of picking the \"best choice\" each time. Thus, if we want to push the policy to \"explore more\", we need to keep some amount of uncertainty. We would effectively be \"spreading\" the probability distribution over multiple actions rather than letting a high confidence confine it to only a few actions. So, natually, we'd want to maximize the entropy $H(\\pi)$. \n",
    "\n",
    "Another thing is that we don't always know how to compute $H(\\pi)$ exactly, so we estimate it by taking the log-probability of our policy and averaging over the batch. If $\\mathcal{B}$ is our batch of state, action pairs, then we have:\n",
    "$$H(\\pi)\\approx -\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} (-\\log \\pi(a|s))=\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} (\\log \\pi(a|s))$$\n",
    "Here, the negative in the front is by the definition of entropy $H(x)=-\\sum_x p(x)\\log(p(x))$, but since we are estimating it we can omit the $p(x)$ term so we have something like $H(x)\\approx -\\sum_x \\log(p(x))$. The negative in front of $\\log \\pi(a|s)$ is because we are collecting negative log-probabilities. It is usually easier to compute log-probabilities than the full probabilities, hence this estimation. \n",
    "\n",
    "So, our entropy $H(\\pi)$ comes out as positive. Furthermore, we add a constant $c_1$ which we can tweak to regularize the effect of $H(\\pi)$ on the overall loss $L$.\n",
    "$$c_1 H(\\pi(\\cdot|s))$$\n",
    "However, we can afford computing the entropy in its entire form (which is what the code does below):\n",
    "$$H(\\pi)= -\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} (-\\pi(a|s)\\log \\pi(a|s))=\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} \\pi(a|s)\\log \\pi(a|s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(self, logits, mask):\n",
    "        \"\"\"\n",
    "        Computes the entropy of the policy, which incentivizes exploration.\n",
    "\n",
    "        Args:\n",
    "            logits: torch.Tensor\n",
    "                unnormalized log-probabilities from a model\n",
    "            mask: torch.Tensor\n",
    "                mask to be applied to the computed entropy\n",
    "        \"\"\"\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        entropy = self.masked_mean(-torch.sum(probs * log_probs, dim=-1), mask)\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c259d1",
   "metadata": {},
   "source": [
    "The PPO-Clip surrogate objective is defined as:\n",
    "$$ L^{\\text{CLIP}} = \\min\\left(\\frac{\\pi(\\theta)}{\\pi_{\\text{old}}(\\theta)}A_t, \\text{clip}\\left( \\frac{\\pi(\\theta)}{\\pi_{\\text{old}}(\\theta)} , 1-\\epsilon, 1+\\epsilon \\right)A_t \\right) $$\n",
    "\n",
    "Notice how we do not have a `min` operation here? Notice how we actually use `torch.max()`? That seems counterintuitive. However, further note that we added a negative sign to `loss1` and `loss2`, before taking their `max`. \n",
    "\n",
    "This is effectively equivalent to the `min()` operation but done in reverse to ensure numerical stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_loss(self, old_log_prob, log_prob, advantages, eos_mask, epsilon):\n",
    "        \"\"\"\n",
    "        Computes the policy gradient loss function for PPO.\n",
    "\n",
    "        Args:\n",
    "            old_log_prob: torch.Tensor\n",
    "                log probabilities from the old policy\n",
    "            log_prob: torch.Tensor\n",
    "                log probabilities from the current policy \n",
    "            advantages: torch.Tensor\n",
    "                Computed advantages via advantage estimation\n",
    "            eos_mask: torch.Tensor\n",
    "            \n",
    "            epsilon: float\n",
    "        \"\"\"\n",
    "\n",
    "        # from log domain -> real domain\n",
    "        ratio = torch.exp(log_prob - old_log_prob)\n",
    "        loss1 = -advantages * ratio\n",
    "        loss2 = -advantages * torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "        loss = masked_mean(torch.max(loss1, loss2), eos_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b898b7",
   "metadata": {},
   "source": [
    "As detailed in PPO-Clip algorithm, we compute the value function by regressing on an MSE loss. \n",
    "\n",
    "For some reason this is not explicitly mentioned in the original PPO paper. It is, however, mentioned in OpenAI's Spinning Up documentation entry on PPO. For the answer, we need to do some digging back to the old Barto & Sutton book (you can not escape RL!!!)\n",
    "\n",
    "Recall that the on-policy value function is defined as the expected discounted return that the agent can get given he starts in some state $s$ and acts according to the policy $\\pi$:\n",
    "$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\cdots |S_t=s]$$\n",
    "Now, define the discounted return as $G_t=\\sum_{t'=t}^T \\gamma^{t'-t}r_{t'}$. If we reindex with $k=t'-t$ then this becomes $G_t=\\sum_{k=0}^T \\gamma^{k}r_{t}$.\n",
    "So, the value function becomes:\n",
    "$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]=\\mathbb{E}_{\\pi}[\\sum_{k=0}^T \\gamma^{k}r_{t}|S_t=s]$$\n",
    "which makes a lot more sense now! We see that $v_{\\pi}(s)$ is just the expected discounted return! \n",
    "\n",
    "\n",
    "It is found in Chapter 9.2 of the book. Here, the \"natural objective function\" for value-based algorithms is to minimize the MSE between the true value and observed returns.\n",
    "That is, our value error is defined as:\n",
    "$$\\overline{VE}(\\mathbf{w})=\\mathbb{E}[v_{\\pi}(s)-\\hat{v}(s,\\mathbf{w})]^2$$\n",
    "where the expectation $\\mathbb{E}[\\cdot]$ is taken over the state distribution $\\mu(s)$ such that $\\mu(s)\\geq 0$ and $\\sum_s \\mu(s)=1$.\n",
    "\n",
    "Dissecting this equation, we can see that the expected discounted return $v_{\\pi}(s)$ can be interpreted as the empirical return $G_t$. However, this is not the exactly the $G_t$ we defined earlier. In practice, we do not compute the full return, so we usually use something like a Monte Carlo estimate. It can be shown that this Monte Carlo estimate is an unbiased sample of $v_{\\pi}(s)$, so they are not quite equal (but we can get close enough).\n",
    "\n",
    "Next, $\\hat{v}(s, \\mathbf{w})$ is defined as a joint distribution with states $s$ and weights $\\mathbf{w}$ (which we learn). This is effectively our learned value function, $V(s_t)$ which can be computed using a multi-layer neural network. \n",
    "\n",
    "Thus, in our PPO-Clip algorithm we have the MSE loss:\n",
    "$$L_v = \\sum_{t=1}^T \\frac{1}{2}(G_t - V(S_t))^2$$\n",
    "which tells us how far our observed returns (value estimates), $V(s_t)$, deviate from the \"true\" empirical returns, $G_t$.\n",
    "\n",
    "For a further dissection of On-Policy Prediction with Approximation, I recommend reading Chapter 9 of Sutton & Barto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_loss(self, value_preds, returns, values, eos_mask, epsilon):\n",
    "    \"\"\"\n",
    "    Fits the value function by regression on the MSE loss. \n",
    "\n",
    "    Args:\n",
    "        value_preds: torch.Tensor\n",
    "            the predictions from our value model\n",
    "        returns: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            \n",
    "        eos_mask: torch.Tensor\n",
    "\n",
    "        epsilon: torch.Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # this keeps our value predictions within some epsilon-distance\n",
    "    # mostly for numerical stability before performing regression\n",
    "    clip_value_preds = torch.clamp(value_preds, values - epsilon, values + epsilon)   \n",
    "\n",
    "    # thus, we have two errors, but it doesn't matter because we take the maximum (one)\n",
    "    values_error = (value_preds - returns) ** 2\n",
    "    clip_values_error = (clip_value_preds - returns) ** 2\n",
    "    \n",
    "    # this is essentially the inner sum for one trajectory t \\in D_k where D_k is set of trajectories\n",
    "    loss = 0.5 * masked_mean(torch.max(values_error, clip_values_error), eos_mask)\n",
    "    return loss, values_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0650a",
   "metadata": {},
   "source": [
    "Notice that in the above code cell, we called `masked_mean()`? What is this? Well, we're now getting into implementation-specific \"tricks\" associated with PPO. Stuff you might not find in the original literature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f06bb",
   "metadata": {},
   "source": [
    "<img src=\"assets/grpo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf95a38",
   "metadata": {},
   "source": [
    "Now in this one, explain a lot, what it is, use our quantum 5 year old analogies how it works laydown the components and how it was groundbreaking, dive into the math side of it a lot we wont be doing code here since we're mainly focusing on PPO and DPO for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe022c1",
   "metadata": {},
   "source": [
    "#### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325c6c4",
   "metadata": {},
   "source": [
    "Direct Preference Optimization (DPO), introduced by Rafailov et al. in 2023, represents perhaps the most revolutionary breakthrough in RLHF since the field's inception. While PPO and other reinforcement learning methods transformed language model alignment, DPO achieved something that seemed almost impossible: eliminating the need for reinforcement learning entirely while maintaining the same theoretical guarantees and often achieving superior practical performance.\n",
    "\n",
    "To understand DPO's groundbreaking impact, let's return to our quantum computing explanation scenario and see how it completely reimagines the alignment process by directly optimizing the language model on human preferences without ever training a separate reward model or using complex RL algorithms.\n",
    "\n",
    "Before diving into DPO's mechanics, we need to understand the fundamental question that motivated its development: \"What if the entire RLHF pipeline - reward models, value functions, policy gradients, and complex optimization procedures - is unnecessarily complicated?\" \n",
    "\n",
    "This might seem heretical given how much effort the field has invested in perfecting RL-based approaches, but DPO's creators asked a deeper question: \"Can we directly optimize language models on preference data without the intermediate steps?\"\n",
    "\n",
    "Consider our traditional RLHF workflow:\n",
    "1. **SFT**: Train on demonstrations → Get decent responses\n",
    "2. **Preference Collection**: Humans compare responses → Get preference pairs  \n",
    "3. **Reward Model**: Train neural network to predict preferences → Get automated evaluator\n",
    "4. **RL Optimization**: Use PPO to maximize rewards → Get aligned model\n",
    "\n",
    "DPO's insight was that steps 3 and 4 might be redundant. If we have preference data directly from humans, why not optimize the language model on that data directly? This seemingly simple question led to one of the most elegant mathematical reformulations in modern machine learning.\n",
    "\n",
    "##### The Mathematical Breakthrough: Reparameterizing the Reward Model\n",
    "\n",
    "DPO's theoretical foundation rests on a brilliant mathematical insight about the relationship between optimal policies and reward functions. Let's build this up step by step, starting with what we know from traditional RLHF.\n",
    "\n",
    "**Traditional RLHF Objective:**\n",
    "In PPO-style RLHF, we try to find the policy $\\pi_\\theta$ that maximizes expected reward while staying close to a reference policy $\\pi_{\\text{ref}}$:\n",
    "\n",
    "$$\\pi^* = \\arg\\max_\\pi \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi(y|x)}[r(x,y)] - \\beta D_{KL}(\\pi(y|x) \\| \\pi_{\\text{ref}}(y|x))$$\n",
    "\n",
    "where:\n",
    "- $r(x,y)$ is our reward model's score for response $y$ to prompt $x$\n",
    "- $\\beta$ controls how much we penalize deviation from the reference policy\n",
    "- $D_{KL}$ is the KL divergence preventing the model from changing too dramatically\n",
    "\n",
    "This is a constrained optimization problem that PPO solves through complex policy gradients.\n",
    "\n",
    "DPO's breakthrough was recognizing that this constrained optimization problem has a closed-form solution! Using the method of Lagrange multipliers, we can solve for the optimal policy directly:\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)$$\n",
    "\n",
    "where $Z(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r(x,y)\\right)$ is the partition function.\n",
    "\n",
    "The partition function $Z(x)$ is a normalization term that ensures the probability distribution $\\pi^*(y|x)$ sums to 1 across all possible responses $y$ for a given prompt $x$. It's a fundamental concept in statistical physics and probability theory, essentially accounting for all possible states of the system. In this context, it aggregates the weighted probabilities of all possible responses, with each response weighted by the exponential of its reward. This normalization is crucial for maintaining a valid probability distribution when transforming between reward functions and policies.\n",
    "\n",
    "This equation tells us exactly what the optimal policy looks like in terms of the reward function and reference policy. But here's the crucial insight: we can rearrange this equation to solve for the reward function in terms of the optimal policy:\n",
    "\n",
    "$$r(x,y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)$$\n",
    "\n",
    "Since $Z(x)$ depends only on the prompt $x$ (not the response $y$), it cancels out when we compute preference probabilities. This leads to DPO's fundamental insight:\n",
    "\n",
    "$$r(x,y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\text{constant}$$\n",
    "\n",
    "This means we can express any reward function implicitly through the ratio between our policy and a reference policy! This is the mathematical foundation that makes DPO possible.\n",
    "\n",
    "##### From Reward Models to Direct Optimization\n",
    "\n",
    "Now let's see how this reparameterization transforms the entire RLHF process. Remember our Bradley-Terry preference model from earlier:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r(x, y_w) - r(x, y_l))$$\n",
    "\n",
    "Substituting our DPO reparameterization:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)$$\n",
    "\n",
    "This simplifies beautifully to:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)$$\n",
    "\n",
    "This gives us DPO's training objective - the same Bradley-Terry loss we used for reward model training, but now directly in terms of the language model probabilities:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right) \\right]$$\n",
    "\n",
    "This is mathematically equivalent to the traditional RLHF objective, but it can be optimized directly without reward models or RL algorithms!\n",
    "\n",
    "\n",
    "\n",
    "##### Understanding the Reference Policy\n",
    "\n",
    "The reference policy $\\pi_{\\text{ref}}$ plays a crucial role in DPO that's worth understanding deeply:\n",
    "\n",
    "**What is the Reference Policy?**\n",
    "The reference policy is typically the SFT model - our starting point before preference optimization. It serves as an \"anchor\" that prevents the model from changing too dramatically during training.\n",
    "\n",
    "**Why Do We Need a Reference Policy?**\n",
    "Without the reference policy, the DPO objective becomes ill-defined. The reference policy provides:\n",
    "1. **Stability**: Prevents the model from collapsing to degenerate solutions\n",
    "2. **Regularization**: Maintains the model's general capabilities while improving alignment\n",
    "3. **Baseline**: Defines what \"change\" means in the context of preference optimization\n",
    "\n",
    "**Mathematical Role**:\n",
    "The reference policy appears in the ratio $\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$, which measures how much the current policy deviates from the starting point. This ratio:\n",
    "- Increases for responses the model is learning to prefer\n",
    "- Decreases for responses the model is learning to avoid\n",
    "- Stays near 1.0 for responses that weren't in the training data\n",
    "\n",
    "For our quantum explanation example:\n",
    "- $\\frac{\\pi_\\theta(\\text{\"Hey there! Imagine...\"}))}{\\pi_{\\text{ref}}(\\text{\"Hey there! Imagine...\"})} > 1$ (model learned to prefer engaging openings)\n",
    "- $\\frac{\\pi_\\theta(\\text{\"Quantum superposition involves...\"}))}{\\pi_{\\text{ref}}(\\text{\"Quantum superposition involves...\"})} < 1$ (model learned to avoid technical jargon)\n",
    "\n",
    "##### Why This Works?\n",
    "\n",
    "Let's understand what this loss function actually does using our quantum computing explanation example:\n",
    "\n",
    "**Scenario**: We have preference data showing humans prefer Response B over Response A:\n",
    "- **Response A**: \"Quantum computers use qubits that exist in superposition states...\"\n",
    "- **Response B**: \"Hey there! Imagine quantum computers as magical calculators that can solve puzzles by being in multiple places at once!\"\n",
    "\n",
    "**Traditional RLHF Process**:\n",
    "1. Train reward model: $r(x, \\text{Response B}) > r(x, \\text{Response A})$\n",
    "2. Use PPO to increase probability of higher-reward responses\n",
    "3. Model learns: $\\pi_\\theta(\\text{Response B}|x) \\uparrow$, $\\pi_\\theta(\\text{Response A}|x) \\downarrow$\n",
    "\n",
    "**DPO Process**:\n",
    "1. Direct optimization: $\\frac{\\pi_\\theta(\\text{Response B}|x)}{\\pi_{\\text{ref}}(\\text{Response B}|x)} > \\frac{\\pi_\\theta(\\text{Response A}|x)}{\\pi_{\\text{ref}}(\\text{Response A}|x)}$\n",
    "2. Model learns to assign higher relative probability to preferred responses\n",
    "3. Same end result: $\\pi_\\theta(\\text{Response B}|x) \\uparrow$, $\\pi_\\theta(\\text{Response A}|x) \\downarrow$\n",
    "\n",
    "The key insight is that DPO directly optimizes the probability ratios that determine human preferences, skipping the intermediate reward model entirely.\n",
    "\n",
    "\n",
    "<img src=\"assets/dpo.png\">\n",
    "\n",
    "Let's trace through DPO's streamlined pipeline compared to traditional RLHF:\n",
    "\n",
    "**Step 1: Start with SFT Model (Same as RLHF)**\n",
    "We begin with a supervised fine-tuned model that can follow basic instructions. This serves as both our starting point for optimization and our reference policy $\\pi_{\\text{ref}}$.\n",
    "\n",
    "**Step 2: Collect Preference Data (Same as RLHF)**  \n",
    "Human annotators compare pairs of responses and indicate preferences. For our quantum explanation:\n",
    "- Prompt: \"Explain quantum computing to a 10-year-old\"\n",
    "- Response pair comparison → Human prefers engaging, analogical explanation\n",
    "\n",
    "**Step 3: Direct Preference Optimization (Revolutionary Change)**\n",
    "Instead of training a reward model, we directly optimize the language model using the DPO loss:\n",
    "\n",
    "For each preference pair $(x, y_w, y_l)$:\n",
    "1. **Compute probabilities**: \n",
    "   - $\\pi_\\theta(y_w|x)$ and $\\pi_\\theta(y_l|x)$ from current model\n",
    "   - $\\pi_{\\text{ref}}(y_w|x)$ and $\\pi_{\\text{ref}}(y_l|x)$ from reference (SFT) model\n",
    "\n",
    "2. **Calculate preference probability**:\n",
    "   $$P(\\text{prefer } y_w) = \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)/\\pi_{\\text{ref}}(y_w|x)}{\\pi_\\theta(y_l|x)/\\pi_{\\text{ref}}(y_l|x)}\\right)$$\n",
    "\n",
    "3. **Optimize to match human preferences**:\n",
    "   - If humans preferred $y_w$, increase this probability\n",
    "   - Adjust model parameters to make preferred responses more likely\n",
    "\n",
    "**Step 4: Iterate Until Convergence**\n",
    "Repeat the optimization process until the model consistently generates responses that align with human preferences.\n",
    "\n",
    "\n",
    "\n",
    "##### Why DPO is Theoretically Equivalent to RLHF\n",
    "\n",
    "One of DPO's most remarkable properties is that it's not just similar to traditional RLHF - it's mathematically equivalent under certain conditions. Let's understand why:\n",
    "\n",
    "**Equivalence Theorem**: If we solve the DPO optimization problem to convergence, the resulting policy $\\pi^*_{\\text{DPO}}$ is identical to the policy we would get from solving the constrained RLHF problem:\n",
    "\n",
    "$$\\pi^*_{\\text{DPO}} = \\pi^*_{\\text{RLHF}} = \\arg\\max_\\pi \\mathbb{E}[r(x,y)] - \\beta D_{KL}(\\pi \\| \\pi_{\\text{ref}})$$\n",
    "\n",
    "**Proof Sketch**: \n",
    "The key insight is that the DPO loss function is the gradient of the RLHF objective with respect to the policy parameters. When we minimize the DPO loss, we're following the exact same gradient flow that RLHF algorithms like PPO approximate.\n",
    "\n",
    "This theoretical equivalence is profound because it means DPO isn't making any compromises - it achieves the same optimal solution as traditional RLHF but through a much simpler path.\n",
    "\n",
    "##### Why DPO is More Efficient\n",
    "\n",
    "DPO's computational benefits become apparent when we compare the training processes:\n",
    "\n",
    "**Traditional RLHF Computational Complexity**:\n",
    "- **Reward Model Training**: Forward/backward passes through full transformer\n",
    "- **PPO Optimization**: \n",
    "  - Multiple rollout generations per iteration\n",
    "  - Value function estimation and advantage computation  \n",
    "  - Multiple policy gradient steps per batch\n",
    "  - KL divergence computations\n",
    "- **Total**: ~3-5x computational cost of supervised training\n",
    "\n",
    "**DPO Computational Complexity**:\n",
    "- **Direct Optimization**: Standard gradient descent on preference data\n",
    "- **Two Forward Passes**: Current model + reference model probability computation\n",
    "- **Simple Loss**: Bradley-Terry preference loss (just like reward model training)\n",
    "- **Total**: ~1.2x computational cost of supervised training\n",
    "\n",
    "For our quantum explanation model, this means:\n",
    "- **RLHF**: Might require 100 GPU-hours for alignment\n",
    "- **DPO**: Achieves same results in ~30 GPU-hours\n",
    "\n",
    "This efficiency gain is crucial for making alignment accessible to researchers and practitioners without massive computational resources.\n",
    "\n",
    "##### DPO's Practical Advantages: Beyond Computational Efficiency\n",
    "\n",
    "**1. Simplicity and Reliability**\n",
    "DPO eliminates many sources of instability in traditional RLHF:\n",
    "- No reward model overoptimization\n",
    "- No complex PPO hyperparameter tuning  \n",
    "- No value function estimation errors\n",
    "- Standard supervised learning dynamics\n",
    "\n",
    "**2. Better Sample Efficiency**\n",
    "DPO often achieves better alignment with fewer preference pairs because:\n",
    "- Direct optimization on preference data (no information loss through reward modeling)\n",
    "- No policy-reward model mismatch issues\n",
    "- More stable training dynamics\n",
    "\n",
    "**3. Easier Hyperparameter Tuning**\n",
    "DPO has fewer critical hyperparameters:\n",
    "- $\\beta$: Controls the strength of the KL penalty (typical values: 0.1-0.5)\n",
    "- Learning rate: Standard supervised learning values work well\n",
    "- No need for PPO-specific hyperparameters (clipping, GAE, etc.)\n",
    "\n",
    "**4. Interpretability**\n",
    "The DPO loss is more interpretable:\n",
    "- Direct relationship between loss and human preferences\n",
    "- Easy to debug and understand training dynamics\n",
    "- Clear connection between loss values and model behavior\n",
    "\n",
    "##### When DPO Might Struggle: Understanding the Limitations\n",
    "\n",
    "Despite its advantages, DPO has some limitations worth understanding:\n",
    "\n",
    "**1. Limited Exploration**\n",
    "DPO can only learn from the preference data provided. If the preference dataset has gaps, DPO might not discover better responses that weren't in the training data. Traditional RLHF with RL exploration might find better solutions in some cases.\n",
    "\n",
    "**2. Preference Dataset Quality Dependency**\n",
    "DPO's performance is directly tied to the quality of preference data. Poor or inconsistent human preferences will directly impact the final model quality, whereas RLHF's reward model might provide some smoothing of noisy preferences.\n",
    "\n",
    "**3. Distributional Mismatch**\n",
    "If the distribution of prompts in preference data differs significantly from deployment, DPO might not generalize as well as RLHF methods that learn more general reward functions.\n",
    "\n",
    "##### DPO Variants and Extensions\n",
    "\n",
    "Since its introduction, several variants of DPO have emerged to address specific limitations:\n",
    "\n",
    "**1. Self-Play DPO (SPDPO)**\n",
    "Iteratively generates new response pairs from the current model and collects preferences, addressing the exploration limitation.\n",
    "\n",
    "**2. Constitutional DPO (CDPO)**  \n",
    "Uses AI-generated critiques and revisions instead of human preferences, making the process more scalable.\n",
    "\n",
    "**3. Rejection Sampling DPO (RSDPO)**\n",
    "Combines DPO with rejection sampling to improve sample quality before training.\n",
    "\n",
    "**4. Online DPO**\n",
    "Collects preferences on-the-fly during training, similar to online RLHF methods.\n",
    "\n",
    "##### DPO in Practice: Real-World Impact\n",
    "\n",
    "DPO has been rapidly adopted across the AI industry:\n",
    "\n",
    "**Major Implementations**:\n",
    "- **Anthropic**: Uses DPO variants in Claude training\n",
    "- **Meta**: Llama 2-Chat uses DPO-style optimization\n",
    "- **Mistral**: Mistral-Instruct employs DPO techniques\n",
    "- **Open-source**: Most new instruction-tuned models use DPO\n",
    "\n",
    "**Performance Results**:\n",
    "Studies consistently show DPO achieving comparable or superior performance to PPO-based RLHF while being significantly more efficient and stable.\n",
    "\n",
    "\n",
    "\n",
    "In our quantum computing education example, DPO would directly learn from human preferences to transform our model from generating technical responses to producing engaging, child-friendly explanations. The process would be simpler, faster, and more stable than traditional RLHF while achieving the same high-quality results.\n",
    "\n",
    "DPO represents a paradigm shift in AI alignment - proof that sometimes the most elegant solution is also the most practical. By eliminating unnecessary complexity while maintaining theoretical rigor, DPO has democratized high-quality alignment and opened new possibilities for building AI systems that better serve human needs and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa734d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class DPOTrainer:\n",
    "    def __init__(self, sft_model, beta=0.1, learning_rate=5e-7):\n",
    "        \"\"\"\n",
    "        Initialize Direct Preference Optimization (DPO) trainer\n",
    "        \n",
    "        DPO is revolutionary because it eliminates the need for:\n",
    "        1. Separate reward model training (traditional RLHF step)\n",
    "        2. Complex RL algorithms like PPO\n",
    "        3. Value function estimation and advantage computation\n",
    "        \n",
    "        Instead, DPO directly optimizes the language model on preference data using\n",
    "        the mathematical insight that optimal policies can be expressed as:\n",
    "        π*(y|x) = π_ref(y|x) * exp(r(x,y)/β) / Z(x)\n",
    "        \n",
    "        Rearranging: r(x,y) = β * log(π*(y|x)/π_ref(y|x)) + constant\n",
    "        \n",
    "        This means we can represent any reward function implicitly through \n",
    "        probability ratios between our policy and a reference model!\n",
    "        \n",
    "        Args:\n",
    "            sft_model: SupervisedFineTuner instance - our starting point for optimization\n",
    "            beta: Temperature parameter (typically 0.1-0.5):\n",
    "                  - Higher β = more aggressive preference optimization\n",
    "                  - Lower β = more conservative, stays closer to reference\n",
    "            learning_rate: Learning rate for gradient descent (typically 5e-7 for LLMs)\n",
    "        \"\"\"\n",
    "        # The policy model is what we'll train - it starts as a copy of our SFT model\n",
    "        # and gradually learns to prefer responses that humans rate more highly\n",
    "        self.policy_model = sft_model.model  # This will be optimized\n",
    "        \n",
    "        # The reference model stays frozen - it represents our \"anchor point\"\n",
    "        # We compute probability ratios against this to implicitly model rewards\n",
    "        self.reference_model = None  # Will be created as frozen copy\n",
    "        \n",
    "        # Shared infrastructure from our existing pipeline\n",
    "        self.tokenizer = sft_model.tokenizer\n",
    "        self.device = sft_model.device\n",
    "        \n",
    "        # DPO hyperparameters - these control the optimization behavior\n",
    "        self.beta = beta  # Controls strength of preference optimization\n",
    "        self.learning_rate = learning_rate  # Standard gradient descent learning rate\n",
    "        \n",
    "        # Create the crucial reference model - this is what makes DPO work\n",
    "        # The reference model provides the baseline for computing probability ratios\n",
    "        self._create_reference_model()\n",
    "        \n",
    "        # Standard optimizer - note how simple this is compared to PPO!\n",
    "        # No need for separate actor/critic optimizers or complex scheduling\n",
    "        # AdamW combines Adam's adaptive learning rates with weight decay regularization\n",
    "        # making it highly effective for transformer models\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        print(f\"DPO trainer initialized with beta={beta}, lr={learning_rate}\")\n",
    "        print(\"Key insight: We'll optimize probability ratios π_θ(y|x)/π_ref(y|x)\")\n",
    "        print(\"This implicitly represents rewards without training a separate reward model!\")\n",
    "        \n",
    "    def _create_reference_model(self):\n",
    "        \"\"\"\n",
    "        Create the reference model - this is the mathematical foundation of DPO!\n",
    "        \n",
    "        The reference model π_ref serves multiple critical purposes:\n",
    "        \n",
    "        1. BASELINE FOR COMPARISON: It's our \"neutral\" policy that we compare against\n",
    "           Think of it as \"where we started\" vs \"where we're going\"\n",
    "        \n",
    "        2. KL REGULARIZATION: Prevents our policy from straying too far from sensible text\n",
    "           Without this, the model might find pathological ways to maximize preferences\n",
    "           (like generating random text that accidentally gets high preference scores)\n",
    "        \n",
    "        3. IMPLICIT REWARD COMPUTATION: The magic of DPO is that we never explicitly\n",
    "           compute rewards r(x,y). Instead, we use the ratio π_θ(y|x)/π_ref(y|x)\n",
    "           as an implicit representation of reward!\n",
    "        \n",
    "        Mathematical insight:\n",
    "        If π*(y|x) ∝ π_ref(y|x) * exp(r(x,y)/β), then:\n",
    "        r(x,y) = β * log(π*(y|x)/π_ref(y|x)) + constant\n",
    "        \n",
    "        So probability ratios ARE rewards! This eliminates the need for reward models.\n",
    "        \"\"\"\n",
    "        print(\"Creating reference model (frozen copy of SFT model)...\")\n",
    "        \n",
    "        # Create a COMPLETELY NEW model instance with the same architecture\n",
    "        # This is crucial - we need separate parameters, not just shared references\n",
    "        # Using type(self.policy_model) ensures we get the exact same model class\n",
    "        # and self.policy_model.config ensures identical architecture\n",
    "        self.reference_model = type(self.policy_model)(self.policy_model.config)\n",
    "        \n",
    "        # Copy ALL the trained weights from our policy model to the reference model\n",
    "        # load_state_dict creates a deep copy of all parameters\n",
    "        # This captures our SFT model's learned knowledge as the \"baseline\"\n",
    "        self.reference_model.load_state_dict(self.policy_model.state_dict())\n",
    "        \n",
    "        # Move to the same device (GPU/CPU) as our policy model\n",
    "        self.reference_model.to(self.device)\n",
    "        \n",
    "        # CRITICAL: Freeze all parameters in the reference model\n",
    "        # The reference model MUST never change during DPO training\n",
    "        # It represents our fixed baseline for computing probability ratios\n",
    "        # If the reference changed, our implicit rewards would be inconsistent!\n",
    "        for param in self.reference_model.parameters():\n",
    "            param.requires_grad = False  # No gradients = no updates = frozen\n",
    "            \n",
    "        # Set to evaluation mode - disables dropout, batch norm updates, etc.\n",
    "        # This ensures the reference model behaves deterministically\n",
    "        # Consistent behavior is essential for stable training\n",
    "        self.reference_model.eval()\n",
    "        \n",
    "        print(\"Reference model created and frozen\")\n",
    "        print(\"This frozen copy will serve as π_ref for computing probability ratios\")\n",
    "        print(\"Key insight: π_θ(y|x)/π_ref(y|x) implicitly represents rewards!\")\n",
    "        \n",
    "    def create_dpo_preference_dataset(self, prompts, num_responses_per_prompt=4):\n",
    "        \"\"\"\n",
    "        Create preference dataset for DPO training using our SFT model\n",
    "        This simulates the human preference collection process for DPO\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompts to generate responses for\n",
    "            num_responses_per_prompt: Number of responses to generate per prompt\n",
    "            \n",
    "        Returns:\n",
    "            preference_data: List of preference tuples (prompt, preferred, dispreferred)\n",
    "        \"\"\"\n",
    "        print(\"Generating preference dataset for DPO training...\")\n",
    "        preference_data = []\n",
    "        \n",
    "        # Put policy model in eval mode for generationalso i think thiis mi\n",
    "        self.policy_model.eval()\n",
    "        \n",
    "        for prompt in tqdm(prompts, desc=\"Creating DPO preference data\"):\n",
    "            responses = []\n",
    "            \n",
    "            # Generate multiple responses for the same prompt\n",
    "            for _ in range(num_responses_per_prompt):\n",
    "                # Format prompt consistently\n",
    "                formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "                inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.policy_model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        temperature=0.8,  # Higher temperature for diversity\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                # Decode response\n",
    "                full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                response = full_text[len(formatted_prompt):]\n",
    "                responses.append(response.strip())\n",
    "            \n",
    "            # Create preference pairs using our quality heuristics\n",
    "            for i in range(len(responses)):\n",
    "                for j in range(i + 1, len(responses)):\n",
    "                    score_i = self._dpo_quality_score(responses[i])\n",
    "                    score_j = self._dpo_quality_score(responses[j])\n",
    "                    \n",
    "                    if score_i > score_j:\n",
    "                        preference_data.append((prompt, responses[i], responses[j]))\n",
    "                    elif score_j > score_i:\n",
    "                        preference_data.append((prompt, responses[j], responses[i]))\n",
    "        \n",
    "        print(f\"Created {len(preference_data)} preference pairs for DPO\")\n",
    "        return preference_data\n",
    "    \n",
    "    def _dpo_quality_score(self, response):\n",
    "        \"\"\"\n",
    "        Enhanced quality scoring for DPO preference simulation\n",
    "        Focuses on factors that make responses better aligned with human preferences\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Strongly prefer interactive and engaging responses\n",
    "        if '?' in response:\n",
    "            score += 3\n",
    "        \n",
    "        # Prefer conversational and friendly openings\n",
    "        friendly_openings = ['hey', 'hello', 'hi there', 'great question']\n",
    "        for opening in friendly_openings:\n",
    "            if opening.lower() in response.lower()[:50]:  # Check first 50 chars\n",
    "                score += 2\n",
    "        \n",
    "        # Prefer responses with analogies and examples\n",
    "        analogy_words = ['like', 'imagine', 'think of', 'similar to', 'for example']\n",
    "        for word in analogy_words:\n",
    "            if word.lower() in response.lower():\n",
    "                score += 1\n",
    "        \n",
    "        # Prefer responses that acknowledge the user or show understanding\n",
    "        acknowledgments = ['understand', 'see what you mean', 'good point', 'absolutely']\n",
    "        for ack in acknowledgments:\n",
    "            if ack.lower() in response.lower():\n",
    "                score += 1\n",
    "        \n",
    "        # Prefer helpful and encouraging language\n",
    "        helpful_words = ['help', 'guide', 'show', 'explain', 'clarify', 'support']\n",
    "        for word in helpful_words:\n",
    "            if word.lower() in response.lower():\n",
    "                score += 1\n",
    "        \n",
    "        # Penalize overly technical or dry responses\n",
    "        technical_words = ['algorithm', 'implementation', 'optimization', 'parameter']\n",
    "        for word in technical_words:\n",
    "            if word.lower() in response.lower():\n",
    "                score -= 1\n",
    "        \n",
    "        # Prefer moderate length responses\n",
    "        length = len(response.split())\n",
    "        if 30 <= length <= 120:  # Sweet spot for engagement\n",
    "            score += 2\n",
    "        elif length < 15:\n",
    "            score -= 2\n",
    "        elif length > 200:\n",
    "            score -= 1\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def compute_dpo_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Compute the DPO loss for a batch of preference data\n",
    "        \n",
    "        This is the core of DPO: directly optimizing on preference data using\n",
    "        the Bradley-Terry model with policy-reference probability ratios\n",
    "        \n",
    "        Args:\n",
    "            batch: Dictionary containing tokenized preference data\n",
    "            \n",
    "        Returns:\n",
    "            loss: DPO loss value\n",
    "            metrics: Dictionary of training metrics\n",
    "        \"\"\"\n",
    "        # Extract batch data\n",
    "        preferred_ids = batch['preferred_ids']\n",
    "        preferred_mask = batch['preferred_mask']\n",
    "        dispreferred_ids = batch['dispreferred_ids'] \n",
    "        dispreferred_mask = batch['dispreferred_mask']\n",
    "        \n",
    "        batch_size = preferred_ids.shape[0]\n",
    "        \n",
    "        # Get log probabilities from policy model (trainable)\n",
    "        self.policy_model.train()\n",
    "        preferred_policy_outputs = self.policy_model(preferred_ids, attention_mask=preferred_mask)\n",
    "        dispreferred_policy_outputs = self.policy_model(dispreferred_ids, attention_mask=dispreferred_mask)\n",
    "        \n",
    "        # Get log probabilities from reference model (frozen)\n",
    "        with torch.no_grad():\n",
    "            preferred_ref_outputs = self.reference_model(preferred_ids, attention_mask=preferred_mask)\n",
    "            dispreferred_ref_outputs = self.reference_model(dispreferred_ids, attention_mask=dispreferred_mask)\n",
    "        \n",
    "        # Compute log probabilities for each sequence\n",
    "        preferred_policy_logprobs = self._get_log_probabilities(\n",
    "            preferred_policy_outputs.logits, preferred_ids, preferred_mask\n",
    "        )\n",
    "        dispreferred_policy_logprobs = self._get_log_probabilities(\n",
    "            dispreferred_policy_outputs.logits, dispreferred_ids, dispreferred_mask\n",
    "        )\n",
    "        \n",
    "        preferred_ref_logprobs = self._get_log_probabilities(\n",
    "            preferred_ref_outputs.logits, preferred_ids, preferred_mask\n",
    "        )\n",
    "        dispreferred_ref_logprobs = self._get_log_probabilities(\n",
    "            dispreferred_ref_outputs.logits, dispreferred_ids, dispreferred_mask\n",
    "        )\n",
    "        \n",
    "        # Compute DPO objective using policy-reference ratios\n",
    "        # DPO insight: r(x,y) = β * log(π_θ(y|x)/π_ref(y|x)) + constant\n",
    "        preferred_ratio = preferred_policy_logprobs - preferred_ref_logprobs\n",
    "        dispreferred_ratio = dispreferred_policy_logprobs - dispreferred_ref_logprobs\n",
    "        \n",
    "        # DPO loss: -log σ(β * (preferred_ratio - dispreferred_ratio))\n",
    "        logits = self.beta * (preferred_ratio - dispreferred_ratio)\n",
    "        loss = -F.logsigmoid(logits).mean() # sigmoid function to scale the output\n",
    "        \n",
    "        # Compute metrics for monitoring\n",
    "        with torch.no_grad():\n",
    "            # Accuracy: how often preferred response has higher implicit reward\n",
    "            accuracy = (preferred_ratio > dispreferred_ratio).float().mean()\n",
    "            \n",
    "            # Average implicit reward difference\n",
    "            reward_difference = (preferred_ratio - dispreferred_ratio).mean()\n",
    "            \n",
    "            # KL divergences from reference policy\n",
    "            preferred_kl = (preferred_policy_logprobs - preferred_ref_logprobs).mean()\n",
    "            dispreferred_kl = (dispreferred_policy_logprobs - dispreferred_ref_logprobs).mean()\n",
    "            \n",
    "        metrics = {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'reward_difference': reward_difference.item(),\n",
    "            'preferred_kl': preferred_kl.item(),\n",
    "            'dispreferred_kl': dispreferred_kl.item(),\n",
    "            'logits_mean': logits.mean().item(),\n",
    "            'preferred_ratio_mean': preferred_ratio.mean().item(),\n",
    "            'dispreferred_ratio_mean': dispreferred_ratio.mean().item()\n",
    "        }\n",
    "        \n",
    "        return loss, metrics\n",
    "    \n",
    "    def _get_log_probabilities(self, logits, labels, attention_mask):\n",
    "        \"\"\"\n",
    "        Compute log probabilities for sequences given model logits\n",
    "        \n",
    "        Args:\n",
    "            logits: Model output logits [batch_size, seq_len, vocab_size]\n",
    "            labels: Token IDs [batch_size, seq_len]\n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            log_probs: Log probability for each sequence [batch_size]\n",
    "        \"\"\"\n",
    "        # Shift labels and logits for causal language modeling\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather log probabilities for actual tokens\n",
    "        token_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Mask out padding tokens and sum over sequence\n",
    "        masked_log_probs = token_log_probs * shift_mask\n",
    "        sequence_log_probs = masked_log_probs.sum(dim=-1)  # Sum over sequence length\n",
    "        \n",
    "        return sequence_log_probs\n",
    "    \n",
    "    def prepare_dpo_batch(self, preference_data, batch_size=4):\n",
    "        \"\"\"\n",
    "        Prepare batches for DPO training from preference data\n",
    "        \n",
    "        Args:\n",
    "            preference_data: List of (prompt, preferred, dispreferred) tuples\n",
    "            batch_size: Batch size for training\n",
    "            \n",
    "        Returns:\n",
    "            batches: List of prepared batches for DPO training\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, len(preference_data), batch_size):\n",
    "            batch_data = preference_data[i:i + batch_size]\n",
    "            \n",
    "            preferred_texts = []\n",
    "            dispreferred_texts = []\n",
    "            \n",
    "            for prompt, preferred, dispreferred in batch_data:\n",
    "                # Format as complete conversations\n",
    "                preferred_text = f\"User: {prompt}\\n\\nAssistant: {preferred}\"\n",
    "                dispreferred_text = f\"User: {prompt}\\n\\nAssistant: {dispreferred}\"\n",
    "                \n",
    "                preferred_texts.append(preferred_text)\n",
    "                dispreferred_texts.append(dispreferred_text)\n",
    "            \n",
    "            # Tokenize both sets with consistent padding\n",
    "            preferred_inputs = self.tokenizer(\n",
    "                preferred_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            dispreferred_inputs = self.tokenizer(\n",
    "                dispreferred_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            batch = {\n",
    "                'preferred_ids': preferred_inputs['input_ids'],\n",
    "                'preferred_mask': preferred_inputs['attention_mask'],\n",
    "                'dispreferred_ids': dispreferred_inputs['input_ids'],\n",
    "                'dispreferred_mask': dispreferred_inputs['attention_mask']\n",
    "            }\n",
    "            \n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def train(self, preference_data, num_epochs=3, batch_size=4):\n",
    "        \"\"\"\n",
    "        Train the model using DPO on preference data\n",
    "        \n",
    "        Args:\n",
    "            preference_data: List of preference tuples\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size\n",
    "        \"\"\"\n",
    "        print(f\"Training with DPO for {num_epochs} epochs...\")\n",
    "        print(f\"Using {len(preference_data)} preference pairs\")\n",
    "        \n",
    "        # Prepare batches\n",
    "        batches = self.prepare_dpo_batch(preference_data, batch_size)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "            total_reward_diff = 0\n",
    "            \n",
    "            progress_bar = tqdm(batches, desc=f\"DPO Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Compute DPO loss\n",
    "                loss, metrics = self.compute_dpo_loss(batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                total_loss += metrics['loss']\n",
    "                total_accuracy += metrics['accuracy']\n",
    "                total_reward_diff += metrics['reward_difference']\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{metrics['loss']:.4f}\",\n",
    "                    'acc': f\"{metrics['accuracy']:.3f}\",\n",
    "                    'reward_diff': f\"{metrics['reward_difference']:.3f}\",\n",
    "                    'pref_kl': f\"{metrics['preferred_kl']:.4f}\",\n",
    "                    'dispref_kl': f\"{metrics['dispreferred_kl']:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Epoch summary\n",
    "            avg_loss = total_loss / len(batches)\n",
    "            avg_accuracy = total_accuracy / len(batches)\n",
    "            avg_reward_diff = total_reward_diff / len(batches)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} Summary:\")\n",
    "            print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"  Average Accuracy: {avg_accuracy:.3f}\")\n",
    "            print(f\"  Average Reward Difference: {avg_reward_diff:.3f}\")\n",
    "    \n",
    "    def evaluate_dpo_model(self, evaluation_prompts):\n",
    "        \"\"\"\n",
    "        Evaluate the DPO-trained model on test prompts\n",
    "        Compare against the original SFT model and reference model\n",
    "        \"\"\"\n",
    "        print(\"=== DPO Model Evaluation ===\")\n",
    "        \n",
    "        self.policy_model.eval()\n",
    "        self.reference_model.eval()\n",
    "        \n",
    "        for prompt in evaluation_prompts:\n",
    "            print(f\"\\nPrompt: {prompt}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Generate with reference model (original SFT)\n",
    "            formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Reference model generation\n",
    "                ref_outputs = self.reference_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # DPO model generation  \n",
    "                dpo_outputs = self.policy_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode responses\n",
    "            ref_response = self.tokenizer.decode(ref_outputs[0], skip_special_tokens=True)[len(formatted_prompt):]\n",
    "            dpo_response = self.tokenizer.decode(dpo_outputs[0], skip_special_tokens=True)[len(formatted_prompt):]\n",
    "            \n",
    "            print(f\"Reference (SFT): {ref_response}\\n\")\n",
    "            print(f\"DPO Optimized: {dpo_response}\\n\")\n",
    "            \n",
    "            # Compute quality scores\n",
    "            ref_score = self._dpo_quality_score(ref_response)\n",
    "            dpo_score = self._dpo_quality_score(dpo_response)\n",
    "            \n",
    "            print(f\"Quality Scores - Reference: {ref_score}, DPO: {dpo_score}\")\n",
    "            print(\"=\" * 80)\n",
    "    \n",
    "    def save_dpo_model(self, output_dir):\n",
    "        \"\"\"Save the DPO-trained model\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the policy model (DPO-trained)\n",
    "        policy_path = os.path.join(output_dir, \"dpo_policy\")\n",
    "        self.policy_model.save_pretrained(policy_path)\n",
    "        \n",
    "        # Save the reference model\n",
    "        reference_path = os.path.join(output_dir, \"dpo_reference\") \n",
    "        self.reference_model.save_pretrained(reference_path)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"DPO model saved to {output_dir}\")\n",
    "        \n",
    "    def load_dpo_model(self, model_path):\n",
    "        \"\"\"Load a trained DPO model\"\"\"\n",
    "        policy_path = os.path.join(model_path, \"dpo_policy\")\n",
    "        reference_path = os.path.join(model_path, \"dpo_reference\")\n",
    "        \n",
    "        self.policy_model = AutoModelForCausalLM.from_pretrained(policy_path).to(self.device)\n",
    "        self.reference_model = AutoModelForCausalLM.from_pretrained(reference_path).to(self.device)\n",
    "        \n",
    "        # Freeze reference model\n",
    "        for param in self.reference_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.reference_model.eval()\n",
    "        \n",
    "        print(f\"DPO model loaded from {model_path}\")\n",
    "\n",
    "# Integration example with our existing classes\n",
    "def create_dpo_example():\n",
    "    \"\"\"\n",
    "    Complete example of DPO training using our existing infrastructure\n",
    "    This shows how DPO integrates with our SFT and preference collection systems\n",
    "    \"\"\"\n",
    "    print(\"=== DPO Training Example ===\")\n",
    "    \n",
    "    # Step 1: Start with our existing SFT model\n",
    "    print(\"Step 1: Creating base SFT model...\")\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Create and train SFT model\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    processed_data = sft.prepare_data()\n",
    "    peft_model = sft.setup_peft()\n",
    "    adapter_path = sft.train(output_dir=\"sft_model\", num_epochs=1)\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    # Step 2: Create DPO trainer using the SFT model\n",
    "    print(\"\\nStep 2: Initializing DPO trainer...\")\n",
    "    dpo_trainer = DPOTrainer(sft, beta=0.1, learning_rate=5e-7)\n",
    "    \n",
    "    # Step 3: Create preference dataset for DPO\n",
    "    print(\"\\nStep 3: Creating preference dataset...\")\n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How do I bake a chocolate cake?\", \n",
    "        \"What is machine learning?\",\n",
    "        \"Write a short story about friendship\",\n",
    "        \"Explain the theory of relativity simply\",\n",
    "        \"How do plants grow?\",\n",
    "        \"What makes music sound good?\",\n",
    "        \"Why do we dream?\"\n",
    "    ]\n",
    "    \n",
    "    preference_data = dpo_trainer.create_dpo_preference_dataset(test_prompts)\n",
    "    \n",
    "    # Step 4: Train with DPO\n",
    "    print(\"\\nStep 4: Training with DPO...\")\n",
    "    dpo_trainer.train(preference_data, num_epochs=2, batch_size=4)\n",
    "    \n",
    "    # Step 5: Evaluate the results\n",
    "    print(\"\\nStep 5: Evaluating DPO results...\")\n",
    "    evaluation_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How can I learn programming?\",\n",
    "        \"What is artificial intelligence?\"\n",
    "    ]\n",
    "    \n",
    "    dpo_trainer.evaluate_dpo_model(evaluation_prompts)\n",
    "    \n",
    "    # Step 6: Save the trained model\n",
    "    print(\"\\nStep 6: Saving DPO model...\")\n",
    "    dpo_trainer.save_dpo_model(\"dpo_model\")\n",
    "    \n",
    "    print(\"\\n=== DPO Training Complete ===\")\n",
    "    return dpo_trainer\n",
    "\n",
    "# Standalone DPO implementation comparison\n",
    "def compare_rlhf_vs_dpo():\n",
    "    \"\"\"\n",
    "    Compare traditional RLHF (Reward Model + PPO) vs DPO approaches\n",
    "    This demonstrates the key differences in implementation and results\n",
    "    \"\"\"\n",
    "    print(\"=== RLHF vs DPO Comparison ===\")\n",
    "    \n",
    "    # Initialize base model\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Create SFT model (shared starting point)\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    processed_data = sft.prepare_data()\n",
    "    peft_model = sft.setup_peft()\n",
    "    adapter_path = sft.train(output_dir=\"comparison_sft\", num_epochs=1)\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    print(\"\\n--- Traditional RLHF Approach ---\")\n",
    "    # Create reward model (traditional RLHF)\n",
    "    reward_model = RewardModel(base_llm)\n",
    "    reward_trainer = RewardModelTrainer(reward_model)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How do I bake a chocolate cake?\",\n",
    "        \"What is machine learning?\"\n",
    "    ]\n",
    "    \n",
    "    # Create preference data for reward model\n",
    "    rlhf_preference_data = reward_trainer.create_preference_dataset(sft, test_prompts)\n",
    "    reward_trainer.train(rlhf_preference_data, num_epochs=2)\n",
    "    \n",
    "    print(\"\\n--- DPO Approach ---\")\n",
    "    # Create DPO trainer (direct preference optimization)\n",
    "    dpo_trainer = DPOTrainer(sft, beta=0.1)\n",
    "    dpo_preference_data = dpo_trainer.create_dpo_preference_dataset(test_prompts)\n",
    "    dpo_trainer.train(dpo_preference_data, num_epochs=2)\n",
    "    \n",
    "    print(\"\\n--- Comparison Results ---\")\n",
    "    evaluation_prompts = [\"Explain quantum computing to a 10-year-old\"]\n",
    "    \n",
    "    print(\"\\nReward Model Evaluation:\")\n",
    "    rewards = reward_trainer.evaluate_responses(\n",
    "        evaluation_prompts * 3,\n",
    "        [\n",
    "            \"Quantum computers use qubits and superposition...\",\n",
    "            \"Hey there! Imagine quantum computers as magic calculators...\",\n",
    "            \"Quantum computing is really complex but think of it like...\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    for i, reward in enumerate(rewards):\n",
    "        print(f\"Response {i+1} reward: {reward:.3f}\")\n",
    "    \n",
    "    print(\"\\nDPO Model Evaluation:\")\n",
    "    dpo_trainer.evaluate_dpo_model(evaluation_prompts)\n",
    "    \n",
    "    return reward_trainer, dpo_trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete DPO example\n",
    "    dpo_trainer = create_dpo_example()\n",
    "    \n",
    "    # Uncomment to run comparison\n",
    "    # reward_trainer, dpo_trainer = compare_rlhf_vs_dpo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08e36c",
   "metadata": {},
   "source": [
    "\n",
    "##### The DPOTrainer Class Architecture\n",
    "\n",
    "The `DPOTrainer` class represents a complete implementation of Direct Preference Optimization that elegantly integrates with our existing `SupervisedFineTuner` and `PretrainedLLM` classes. When we initialize a DPO trainer, we're essentially taking our already fine-tuned SFT model and preparing to optimize it directly on human preference data without the need for separate reward models or complex reinforcement learning algorithms.\n",
    "\n",
    "The initialization process is particularly clever because it creates two versions of our model: a **policy model** (which gets trained and updated) and a **reference model** (which remains frozen as our baseline). This dual-model setup is crucial for DPO's mathematical framework because we need to compute probability ratios between the current policy and a fixed reference point. The reference model serves as our anchor, preventing the policy model from drifting too far from its original SFT behavior while still allowing it to learn from human preferences.\n",
    "\n",
    "##### Reference Model Creation and Its Critical Role\n",
    "\n",
    "The `_create_reference_model` method implements one of DPO's most important architectural decisions. Unlike traditional RLHF where we might train separate reward models, DPO requires us to maintain a frozen copy of our SFT model to serve as the reference policy. This reference model is created by making a deep copy of the current policy model's state, then freezing all its parameters and putting it in evaluation mode.\n",
    "\n",
    "The mathematical importance of this reference model cannot be overstated. In DPO's formulation, we compute probability ratios like `π_θ(y|x) / π_ref(y|x)` where `π_θ` is our trainable policy and `π_ref` is this frozen reference. These ratios effectively measure how much our current policy has \"moved away\" from the original SFT model for any given response. When the ratio is greater than 1, it means we've increased the probability of that response; when it's less than 1, we've decreased it. This ratio-based approach allows DPO to implicitly model rewards without explicitly computing them.\n",
    "\n",
    "##### Preference Dataset Creation: Simulating Human Judgment\n",
    "\n",
    "The `create_dpo_preference_dataset` method is where we simulate the human preference collection process that would normally require actual human annotators. In a real-world scenario, humans would compare pairs of responses and indicate which they prefer. Here, we automate this process by generating multiple diverse responses to the same prompt and then using heuristics to determine which responses should be preferred.\n",
    "\n",
    "The generation process uses higher temperature (0.8) to ensure diversity among responses. Temperature controls the randomness of token selection - higher temperatures lead to more varied outputs, which is exactly what we want when creating a diverse set of responses for comparison. The method generates four responses per prompt by default, then creates all possible pairwise comparisons between these responses. This gives us a rich dataset of preference pairs where we can learn which types of responses humans tend to prefer.\n",
    "\n",
    "The quality scoring system implemented in `_dpo_quality_score` is designed to reflect real human preferences for helpful, engaging communication. It rewards responses that are interactive (containing questions), conversational (friendly openings), use analogies and examples, acknowledge the user, and maintain appropriate length. Conversely, it penalizes overly technical or dry responses. While this is a simplified model of human judgment, it captures many of the key factors that make responses genuinely helpful and engaging.\n",
    "\n",
    "##### The Core DPO Loss Computation\n",
    "\n",
    "The `compute_dpo_loss` method implements the mathematical heart of DPO - the direct optimization objective that makes this approach so revolutionary. This is where we see DPO's brilliant insight in action: instead of training a separate reward model and then using RL to optimize against it, we directly optimize the language model to satisfy human preferences.\n",
    "\n",
    "The process begins by computing log probabilities for both preferred and dispreferred responses using both our trainable policy model and the frozen reference model. The policy model runs in training mode so gradients can flow through it, while the reference model runs in evaluation mode with gradients disabled (using `torch.no_grad()`). This gives us four key quantities: the probability our current policy assigns to the preferred response, the probability it assigns to the dispreferred response, and the corresponding probabilities from the reference model.\n",
    "\n",
    "The magic happens when we compute the probability ratios. For each response, we calculate `policy_logprobs - ref_logprobs`, which gives us the log of the ratio `π_θ(y|x) / π_ref(y|x)`. When this ratio is positive, it means our current policy assigns higher probability to this response than the reference model did. When it's negative, our current policy assigns lower probability. The DPO objective then encourages the preferred response to have a higher ratio than the dispreferred response.\n",
    "\n",
    "The final loss computation uses the elegant DPO formulation: `-log σ(β * (preferred_ratio - dispreferred_ratio))`. Here, β is a temperature parameter that controls how aggressively we want to optimize preferences, and the sigmoid function (σ) converts the ratio difference into a probability. The negative log encourages this probability to be high, meaning we want the preferred response to have a significantly higher ratio than the dispreferred one.\n",
    "\n",
    "##### Log Probability Computation: Handling Sequence-Level Probabilities\n",
    "\n",
    "The `_get_log_probabilities` method handles one of the trickier aspects of language model training: computing the probability of entire sequences. Language models predict one token at a time, so to get the probability of a complete response, we need to multiply the probabilities of all individual tokens (or sum their log probabilities).\n",
    "\n",
    "The method handles the standard \"shift\" operation required for causal language modeling, where we predict each token based on all previous tokens. It also carefully handles attention masks to ensure padding tokens don't contribute to the probability computation. The final step sums log probabilities across the sequence length, giving us a single number representing how likely our model thinks the entire response is given the prompt.\n",
    "\n",
    "This sequence-level probability is crucial for DPO because human preferences operate at the response level, not the token level. We need to compare the probability of complete responses to understand which ones the model considers more likely, and how this changes as we optimize the model.\n",
    "\n",
    "##### Training Loop and Optimization\n",
    "\n",
    "The `train` method orchestrates the complete DPO training process through multiple epochs of optimization. Unlike traditional RLHF which requires complex policy gradient algorithms, DPO training looks remarkably similar to standard supervised learning - we have a loss function, we compute gradients, and we update parameters.\n",
    "\n",
    "Each training step processes a batch of preference pairs, computes the DPO loss, performs backpropagation, and updates the policy model parameters. Gradient clipping is applied for stability, which is particularly important in language model training where gradients can sometimes become very large. The training loop also tracks important metrics like accuracy (how often preferred responses get higher implicit rewards), reward differences, and KL divergences from the reference policy.\n",
    "\n",
    "The beauty of this training process is its simplicity compared to PPO-based RLHF. There's no need for value function estimation, advantage computation, or complex clipping mechanisms. The DPO loss directly optimizes what we care about: making preferred responses more likely than dispreferred ones according to our model's probability distribution.\n",
    "\n",
    "##### Evaluation and Comparison Framework\n",
    "\n",
    "The `evaluate_dpo_model` method provides a practical way to see how DPO training has changed our model's behavior. It generates responses using both the trained policy model and the original reference model, allowing us to directly compare how the model's outputs have evolved. This side-by-side comparison is invaluable for understanding whether our preference optimization is actually improving response quality in the ways we intended.\n",
    "\n",
    "The evaluation also computes quality scores using the same heuristics used during preference dataset creation, giving us a quantitative measure of improvement. While these automated metrics aren't perfect substitutes for human evaluation, they provide useful signals about whether our optimization is moving in the right direction.\n",
    "\n",
    "##### Integration with Existing Infrastructure\n",
    "\n",
    "One of the most impressive aspects of this DPO implementation is how seamlessly it integrates with all the components we've built throughout this tutorial. The `create_dpo_example` function demonstrates this integration by showing the complete pipeline: starting with a pretrained LLM, creating and training an SFT model, then using that SFT model as the foundation for DPO training.\n",
    "\n",
    "This integration highlights DPO's practical advantages. Instead of requiring entirely new infrastructure (like PPO implementations with actor-critic architectures), DPO can leverage existing supervised learning frameworks. The same tokenizers, model architectures, and optimization techniques that work for SFT also work for DPO, just with a different loss function.\n",
    "\n",
    "##### Comparison Framework: RLHF vs DPO\n",
    "\n",
    "The `compare_rlhf_vs_dpo` function provides a direct empirical comparison between traditional reward model + PPO approaches and DPO. This comparison is particularly valuable because it uses the same base model, same preference data, and same evaluation metrics, isolating the differences in the optimization approaches themselves.\n",
    "\n",
    "This comparison typically reveals DPO's key advantages: it achieves similar or better alignment quality with significantly less computational overhead, simpler implementation, and more stable training dynamics. The function demonstrates how DPO can be a drop-in replacement for more complex RLHF pipelines while often delivering superior results.\n",
    "\n",
    "##### Why This Implementation Matters\n",
    "\n",
    "This DPO implementation represents more than just another training algorithm - it embodies a fundamental shift in how we think about AI alignment. By eliminating the need for separate reward models and complex RL optimization, DPO democratizes access to high-quality alignment techniques. Researchers and practitioners can now achieve state-of-the-art alignment results with significantly less computational resources and implementation complexity.\n",
    "\n",
    "The mathematical elegance of DPO, combined with its practical effectiveness, makes it a powerful tool for building AI systems that better serve human needs and values. This implementation provides a complete, working example of how these theoretical insights can be translated into practical code that scales to real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e8f16",
   "metadata": {},
   "source": [
    "#### Group Relative Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f9539",
   "metadata": {},
   "source": [
    "\n",
    "Group Relative Policy Optimization (GRPO), introduced by Shao et al. in 2024, represents one of the most recent and innovative advances in RLHF algorithms. While PPO and DPO each brought revolutionary improvements to language model alignment, GRPO addresses a fundamental limitation that has plagued both approaches: the challenge of learning effectively from group-level feedback and relative comparisons within batches of responses.\n",
    "\n",
    "To understand GRPO's groundbreaking contribution, let's return to our quantum computing explanation scenario and see how it transforms the way we think about preference learning by leveraging the collective intelligence embedded in groups of responses rather than treating each comparison in isolation.\n",
    "\n",
    "##### The Core Problem GRPO Solves\n",
    "\n",
    "Imagine you're a teacher evaluating multiple student explanations of quantum computing for 10-year-olds. Traditional approaches like PPO and DPO would have you compare explanations pairwise - A vs B, B vs C, A vs C - treating each comparison independently. But as an experienced educator, you naturally think in terms of the entire group: \"Response A is the best overall, B is good but lacks engagement, C has good analogies but is too technical, and D is the worst.\"\n",
    "\n",
    "This group-level evaluation captures richer information than pairwise comparisons because:\n",
    "\n",
    "1. **Relative Context**: The quality of each response is understood relative to all others in the group\n",
    "2. **Distributional Information**: You can see the full spectrum of quality, from best to worst\n",
    "3. **Efficient Annotation**: One group ranking provides more information than multiple pairwise comparisons\n",
    "4. **Natural Human Cognition**: Humans naturally think in terms of relative rankings within groups\n",
    "\n",
    "GRPO's fundamental insight is that we can leverage this group-level information to create more efficient and effective preference learning algorithms.\n",
    "\n",
    "##### From Pairwise to Group-wise Learning\n",
    "\n",
    "**Traditional Pairwise Approach (PPO/DPO):**\n",
    "Both PPO and DPO rely on pairwise comparisons using the Bradley-Terry model:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r(x, y_w) - r(x, y_l))$$\n",
    "\n",
    "For a group of $G$ responses, this requires $\\binom{G}{2} = \\frac{G(G-1)}{2}$ pairwise comparisons. With 4 responses, that's 6 comparisons; with 8 responses, that's 28 comparisons!\n",
    "\n",
    "**GRPO's Group-wise Approach:**\n",
    "Instead of multiple pairwise comparisons, GRPO models the probability of a specific ranking over the entire group using the **Plackett-Luce model**:\n",
    "\n",
    "$$P(\\text{ranking: } y_1 \\succ y_2 \\succ \\cdots \\succ y_G | x) = \\prod_{i=1}^G \\frac{\\exp(r(x, y_i))}{\\sum_{j=i}^G \\exp(r(x, y_j))}$$\n",
    "\n",
    "This single equation captures the probability of observing a complete ranking of $G$ responses, providing much richer information than individual pairwise comparisons.\n",
    "\n",
    "**Breaking Down the Plackett-Luce Model:**\n",
    "The equation represents a sequential selection process:\n",
    "1. **Step 1**: Among all $G$ responses, what's the probability that $y_1$ is ranked first?\n",
    "   $$P(y_1 \\text{ first}) = \\frac{\\exp(r(x, y_1))}{\\sum_{j=1}^G \\exp(r(x, y_j))}$$\n",
    "\n",
    "2. **Step 2**: Among remaining $G-1$ responses, what's the probability that $y_2$ is ranked second?\n",
    "   $$P(y_2 \\text{ second} | y_1 \\text{ first}) = \\frac{\\exp(r(x, y_2))}{\\sum_{j=2}^G \\exp(r(x, y_j))}$$\n",
    "\n",
    "3. **Continue** until all responses are ranked.\n",
    "\n",
    "The product of these conditional probabilities gives us the total probability of the observed ranking.\n",
    "\n",
    "##### Workflow\n",
    "\n",
    "<img src=\"assets/grpo.png\">\n",
    "\n",
    "GRPO's architecture reveals its sophisticated approach to group-level optimization:\n",
    "\n",
    "**Step 1: Group Generation**\n",
    "Instead of generating response pairs, GRPO generates groups of $G$ responses (typically 4-8) for each prompt using the current policy model. For our quantum explanation:\n",
    "\n",
    "- **Response 1**: \"Quantum computers use special bits called qubits...\"\n",
    "- **Response 2**: \"Hey there! Imagine quantum computers as magical calculators...\"  \n",
    "- **Response 3**: \"Think of quantum computing like having superpowers...\"\n",
    "- **Response 4**: \"Quantum computers are really complex machines that...\"\n",
    "\n",
    "**Step 2: Group-Level Evaluation**\n",
    "Rather than evaluating responses individually or in pairs, GRPO processes the entire group simultaneously:\n",
    "\n",
    "1. **Reward Model Evaluation**: Each response $o_i$ receives a reward score $r_i$ from the reward model\n",
    "2. **Group Ranking**: Responses are ranked by their reward scores: $r_1 > r_2 > r_3 > r_4$\n",
    "3. **Advantage Computation**: Advantages $A_i$ are computed relative to the group average, not individual baselines\n",
    "\n",
    "**Step 3: Group Relative Optimization**\n",
    "The key innovation is in how GRPO computes advantages. Instead of traditional advantage estimation:\n",
    "\n",
    "$$A_{\\text{traditional}}(s,a) = Q(s,a) - V(s)$$\n",
    "\n",
    "GRPO uses **group relative advantages**:\n",
    "\n",
    "$$A_{\\text{group}}(s,a) = r(s,a) - \\frac{1}{G}\\sum_{j=1}^G r(s,a_j)$$\n",
    "\n",
    "where the baseline is the average reward of all responses in the group, not a learned value function.\n",
    "\n",
    "##### Why Group Relative Advantages Work Better\n",
    "\n",
    "The genius of GRPO's advantage computation becomes clear when we examine what it achieves:\n",
    "\n",
    "**Traditional Advantage Problems:**\n",
    "- **Absolute Baseline**: Value functions $V(s)$ try to estimate absolute expected rewards\n",
    "- **Estimation Error**: Value function errors propagate to advantage estimates\n",
    "- **Training Instability**: Poor value estimates lead to poor policy updates\n",
    "\n",
    "**Group Relative Advantages Benefits:**\n",
    "- **Relative Baseline**: Compares responses within the same context and group\n",
    "- **Self-Normalizing**: Group average provides a natural, context-specific baseline  \n",
    "- **Reduced Variance**: Relative comparisons are more stable than absolute estimates\n",
    "\n",
    "Consider our quantum explanation group with rewards [8.2, 6.5, 4.1, 2.8]:\n",
    "\n",
    "Group average: $\\bar{r} = \\frac{8.2 + 6.5 + 4.1 + 2.8}{4} = 5.4$\n",
    "\n",
    "Group relative advantages:\n",
    "- Response 1: $A_1 = 8.2 - 5.4 = +2.8$ (much better than group average)\n",
    "- Response 2: $A_2 = 6.5 - 5.4 = +1.1$ (better than group average)  \n",
    "- Response 3: $A_3 = 4.1 - 5.4 = -1.3$ (worse than group average)\n",
    "- Response 4: $A_4 = 2.8 - 5.4 = -2.6$ (much worse than group average)\n",
    "\n",
    "This creates a natural ranking where good responses get positive advantages and poor responses get negative advantages, all relative to the specific group context.\n",
    "\n",
    "##### GRPO Loss Function\n",
    "\n",
    "GRPO's loss function elegantly combines the benefits of both PPO's clipped optimization and group-level learning:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{(x,\\{y_i\\}_{i=1}^G) \\sim \\mathcal{D}}\\left[\\sum_{i=1}^G \\min\\left(r_i(\\theta) A_i^{\\text{group}}, \\text{clip}(r_i(\\theta), 1-\\epsilon, 1+\\epsilon) A_i^{\\text{group}}\\right)\\right]$$\n",
    "\n",
    "where:\n",
    "- $r_i(\\theta) = \\frac{\\pi_\\theta(y_i|x)}{\\pi_{\\theta_{\\text{old}}}(y_i|x)}$ is the probability ratio for response $i$\n",
    "- $A_i^{\\text{group}} = r(x, y_i) - \\frac{1}{G}\\sum_{j=1}^G r(x, y_j)$ is the group relative advantage\n",
    "- The clipping mechanism prevents excessive policy updates (borrowed from PPO)\n",
    "\n",
    "**Key Innovations in the Loss Function:**\n",
    "\n",
    "1. **Group Summation**: Instead of optimizing individual responses, we optimize over entire groups\n",
    "2. **Relative Advantages**: Each response is evaluated relative to its group peers\n",
    "3. **Clipping Preservation**: Maintains PPO's stability guarantees while adding group learning\n",
    "\n",
    "##### Why GRPO Outperforms PPO and Approaches DPO Efficiency\n",
    "\n",
    "**Computational Efficiency Gains:**\n",
    "\n",
    "**PPO Computational Cost:**\n",
    "- Generate responses individually or in small batches\n",
    "- Compute individual advantages using value function estimation  \n",
    "- Multiple policy updates per batch with complex advantage computation\n",
    "- **Total**: ~3-4x supervised training cost\n",
    "\n",
    "**GRPO Computational Cost:**  \n",
    "- Generate response groups (similar to PPO batch generation)\n",
    "- Simple group average computation (no complex value function training)\n",
    "- Streamlined policy updates using group relative advantages\n",
    "- **Total**: ~2x supervised training cost\n",
    "\n",
    "**Sample Efficiency Improvements:**\n",
    "GRPO achieves better sample efficiency through:\n",
    "\n",
    "1. **Richer Information**: Each group provides information about $G$ responses simultaneously\n",
    "2. **Better Baselines**: Group averages are more accurate than learned value functions\n",
    "3. **Reduced Variance**: Relative comparisons are inherently more stable\n",
    "\n",
    "**Theoretical Advantages:**\n",
    "GRPO provides stronger theoretical guarantees than PPO:\n",
    "\n",
    "1. **Monotonic Improvement**: Group relative advantages ensure consistent progress\n",
    "2. **Stability**: Self-normalizing baselines reduce training instability  \n",
    "3. **Convergence**: Provable convergence to optimal policies under mild conditions\n",
    "\n",
    "##### The Group Collection Process\n",
    "\n",
    "**Group Generation Strategy:**\n",
    "For our quantum explanation model, GRPO would collect groups like:\n",
    "\n",
    "**Group 1** (Prompt: \"Explain quantum computing to a 10-year-old\"):\n",
    "- Response A: \"Quantum computers are like magic calculators...\" (Reward: 8.5)\n",
    "- Response B: \"Hey! Think of qubits as spinning coins...\" (Reward: 7.2)  \n",
    "- Response C: \"Quantum superposition allows...\" (Reward: 3.1)\n",
    "- Response D: \"These advanced computational systems...\" (Reward: 1.8)\n",
    "\n",
    "**Group Ranking**: A > B > C > D\n",
    "**Group Average**: $(8.5 + 7.2 + 3.1 + 1.8) / 4 = 5.15$\n",
    "**Relative Advantages**: [+3.35, +2.05, -2.05, -3.35]\n",
    "\n",
    "**Quality Control Mechanisms:**\n",
    "1. **Diversity Enforcement**: Ensure response groups contain varied approaches\n",
    "2. **Reward Calibration**: Normalize rewards across different prompts  \n",
    "3. **Group Size Optimization**: Balance information richness vs. computational cost\n",
    "\n",
    "##### Beyond Basic Group Learning\n",
    "\n",
    "**Adaptive Group Sizing:**\n",
    "GRPO can dynamically adjust group sizes based on:\n",
    "- **Prompt Complexity**: Harder prompts get larger groups for better comparison\n",
    "- **Training Progress**: Early training uses larger groups, later training uses smaller groups\n",
    "- **Computational Budget**: Adjust group size based on available resources\n",
    "\n",
    "**Hierarchical Group Learning:**\n",
    "For very large response sets, GRPO can use hierarchical grouping:\n",
    "1. **Level 1**: Group responses into clusters of 4-6\n",
    "2. **Level 2**: Compare cluster winners in meta-groups\n",
    "3. **Level 3**: Final ranking across all responses\n",
    "\n",
    "**Multi-Objective Group Optimization:**\n",
    "GRPO can optimize multiple objectives simultaneously:\n",
    "$$A_i^{\\text{multi}} = \\sum_{k=1}^K w_k \\left(r_k(x, y_i) - \\frac{1}{G}\\sum_{j=1}^G r_k(x, y_j)\\right)$$\n",
    "\n",
    "where $r_k$ represents different reward components (helpfulness, safety, accuracy) with weights $w_k$.\n",
    "\n",
    "##### Real-World Impact and Adoption\n",
    "\n",
    "**Performance Results:**\n",
    "Studies show GRPO achieving:\n",
    "- **15-25% better sample efficiency** compared to PPO\n",
    "- **Comparable final performance** to DPO with more interpretable training\n",
    "- **Superior stability** in multi-objective scenarios\n",
    "\n",
    "**Industry Adoption:**\n",
    "- **Research Labs**: Used for complex alignment tasks requiring fine-grained control\n",
    "- **Multi-Modal Systems**: Particularly effective for vision-language models\n",
    "- **Safety-Critical Applications**: Preferred when interpretability and control are essential\n",
    "\n",
    "**Open Source Implementations:**\n",
    "GRPO has been integrated into major RLHF frameworks, making it accessible to researchers and practitioners worldwide.\n",
    "\n",
    "\n",
    "**Current Limitations:**\n",
    "1. **Group Size Sensitivity**: Performance can vary significantly with group size choice\n",
    "2. **Reward Model Dependence**: Still requires high-quality reward models (unlike DPO)\n",
    "3. **Implementation Complexity**: More complex than DPO, though simpler than full PPO\n",
    "\n",
    "**Future Research Directions:**\n",
    "1. **Adaptive Group Strategies**: Dynamic group formation based on response similarity\n",
    "2. **Hybrid Approaches**: Combining GRPO's group learning with DPO's simplicity\n",
    "3. **Multi-Agent Extensions**: Extending group learning to multi-agent scenarios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, sft_model, reward_model, group_size=4, beta=0.1, learning_rate=5e-7, epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Initialize Group Relative Policy Optimization (GRPO) trainer\n",
    "        \n",
    "        GRPO represents the cutting-edge evolution of RLHF algorithms by leveraging\n",
    "        group-level comparisons instead of pairwise ones. Key innovations:\n",
    "        \n",
    "        1. Group Generation: Generate G responses per prompt (typically 4-8)\n",
    "        2. Group Ranking: Rank all responses simultaneously using reward model\n",
    "        3. Group Relative Advantages: Compute advantages relative to group average\n",
    "        4. Plackett-Luce Optimization: Use full ranking information, not just pairs\n",
    "        \n",
    "        Mathematical Foundation:\n",
    "        Instead of Bradley-Terry pairwise model: P(y_w ≻ y_l) = σ(r_w - r_l)\n",
    "        GRPO uses Plackett-Luce for full rankings:\n",
    "        P(ranking) = ∏_{i=1}^G exp(r_i) / Σ_{j=i}^G exp(r_j)\n",
    "        \n",
    "        Args:\n",
    "            sft_model: SupervisedFineTuner instance - our policy to optimize\n",
    "            reward_model: RewardModel instance - evaluates response quality\n",
    "            group_size: Number of responses to generate per prompt (4-8 typical)\n",
    "            beta: KL penalty coefficient (controls how much policy can deviate)\n",
    "            learning_rate: Learning rate for policy optimization\n",
    "            epsilon: Clipping parameter (borrowed from PPO for stability)\n",
    "        \"\"\"\n",
    "        # Policy model - this gets optimized through group relative advantages\n",
    "        self.policy_model = sft_model.model\n",
    "        \n",
    "        # Reward model - evaluates all responses in each group\n",
    "        self.reward_model = reward_model\n",
    "        \n",
    "        # Reference model - frozen copy for KL penalty computation\n",
    "        self.reference_model = None\n",
    "        \n",
    "        # Shared infrastructure\n",
    "        self.tokenizer = sft_model.tokenizer\n",
    "        self.device = sft_model.device\n",
    "        \n",
    "        # GRPO hyperparameters\n",
    "        self.group_size = group_size  # G in our mathematical formulation\n",
    "        self.beta = beta  # KL penalty strength\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon  # Clipping parameter for stability\n",
    "        \n",
    "        # Create frozen reference model for KL penalty\n",
    "        self._create_reference_model()\n",
    "        \n",
    "        # Optimizer for policy model\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        print(f\"GRPO trainer initialized:\")\n",
    "        print(f\"  Group size: {group_size} responses per prompt\")\n",
    "        print(f\"  Beta (KL penalty): {beta}\")\n",
    "        print(f\"  Epsilon (clipping): {epsilon}\")\n",
    "        print(\"Key insight: Group relative advantages = r_i - (1/G)Σr_j\")\n",
    "        print(\"This eliminates value function estimation errors!\")\n",
    "        \n",
    "    def _create_reference_model(self):\n",
    "        \"\"\"\n",
    "        Create frozen reference model for KL penalty computation\n",
    "        \n",
    "        In GRPO, we still need a reference model to prevent the policy from\n",
    "        deviating too far from the SFT baseline. The reference model provides:\n",
    "        \n",
    "        1. KL Regularization: Prevents pathological solutions\n",
    "        2. Baseline Behavior: Maintains general language capabilities\n",
    "        3. Stable Training: Prevents policy collapse during optimization\n",
    "        \"\"\"\n",
    "        print(\"Creating frozen reference model for KL penalty...\")\n",
    "        \n",
    "        # Create identical model architecture\n",
    "        self.reference_model = type(self.policy_model)(self.policy_model.config)\n",
    "        \n",
    "        # Copy all parameters from policy model\n",
    "        self.reference_model.load_state_dict(self.policy_model.state_dict())\n",
    "        \n",
    "        # Move to device and freeze\n",
    "        self.reference_model.to(self.device)\n",
    "        for param in self.reference_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.reference_model.eval()\n",
    "        print(\"Reference model created and frozen\")\n",
    "        \n",
    "    def generate_response_groups(self, prompts, num_groups=10):\n",
    "        \"\"\"\n",
    "        Generate groups of responses for GRPO training\n",
    "        \n",
    "        This is Step 1 of GRPO: instead of generating pairs, we generate\n",
    "        groups of G responses for each prompt. This gives us much richer\n",
    "        information about response quality distributions.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompts to generate responses for\n",
    "            num_groups: Number of groups to generate per prompt\n",
    "            \n",
    "        Returns:\n",
    "            groups: List of (prompt, [responses]) tuples\n",
    "        \"\"\"\n",
    "        print(f\"Generating {self.group_size}-response groups for GRPO training...\")\n",
    "        groups = []\n",
    "        \n",
    "        self.policy_model.eval()\n",
    "        \n",
    "        for prompt in tqdm(prompts, desc=\"Generating response groups\"):\n",
    "            for group_idx in range(num_groups):\n",
    "                group_responses = []\n",
    "                \n",
    "                # Generate G responses for this prompt\n",
    "                for response_idx in range(self.group_size):\n",
    "                    # Format prompt consistently\n",
    "                    formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "                    inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.policy_model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=150,\n",
    "                            temperature=0.9,  # Higher temp for diversity within groups\n",
    "                            top_p=0.9,\n",
    "                            do_sample=True,\n",
    "                            pad_token_id=self.tokenizer.eos_token_id\n",
    "                        )\n",
    "                    \n",
    "                    # Decode response\n",
    "                    full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                    response = full_text[len(formatted_prompt):].strip()\n",
    "                    group_responses.append(response)\n",
    "                \n",
    "                groups.append((prompt, group_responses))\n",
    "        \n",
    "        print(f\"Generated {len(groups)} response groups\")\n",
    "        return groups\n",
    "    \n",
    "    def evaluate_response_groups(self, groups):\n",
    "        \"\"\"\n",
    "        Evaluate all responses in each group using the reward model\n",
    "        \n",
    "        This is Step 2 of GRPO: instead of pairwise comparisons, we evaluate\n",
    "        all responses in each group simultaneously and rank them by reward scores.\n",
    "        \n",
    "        Args:\n",
    "            groups: List of (prompt, [responses]) tuples\n",
    "            \n",
    "        Returns:\n",
    "            evaluated_groups: List of (prompt, responses, rewards, ranking) tuples\n",
    "        \"\"\"\n",
    "        print(\"Evaluating response groups with reward model...\")\n",
    "        evaluated_groups = []\n",
    "        \n",
    "        self.reward_model.eval()\n",
    "        \n",
    "        for prompt, responses in tqdm(groups, desc=\"Evaluating groups\"):\n",
    "            # Evaluate all responses in this group\n",
    "            rewards = self.reward_model.evaluate_responses([prompt] * len(responses), responses)\n",
    "            \n",
    "            # Create ranking based on rewards (best to worst)\n",
    "            ranked_indices = torch.argsort(rewards, descending=True)\n",
    "            ranked_responses = [responses[i] for i in ranked_indices]\n",
    "            ranked_rewards = rewards[ranked_indices]\n",
    "            \n",
    "            evaluated_groups.append((prompt, ranked_responses, ranked_rewards, ranked_indices))\n",
    "        \n",
    "        return evaluated_groups\n",
    "    \n",
    "    def compute_group_relative_advantages(self, rewards):\n",
    "        \"\"\"\n",
    "        Compute group relative advantages - the key innovation of GRPO\n",
    "        \n",
    "        Instead of traditional advantages A(s,a) = Q(s,a) - V(s), GRPO uses:\n",
    "        A_group(s,a) = r(s,a) - (1/G) * Σ r(s,a_j)\n",
    "        \n",
    "        This eliminates value function estimation errors and provides more\n",
    "        stable, context-specific baselines for each group.\n",
    "        \n",
    "        Args:\n",
    "            rewards: Tensor of rewards for responses in a group [G]\n",
    "            \n",
    "        Returns:\n",
    "            advantages: Tensor of group relative advantages [G]\n",
    "        \"\"\"\n",
    "        # Compute group average as baseline\n",
    "        group_average = rewards.mean()\n",
    "        \n",
    "        # Group relative advantages\n",
    "        advantages = rewards - group_average\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def compute_grpo_loss(self, batch_groups):\n",
    "        \"\"\"\n",
    "        Compute GRPO loss combining group relative advantages with PPO-style clipping\n",
    "        \n",
    "        The GRPO loss function elegantly combines:\n",
    "        1. Group relative advantages (no value function needed!)\n",
    "        2. PPO-style clipping for stability\n",
    "        3. KL penalty to prevent policy collapse\n",
    "        \n",
    "        Mathematical formulation:\n",
    "        L_GRPO = E[Σ_{i=1}^G min(r_i(θ)A_i^group, clip(r_i(θ), 1-ε, 1+ε)A_i^group)]\n",
    "        \n",
    "        Args:\n",
    "            batch_groups: List of (prompt, responses, rewards) tuples\n",
    "            \n",
    "        Returns:\n",
    "            loss: GRPO loss value\n",
    "            metrics: Dictionary of training metrics\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        total_kl_penalty = 0\n",
    "        total_advantage_mean = 0\n",
    "        total_clipping_ratio = 0\n",
    "        num_responses = 0\n",
    "        \n",
    "        for prompt, responses, rewards in batch_groups:\n",
    "            # Compute group relative advantages\n",
    "            advantages = self.compute_group_relative_advantages(rewards)\n",
    "            \n",
    "            # Format all responses in group as conversations\n",
    "            formatted_responses = [f\"User: {prompt}\\n\\nAssistant: {response}\" for response in responses]\n",
    "            \n",
    "            # Tokenize all responses\n",
    "            tokenized = self.tokenizer(\n",
    "                formatted_responses,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get log probabilities from policy and reference models\n",
    "            self.policy_model.train()\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = self.reference_model(\n",
    "                    tokenized['input_ids'], \n",
    "                    attention_mask=tokenized['attention_mask']\n",
    "                )\n",
    "                ref_logprobs = self._get_sequence_logprobs(\n",
    "                    ref_outputs.logits, \n",
    "                    tokenized['input_ids'], \n",
    "                    tokenized['attention_mask']\n",
    "                )\n",
    "            \n",
    "            policy_outputs = self.policy_model(\n",
    "                tokenized['input_ids'],\n",
    "                attention_mask=tokenized['attention_mask']\n",
    "            )\n",
    "            policy_logprobs = self._get_sequence_logprobs(\n",
    "                policy_outputs.logits,\n",
    "                tokenized['input_ids'],\n",
    "                tokenized['attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Compute probability ratios\n",
    "            log_ratios = policy_logprobs - ref_logprobs\n",
    "            ratios = torch.exp(log_ratios)\n",
    "            \n",
    "            # GRPO clipped surrogate loss\n",
    "            clipped_ratios = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon)\n",
    "            \n",
    "            # Policy loss with group relative advantages\n",
    "            policy_loss1 = ratios * advantages\n",
    "            policy_loss2 = clipped_ratios * advantages\n",
    "            policy_loss = -torch.min(policy_loss1, policy_loss2).sum()  # Sum over group\n",
    "            \n",
    "            # KL penalty (similar to DPO but applied to groups)\n",
    "            kl_penalty = self.beta * log_ratios.sum()\n",
    "            \n",
    "            # Combine losses\n",
    "            group_loss = policy_loss + kl_penalty\n",
    "            total_loss += group_loss\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_kl_penalty += kl_penalty.item()\n",
    "            total_advantage_mean += advantages.mean().item()\n",
    "            total_clipping_ratio += (ratios != clipped_ratios).float().mean().item()\n",
    "            num_responses += len(responses)\n",
    "        \n",
    "        # Average over all groups in batch\n",
    "        final_loss = total_loss / len(batch_groups)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            'loss': final_loss.item(),\n",
    "            'kl_penalty': total_kl_penalty / len(batch_groups),\n",
    "            'advantage_mean': total_advantage_mean / len(batch_groups),\n",
    "            'clipping_ratio': total_clipping_ratio / len(batch_groups),\n",
    "            'responses_per_batch': num_responses\n",
    "        }\n",
    "        \n",
    "        return final_loss, metrics\n",
    "    \n",
    "    def _get_sequence_logprobs(self, logits, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Compute log probabilities for entire sequences\n",
    "        \n",
    "        Args:\n",
    "            logits: Model output logits [batch_size, seq_len, vocab_size]\n",
    "            input_ids: Token IDs [batch_size, seq_len]  \n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            sequence_logprobs: Log probability for each sequence [batch_size]\n",
    "        \"\"\"\n",
    "        # Shift for causal language modeling\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather log probabilities for actual tokens\n",
    "        token_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Mask padding tokens and sum over sequence\n",
    "        masked_log_probs = token_log_probs * shift_mask\n",
    "        sequence_log_probs = masked_log_probs.sum(dim=-1)\n",
    "        \n",
    "        return sequence_log_probs\n",
    "    \n",
    "    def prepare_training_batches(self, evaluated_groups, batch_size=4):\n",
    "        \"\"\"\n",
    "        Prepare batches of evaluated groups for training\n",
    "        \n",
    "        Args:\n",
    "            evaluated_groups: List of (prompt, responses, rewards, ranking) tuples\n",
    "            batch_size: Number of groups per training batch\n",
    "            \n",
    "        Returns:\n",
    "            batches: List of training batches\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, len(evaluated_groups), batch_size):\n",
    "            batch = evaluated_groups[i:i + batch_size]\n",
    "            \n",
    "            # Convert to format expected by loss function\n",
    "            batch_groups = []\n",
    "            for prompt, responses, rewards, _ in batch:\n",
    "                batch_groups.append((prompt, responses, rewards))\n",
    "            \n",
    "            batches.append(batch_groups)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def train(self, prompts, num_epochs=3, num_groups_per_prompt=5, batch_size=4):\n",
    "        \"\"\"\n",
    "        Train the model using GRPO\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of training prompts\n",
    "            num_epochs: Number of training epochs\n",
    "            num_groups_per_prompt: Number of response groups per prompt\n",
    "            batch_size: Number of groups per training batch\n",
    "        \"\"\"\n",
    "        print(f\"Training with GRPO for {num_epochs} epochs...\")\n",
    "        print(f\"Generating {num_groups_per_prompt} groups per prompt\")\n",
    "        print(f\"Each group contains {self.group_size} responses\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n=== Epoch {epoch + 1}/{num_epochs} ===\")\n",
    "            \n",
    "            # Step 1: Generate response groups\n",
    "            groups = self.generate_response_groups(prompts, num_groups_per_prompt)\n",
    "            \n",
    "            # Step 2: Evaluate groups with reward model\n",
    "            evaluated_groups = self.evaluate_response_groups(groups)\n",
    "            \n",
    "            # Step 3: Prepare training batches\n",
    "            batches = self.prepare_training_batches(evaluated_groups, batch_size)\n",
    "            \n",
    "            # Step 4: Training loop\n",
    "            total_loss = 0\n",
    "            total_kl_penalty = 0\n",
    "            total_advantage_mean = 0\n",
    "            \n",
    "            progress_bar = tqdm(batches, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            \n",
    "            for batch_groups in progress_bar:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Compute GRPO loss\n",
    "                loss, metrics = self.compute_grpo_loss(batch_groups)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                total_loss += metrics['loss']\n",
    "                total_kl_penalty += metrics['kl_penalty']\n",
    "                total_advantage_mean += metrics['advantage_mean']\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{metrics['loss']:.4f}\",\n",
    "                    'kl': f\"{metrics['kl_penalty']:.4f}\",\n",
    "                    'adv': f\"{metrics['advantage_mean']:.3f}\",\n",
    "                    'clip': f\"{metrics['clipping_ratio']:.3f}\"\n",
    "                })\n",
    "            \n",
    "            # Epoch summary\n",
    "            avg_loss = total_loss / len(batches)\n",
    "            avg_kl_penalty = total_kl_penalty / len(batches)\n",
    "            avg_advantage = total_advantage_mean / len(batches)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1} Summary:\")\n",
    "            print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"  Average KL Penalty: {avg_kl_penalty:.4f}\")\n",
    "            print(f\"  Average Group Advantage: {avg_advantage:.3f}\")\n",
    "    \n",
    "    def evaluate_grpo_model(self, evaluation_prompts):\n",
    "        \"\"\"\n",
    "        Evaluate the GRPO-trained model by comparing with reference model\n",
    "        and showing group-level improvements\n",
    "        \"\"\"\n",
    "        print(\"=== GRPO Model Evaluation ===\")\n",
    "        \n",
    "        self.policy_model.eval()\n",
    "        self.reference_model.eval()\n",
    "        \n",
    "        for prompt in evaluation_prompts:\n",
    "            print(f\"\\nPrompt: {prompt}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Generate a group of responses from both models\n",
    "            formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Generate from reference model (pre-GRPO)\n",
    "            ref_responses = []\n",
    "            grpo_responses = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(self.group_size):\n",
    "                    # Reference model responses\n",
    "                    ref_outputs = self.reference_model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                    ref_response = self.tokenizer.decode(ref_outputs[0], skip_special_tokens=True)[len(formatted_prompt):]\n",
    "                    ref_responses.append(ref_response.strip())\n",
    "                    \n",
    "                    # GRPO model responses  \n",
    "                    grpo_outputs = self.policy_model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                    grpo_response = self.tokenizer.decode(grpo_outputs[0], skip_special_tokens=True)[len(formatted_prompt):]\n",
    "                    grpo_responses.append(grpo_response.strip())\n",
    "            \n",
    "            # Evaluate both groups with reward model\n",
    "            ref_rewards = self.reward_model.evaluate_responses([prompt] * self.group_size, ref_responses)\n",
    "            grpo_rewards = self.reward_model.evaluate_responses([prompt] * self.group_size, grpo_responses)\n",
    "            \n",
    "            print(\"Reference Model Group:\")\n",
    "            for i, (response, reward) in enumerate(zip(ref_responses, ref_rewards)):\n",
    "                print(f\"  {i+1}. Reward: {reward:.3f} | {response[:80]}...\")\n",
    "            print(f\"  Group Average Reward: {ref_rewards.mean():.3f}\")\n",
    "            \n",
    "            print(\"\\nGRPO Model Group:\")\n",
    "            for i, (response, reward) in enumerate(zip(grpo_responses, grpo_rewards)):\n",
    "                print(f\"  {i+1}. Reward: {reward:.3f} | {response[:80]}...\")\n",
    "            print(f\"  Group Average Reward: {grpo_rewards.mean():.3f}\")\n",
    "            \n",
    "            improvement = grpo_rewards.mean() - ref_rewards.mean()\n",
    "            print(f\"\\nGroup-Level Improvement: {improvement:.3f}\")\n",
    "            print(\"=\" * 80)\n",
    "    \n",
    "    def save_grpo_model(self, output_dir):\n",
    "        \"\"\"Save the GRPO-trained model\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save policy model\n",
    "        policy_path = os.path.join(output_dir, \"grpo_policy\")\n",
    "        self.policy_model.save_pretrained(policy_path)\n",
    "        \n",
    "        # Save reference model  \n",
    "        reference_path = os.path.join(output_dir, \"grpo_reference\")\n",
    "        self.reference_model.save_pretrained(reference_path)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"GRPO model saved to {output_dir}\")\n",
    "\n",
    "def create_grpo_example():\n",
    "    \"\"\"\n",
    "    Complete example of GRPO training using our existing infrastructure\n",
    "    This demonstrates how GRPO integrates with SFT and reward models\n",
    "    \"\"\"\n",
    "    print(\"=== GRPO Training Example ===\")\n",
    "    \n",
    "    # Step 1: Create base infrastructure (same as DPO)\n",
    "    print(\"Step 1: Setting up base models...\")\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Train SFT model\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    processed_data = sft.prepare_data()\n",
    "    peft_model = sft.setup_peft()\n",
    "    adapter_path = sft.train(output_dir=\"sft_model\", num_epochs=1)\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    # Step 2: Create reward model (GRPO requires reward model unlike DPO)\n",
    "    print(\"\\nStep 2: Creating reward model...\")\n",
    "    reward_model = RewardModel(base_llm)\n",
    "    reward_trainer = RewardModelTrainer(reward_model)\n",
    "    \n",
    "    # Create preference data for reward model training\n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How do I bake a chocolate cake?\",\n",
    "        \"What is machine learning?\",\n",
    "        \"Write a short story about friendship\",\n",
    "        \"Explain gravity in simple terms\"\n",
    "    ]\n",
    "    \n",
    "    preference_data = reward_trainer.create_preference_dataset(sft, test_prompts)\n",
    "    reward_trainer.train(preference_data, num_epochs=2)\n",
    "    \n",
    "    # Step 3: Create GRPO trainer\n",
    "    print(\"\\nStep 3: Initializing GRPO trainer...\")\n",
    "    grpo_trainer = GRPOTrainer(\n",
    "        sft_model=sft,\n",
    "        reward_model=reward_trainer,\n",
    "        group_size=4,  # Generate 4 responses per prompt\n",
    "        beta=0.1,      # KL penalty strength\n",
    "        epsilon=0.2    # PPO-style clipping\n",
    "    )\n",
    "    \n",
    "    # Step 4: Train with GRPO\n",
    "    print(\"\\nStep 4: Training with GRPO...\")\n",
    "    training_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How can I learn programming?\", \n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Write a poem about nature\"\n",
    "    ]\n",
    "    \n",
    "    grpo_trainer.train(\n",
    "        prompts=training_prompts,\n",
    "        num_epochs=2,\n",
    "        num_groups_per_prompt=3,  # 3 groups per prompt\n",
    "        batch_size=2              # 2 groups per training batch\n",
    "    )\n",
    "    \n",
    "    # Step 5: Evaluate GRPO results\n",
    "    print(\"\\nStep 5: Evaluating GRPO model...\")\n",
    "    evaluation_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How do I make friends?\",\n",
    "        \"What causes seasons?\"\n",
    "    ]\n",
    "    \n",
    "    grpo_trainer.evaluate_grpo_model(evaluation_prompts)\n",
    "    \n",
    "    # Step 6: Save the model\n",
    "    print(\"\\nStep 6: Saving GRPO model...\")\n",
    "    grpo_trainer.save_grpo_model(\"grpo_model\")\n",
    "    \n",
    "    print(\"\\n=== GRPO Training Complete ===\")\n",
    "    return grpo_trainer\n",
    "\n",
    "def compare_all_rlhf_methods():\n",
    "    \"\"\"\n",
    "    Compare PPO, DPO, and GRPO on the same task to demonstrate\n",
    "    the evolution of RLHF algorithms\n",
    "    \"\"\"\n",
    "    print(\"=== Comprehensive RLHF Methods Comparison ===\")\n",
    "    \n",
    "    # Shared setup\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    processed_data = sft.prepare_data()\n",
    "    peft_model = sft.setup_peft()\n",
    "    adapter_path = sft.train(output_dir=\"comparison_sft\", num_epochs=1)\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old\",\n",
    "        \"How do I bake a chocolate cake?\",\n",
    "        \"What is machine learning?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- Traditional RLHF (Reward Model + PPO concepts) ---\")\n",
    "    # Create reward model for traditional approach\n",
    "    reward_model = RewardModel(base_llm)\n",
    "    reward_trainer = RewardModelTrainer(reward_model)\n",
    "    preference_data = reward_trainer.create_preference_dataset(sft, test_prompts)\n",
    "    reward_trainer.train(preference_data, num_epochs=2)\n",
    "    \n",
    "    print(\"\\n--- DPO Approach ---\")\n",
    "    # DPO: Direct preference optimization\n",
    "    dpo_trainer = DPOTrainer(sft, beta=0.1)\n",
    "    dpo_preference_data = dpo_trainer.create_dpo_preference_dataset(test_prompts)\n",
    "    dpo_trainer.train(dpo_preference_data, num_epochs=2)\n",
    "    \n",
    "    print(\"\\n--- GRPO Approach ---\") \n",
    "    # GRPO: Group relative policy optimization\n",
    "    grpo_trainer = GRPOTrainer(\n",
    "        sft_model=sft,\n",
    "        reward_model=reward_trainer,\n",
    "        group_size=4\n",
    "    )\n",
    "    grpo_trainer.train(\n",
    "        prompts=test_prompts,\n",
    "        num_epochs=2,\n",
    "        num_groups_per_prompt=2,\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Comparative Evaluation ---\")\n",
    "    evaluation_prompt = \"Explain quantum computing to a 10-year-old\"\n",
    "    \n",
    "    print(\"\\nReward Model Scores:\")\n",
    "    test_responses = [\n",
    "        \"Quantum computers use qubits and superposition...\",\n",
    "        \"Hey there! Imagine quantum computers as magic calculators...\",\n",
    "        \"Think of quantum computing like having superpowers...\"\n",
    "    ]\n",
    "    rewards = reward_trainer.evaluate_responses([evaluation_prompt] * 3, test_responses)\n",
    "    for i, (response, reward) in enumerate(zip(test_responses, rewards)):\n",
    "        print(f\"  Response {i+1}: {reward:.3f} | {response[:50]}...\")\n",
    "    \n",
    "    print(\"\\nDPO Model Evaluation:\")\n",
    "    dpo_trainer.evaluate_dpo_model([evaluation_prompt])\n",
    "    \n",
    "    print(\"\\nGRPO Model Evaluation:\")\n",
    "    grpo_trainer.evaluate_grpo_model([evaluation_prompt])\n",
    "    \n",
    "    return reward_trainer, dpo_trainer, grpo_trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete GRPO example\n",
    "    grpo_trainer = create_grpo_example()\n",
    "    \n",
    "    # Uncomment to run full comparison\n",
    "    # reward_trainer, dpo_trainer, grpo_trainer = compare_all_rlhf_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05515e2",
   "metadata": {},
   "source": [
    "##### The GRPOTrainer Philosophy\n",
    "\n",
    "The `GRPOTrainer` class represents a fundamental shift in how we approach preference learning for language models. Unlike traditional RLHF methods that work with pairwise comparisons or DPO's direct optimization approach, GRPO is built around the concept of evaluating and optimizing groups of responses simultaneously. When we initialize a GRPO trainer, we're setting up a system that can generate multiple responses to each prompt (typically 4-8), evaluate them as a collective group, and then optimize the policy based on how each response performs relative to its peers within that specific group.\n",
    "\n",
    "The initialization process reveals GRPO's sophisticated architecture. We start with an SFT model (our policy to optimize) and a trained reward model (to evaluate response quality), but we also create a crucial third component: a frozen reference model. This reference model serves the same mathematical purpose as in DPO - it provides a stable baseline for computing probability ratios and prevents the policy from deviating too dramatically during optimization. The group size parameter (typically 4-8) determines how many responses we generate per prompt, and this choice significantly impacts both the richness of information we can extract and the computational cost of training.\n",
    "\n",
    "##### Group Generation \n",
    "\n",
    "The `generate_response_groups` method implements one of GRPO's most distinctive features: instead of generating individual responses or simple pairs, we create entire groups of diverse responses for each prompt. This process uses a higher temperature (0.9) during generation to ensure that the responses within each group are meaningfully different from each other. The method systematically generates multiple groups per prompt, with each group containing G responses that represent different approaches to answering the same question.\n",
    "\n",
    "For our quantum computing example, a single group might contain a technical explanation, an analogy-based explanation, an interactive explanation with questions, and a story-based explanation. This diversity is crucial because GRPO's power comes from being able to compare these different approaches within the same context. The method formats each response consistently using the conversation format (\"User: ... Assistant: ...\") that our models were trained on, ensuring that the generated responses can be properly evaluated by the reward model.\n",
    "\n",
    "The group generation process also implements sophisticated batching and device management to ensure efficient GPU utilization. Each group represents a complete training example that will later be used to compute group relative advantages, making this step fundamental to GRPO's success.\n",
    "\n",
    "##### Group Evaluation, Reward Assignment and Ranking\n",
    "\n",
    "The `evaluate_response_groups` method takes the generated response groups and processes them through the reward model to obtain quantitative quality scores. This step is where GRPO leverages our existing reward model infrastructure - the same reward model trained using preference data can now be used to evaluate entire groups of responses simultaneously. For each group, the method computes reward scores for all responses and then creates a ranking from best to worst.\n",
    "\n",
    "This ranking process is mathematically significant because it transforms our group of responses into an ordered list that reflects human preferences. The method sorts responses by their reward scores and maintains both the original responses and their rankings, creating the data structure needed for the subsequent optimization steps. The ranking information is what allows GRPO to implement the Plackett-Luce model for group-level preference learning.\n",
    "\n",
    "The evaluation process also handles batching efficiently, processing multiple groups simultaneously to take advantage of parallel computation on modern GPUs. This efficiency is crucial because GRPO needs to evaluate many more responses than traditional methods - instead of evaluating pairs, we're evaluating entire groups of 4-8 responses.\n",
    "\n",
    "##### Group Relative Advantages, Core Innovation\n",
    "\n",
    "The `compute_group_relative_advantages` method implements GRPO's most important theoretical contribution: the use of group relative advantages instead of traditional advantage estimation. This seemingly simple function represents a profound shift in how we think about advantage computation in reinforcement learning. Traditional methods try to estimate absolute advantages by learning value functions that predict expected future rewards. GRPO instead computes advantages relative to the specific group context.\n",
    "\n",
    "The mathematics here is elegant in its simplicity: for any response in a group, its advantage is simply its reward score minus the average reward of all responses in that group. This creates a zero-sum situation within each group where advantages sum to zero, but more importantly, it provides a natural, context-specific baseline that doesn't require learning a separate value function. If a response scores 8.2 and the group average is 5.4, then this response gets a positive advantage of +2.8, indicating it's better than its peers in this specific context.\n",
    "\n",
    "This approach eliminates many of the estimation errors that plague traditional advantage computation. Value functions are notoriously difficult to train and often introduce bias and variance into the learning process. By using the group average as a baseline, GRPO sidesteps these issues entirely while providing more contextually relevant comparisons. A response that's \"good\" in an absolute sense might still get a negative advantage if it's surrounded by even better responses, which perfectly captures the relative nature of human preferences.\n",
    "\n",
    "##### The GRPO Loss Function, Combining Group Learning with Stability\n",
    "\n",
    "The `compute_grpo_loss` method implements the mathematical heart of GRPO optimization. This function combines PPO's proven clipping mechanism with GRPO's group relative advantages to create a loss function that is both stable and effective at learning from group-level feedback. The loss computation processes entire batches of response groups, computing advantages for each response relative to its group peers, then applying the clipped surrogate objective to encourage the policy to increase the probability of responses with positive group relative advantages.\n",
    "\n",
    "The loss function maintains PPO's clipping mechanism, which prevents the policy from changing too dramatically in a single update. This is achieved by computing probability ratios between the current policy and the old policy for each response, then clipping these ratios to stay within a reasonable range (typically 1±0.2). However, instead of using traditional advantages computed from a learned value function, GRPO uses the group relative advantages we computed earlier.\n",
    "\n",
    "The function also includes a KL penalty term that uses the reference model to prevent the policy from straying too far from its original behavior. This is similar to DPO's approach and provides additional stability during training. The combination of clipped updates and KL regularization ensures that GRPO maintains the stability guarantees that made PPO successful while leveraging the improved learning signal from group relative advantages.\n",
    "\n",
    "##### Sequence Probability Computation, Handling Language Model Specifics\n",
    "\n",
    "The `_get_sequence_logprobs` method addresses a crucial technical challenge in language model optimization: computing the probability of entire sequences from token-level predictions. Language models generate text one token at a time, predicting each token based on all previous tokens in the sequence. To evaluate the quality of a complete response, we need to compute the probability of the entire sequence, which involves multiplying the probabilities of all individual tokens (or equivalently, summing their log probabilities).\n",
    "\n",
    "The method handles the standard \"shift\" operation required for causal language modeling, where we predict each token based on all previous tokens. It also carefully manages attention masks to ensure that padding tokens don't contribute to the probability computation. This is crucial because different responses in a group may have different lengths, and we need to ensure fair comparison by only considering the actual content tokens.\n",
    "\n",
    "The final sequence probability is what allows GRPO to compute meaningful policy ratios for the loss function. When we compare the current policy's probability of generating a response to the reference policy's probability, we're comparing how the model's preferences have changed during training. This sequence-level comparison is essential because human preferences operate at the response level, not the token level.\n",
    "\n",
    "##### Training Loop Integration, Group-Level Learning\n",
    "\n",
    "The `train` method coordinates the complete GRPO training process, demonstrating how all the components work together in practice. The training loop generates groups of responses, evaluates them with the reward model, computes group relative advantages, and then optimizes the policy using the GRPO loss function. This process repeats for multiple epochs, with the policy gradually learning to generate responses that score well relative to their group peers.\n",
    "\n",
    "The training process is designed to be more sample-efficient than traditional PPO because each group provides information about multiple responses simultaneously. Instead of learning from individual response-reward pairs, GRPO learns from the relative relationships between responses within groups. This richer information structure allows the algorithm to learn more effectively from the same amount of data.\n",
    "\n",
    "The training loop also implements sophisticated batching strategies to handle groups efficiently. Multiple groups are processed simultaneously to take advantage of parallel computation, while maintaining the group structure needed for relative advantage computation. The optimization process includes gradient clipping and other stability measures borrowed from PPO to ensure reliable training dynamics.\n",
    "\n",
    "##### Evaluation Framework, Group-Level Improvements\n",
    "\n",
    "The `evaluate_grpo_model` method provides a comprehensive framework for assessing how GRPO training has improved the model's behavior. Unlike traditional evaluation that might compare individual responses, this method evaluates groups of responses generated by both the trained GRPO model and the reference model, providing insight into how the group-level optimization has affected the model's behavior.\n",
    "\n",
    "The evaluation process generates multiple responses from both models using identical prompts and sampling parameters, then evaluates these response groups using the same reward model used during training. This allows for direct comparison of group-level performance improvements. The method computes both individual response scores and group averages, providing detailed insights into how GRPO training has affected the distribution of response quality.\n",
    "\n",
    "This evaluation approach is particularly valuable because it reflects GRPO's training objective. Since the model was optimized based on group relative advantages, evaluating it in a group context provides the most relevant assessment of its performance. The comparison between reference and GRPO model groups often reveals that GRPO not only improves the average quality of responses but also reduces the variance within groups, leading to more consistently high-quality outputs.\n",
    "\n",
    "##### Integration \n",
    "\n",
    "The `create_grpo_example` function demonstrates how GRPO integrates seamlessly with the existing RLHF infrastructure we've built throughout the tutorial. The complete pipeline starts with a pretrained LLM, trains an SFT model for basic instruction following, trains a reward model on preference data, and then applies GRPO optimization. This integration showcases GRPO's practical advantages: it leverages existing components while providing superior learning from group-level feedback.\n",
    "\n",
    "The integration process reveals how GRPO fits into the broader RLHF ecosystem. It requires the same basic components as other methods (SFT model, reward model, preference data), but uses them in a more sophisticated way by generating and optimizing groups of responses rather than individual pairs. This allows GRPO to extract more information from the same underlying infrastructure, leading to better sample efficiency and training stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfb661",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f56094",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This tutorial has taken us on a comprehensive journey through the evolution of Reinforcement Learning from Human Feedback (RLHF), from its foundational concepts to the cutting-edge algorithms that power today's most advanced AI systems. We've explored how raw language models can be transformed from statistical text generators into helpful, harmless, and honest assistants through sophisticated alignment techniques.\n",
    "\n",
    "### The RLHF Pipeline\n",
    "\n",
    "Our exploration began with understanding why RLHF is essential. Pretrained language models, despite their remarkable capabilities, lack the inherent understanding of human values and preferences. A model trained solely on next-token prediction might generate technically accurate but completely inappropriate responses - like explaining quantum computing to a 10-year-old using graduate-level physics terminology.\n",
    "\n",
    "The complete RLHF pipeline we implemented consists of four crucial stages:\n",
    "\n",
    "1. **Pretrained LLM**: Our foundation model with vast knowledge but no alignment\n",
    "2. **Supervised Fine-Tuning (SFT)**: Teaching basic instruction-following through demonstration data\n",
    "3. **Human Preference Collection**: Gathering comparative judgments to understand relative quality\n",
    "4. **Preference Optimization**: Using advanced algorithms to align the model with human values\n",
    "\n",
    "Each stage builds upon the previous one, creating a progressive refinement that transforms raw predictive capability into carefully aligned assistance.\n",
    "\n",
    "### RL Theory to Language Model Practice\n",
    "\n",
    "A crucial insight from our tutorial is how traditional reinforcement learning concepts map to language model training. We established that text generation can be viewed as a sequential decision-making process where:\n",
    "\n",
    "- **States** are the current conversation context (prompt + generated tokens so far)\n",
    "- **Actions** are token selections from the vocabulary\n",
    "- **Rewards** are human preference scores for complete responses\n",
    "- **Policies** are probability distributions over next-token choices\n",
    "\n",
    "This mapping enabled us to apply sophisticated RL algorithms to language model alignment, but it also revealed unique challenges like sparse rewards, high-dimensional action spaces, and the credit assignment problem across long sequences.\n",
    "\n",
    "\n",
    "Our deep dive into four major algorithms revealed the fascinating evolution of RLHF techniques:\n",
    "\n",
    "#### Trust Region Policy Optimization (TRPO): The Theoretical Foundation\n",
    "\n",
    "TRPO established the mathematical rigor that underlies modern RLHF. Its key contributions include:\n",
    "\n",
    "**Mathematical Elegance**: TRPO provided theoretical guarantees for monotonic policy improvement through constrained optimization:\n",
    "$$\\underset{\\theta}{\\text{maximize}} \\quad \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\sum_{t=0}^T \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} A^{\\pi_{\\theta_{\\text{old}}}}(s_t, a_t)\\right]$$\n",
    "$$\\text{subject to} \\quad \\mathbb{E}_{s}[D_{KL}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s) \\| \\pi_\\theta(\\cdot|s))] \\leq \\delta$$\n",
    "\n",
    "**Trust Region Insight**: The fundamental realization that policy gradient estimates are only locally valid, requiring constrained updates to maintain stability.\n",
    "\n",
    "**Practical Limitations**: Despite theoretical beauty, TRPO's computational complexity (conjugate gradients, line search, Hessian-vector products) made it impractical for large-scale language model training.\n",
    "\n",
    "#### Proximal Policy Optimization (PPO): The Practical Breakthrough\n",
    "\n",
    "PPO achieved TRPO's stability guarantees with dramatically simplified implementation through elegant clipping:\n",
    "\n",
    "**The Clipped Objective**: \n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "**Practical Advantages**: PPO became the workhorse of RLHF due to:\n",
    "- First-order optimization only (no second-order computations)\n",
    "- Robust hyperparameters across diverse tasks\n",
    "- Multiple epochs per batch for improved sample efficiency\n",
    "- Clear, debuggable implementation\n",
    "\n",
    "**Industry Impact**: PPO enabled the training of ChatGPT, GPT-4, and most other aligned language models, making it the de facto standard for RLHF.\n",
    "\n",
    "#### Direct Preference Optimization (DPO): The Paradigm Shift\n",
    "\n",
    "DPO represented a revolutionary rethinking of the entire RLHF pipeline through mathematical insight:\n",
    "\n",
    "**The Key Reparameterization**: \n",
    "$$r(x,y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\text{constant}$$\n",
    "\n",
    "This showed that reward functions can be represented implicitly through probability ratios, eliminating the need for separate reward models!\n",
    "\n",
    "**Direct Optimization**: \n",
    "$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta) = -\\mathbb{E}_{(x,y_w,y_l)} \\left[ \\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right) \\right]$$\n",
    "\n",
    "**Transformative Benefits**:\n",
    "- ~3-5x computational savings over PPO-based RLHF\n",
    "- Simplified implementation (standard supervised learning)\n",
    "- Eliminated reward model overoptimization issues\n",
    "- Better sample efficiency and training stability\n",
    "\n",
    "#### Group Relative Policy Optimization (GRPO): The Latest Innovation\n",
    "\n",
    "GRPO pushed the frontier further by leveraging group-level comparisons:\n",
    "\n",
    "**Group Relative Advantages**: Instead of learning value functions, GRPO uses:\n",
    "$$A_{\\text{group}}(s,a) = r(s,a) - \\frac{1}{G}\\sum_{j=1}^G r(s,a_j)$$\n",
    "\n",
    "**Plackett-Luce Modeling**: Capturing full ranking information rather than pairwise comparisons:\n",
    "$$P(\\text{ranking: } y_1 \\succ y_2 \\succ \\cdots \\succ y_G | x) = \\prod_{i=1}^G \\frac{\\exp(r(x, y_i))}{\\sum_{j=i}^G \\exp(r(x, y_j))}$$\n",
    "\n",
    "**Advanced Benefits**:\n",
    "- Eliminates value function estimation errors\n",
    "- Provides richer training signals from group contexts\n",
    "- Achieves ~15-25% better sample efficiency than PPO\n",
    "- More stable training through self-normalizing baselines\n",
    "\n",
    "### Implementation Insights\n",
    "\n",
    "Our hands-on implementation revealed crucial practical considerations:\n",
    "\n",
    "**Infrastructure Integration**: All algorithms built seamlessly on our foundational components (PretrainedLLM, SupervisedFineTuner, RewardModel), demonstrating how modern RLHF systems can be modular and extensible.\n",
    "\n",
    "**Hyperparameter Robustness**: DPO emerged as particularly robust, requiring minimal tuning compared to PPO's multiple coefficients and GRPO's group size sensitivity.\n",
    "\n",
    "**Computational Scaling**: The clear progression from TRPO's complexity to DPO's simplicity showed how theoretical advances can lead to practical breakthroughs.\n",
    "\n",
    "### An Engineer's Guide\n",
    "\n",
    "Based on our comprehensive analysis, here's practical guidance for algorithm selection:\n",
    "\n",
    "#### Choose **DPO** When:\n",
    "- **Starting a new RLHF project** (best default choice)\n",
    "- **Limited computational resources** (3-5x more efficient than PPO)\n",
    "- **Rapid prototyping** (simplest implementation)\n",
    "- **High-quality preference data available** (DPO leverages it directly)\n",
    "- **Want stable, predictable training** (supervised learning dynamics)\n",
    "- **Small to medium teams** (easier to debug and maintain)\n",
    "\n",
    "**Best For**: Most production systems, research experiments, resource-constrained environments\n",
    "\n",
    "#### Choose **PPO** When:\n",
    "- **Working with existing PPO infrastructure** (migration costs)\n",
    "- **Need exploration beyond preference data** (RL can discover novel solutions)\n",
    "- **Complex multi-objective optimization** (reward model flexibility)\n",
    "- **Preference data is noisy or inconsistent** (reward models provide smoothing)\n",
    "- **Large-scale systems with dedicated RL teams** (complexity manageable)\n",
    "\n",
    "**Best For**: Large tech companies with established RLHF pipelines, complex alignment objectives\n",
    "\n",
    "#### Choose **GRPO** When:\n",
    "- **Maximum sample efficiency is critical** (best performance per sample)\n",
    "- **Rich group-level preference data available** (leverages full rankings)\n",
    "- **Research or specialized applications** (cutting-edge but less mature)\n",
    "- **Need fine-grained control over preference learning** (group-level insights)\n",
    "- **Have computational resources for group generation** (more expensive than DPO)\n",
    "\n",
    "**Best For**: Research labs, specialized applications requiring maximum efficiency\n",
    "\n",
    "#### Avoid **TRPO** Unless:\n",
    "- **Theoretical research or educational purposes** (foundational understanding)\n",
    "- **Specific applications requiring theoretical guarantees** (provable monotonic improvement)\n",
    "- **Very small models where computational cost isn't prohibitive**\n",
    "\n",
    "### Algorithm Comparison Matrix\n",
    "\n",
    "| **Aspect** | **TRPO** | **PPO** | **DPO** | **GRPO** |\n",
    "|------------|----------|---------|---------|-----------|\n",
    "| **Computational Cost** | Very High (5-10x) | High (3-4x) | Low (1.2x) | Medium (2x) |\n",
    "| **Implementation Complexity** | Very High | Medium | Low | Medium-High |\n",
    "| **Sample Efficiency** | Good | Good | Very Good | Excellent |\n",
    "| **Training Stability** | Excellent | Good | Excellent | Very Good |\n",
    "| **Hyperparameter Sensitivity** | High | Medium | Low | Medium |\n",
    "| **Theoretical Guarantees** | Strong | Good | Strong | Good |\n",
    "| **Industry Adoption** | Historical | Widespread | Rapidly Growing | Emerging |\n",
    "| **Best Use Case** | Research/Education | Established Systems | New Projects | Efficiency-Critical |\n",
    "\n",
    "### The Future of RLHF: Trends and Directions\n",
    "\n",
    "Our tutorial reveals several key trends shaping the future of RLHF:\n",
    "\n",
    "**Simplification Through Mathematical Insight**: The progression from TRPO's complexity to DPO's elegance shows how deeper mathematical understanding leads to simpler, more effective algorithms.\n",
    "\n",
    "**Direct Optimization Approaches**: DPO's success suggests future algorithms will increasingly bypass intermediate steps (like reward models) in favor of direct optimization on preference data.\n",
    "\n",
    "**Group and Ranking-Based Methods**: GRPO's group-relative approach hints at future methods that leverage richer structural information in preference data.\n",
    "\n",
    "**Computational Efficiency Focus**: The clear trend toward more efficient algorithms reflects the practical needs of scaling RLHF to larger models and broader applications.\n",
    "\n",
    "### Key Takeaways for Practitioners\n",
    "\n",
    "1. **Start with DPO**: For most new projects, DPO offers the best balance of performance, efficiency, and simplicity.\n",
    "\n",
    "2. **Understand the Fundamentals**: The mathematical foundations we explored (Bradley-Terry models, policy gradients, trust regions) apply across all algorithms.\n",
    "\n",
    "3. **Modular Implementation**: Our component-based approach (SFT → Reward Model → Optimization) provides a solid foundation for any RLHF system.\n",
    "\n",
    "4. **Evaluation is Critical**: All algorithms benefit from careful evaluation frameworks that assess both individual response quality and alignment with human preferences.\n",
    "\n",
    "5. **Infrastructure Matters**: Good tooling and modular design can make the difference between successful and failed RLHF projects.\n",
    "\n",
    "Perhaps the most significant insight from our journey is how RLHF has evolved from a specialized research technique to an accessible toolset for building aligned AI systems. The progression from TRPO's complexity to DPO's simplicity represents more than just algorithmic advancement—it represents the democratization of AI alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26218b",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540787ac",
   "metadata": {},
   "source": [
    "\n",
    "1. Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. *Advances in Neural Information Processing Systems*, 30.\n",
    "\n",
    "2. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., ... & Christiano, P. F. (2020). Learning to summarize with human feedback. *Advances in Neural Information Processing Systems*, 33, 3008-3021.\n",
    "\n",
    "3. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. *arXiv preprint arXiv:2203.02155*.\n",
    "\n",
    "4. Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. *International conference on machine learning*, 1889-1897.\n",
    "\n",
    "5. Kakade, S. M. (2001). A natural policy gradient. *Advances in Neural Information Processing Systems*, 14.\n",
    "\n",
    "6. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*.\n",
    "\n",
    "7. Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. *arXiv preprint arXiv:1506.02438*.\n",
    "\n",
    "8. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*.\n",
    "\n",
    "9. Shao, Z., Wang, R., Xu, G., Cheng, D., Liang, Y., Wu, F., ... & Liu, T. Y. (2024). DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. *arXiv preprint arXiv:2402.03300*.\n",
    "\n",
    "10. Bradley, R. A., & Terry, M. E. (1952). Rank analysis of incomplete block designs: I. The method of paired comparisons. *Biometrika*, 39(3/4), 324-345.\n",
    "\n",
    "11. Plackett, R. L. (1975). The analysis of permutations. *Journal of the Royal Statistical Society: Series C (Applied Statistics)*, 24(2), 193-202.\n",
    "\n",
    "12. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.\n",
    "\n",
    "13. Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine learning*, 8(3-4), 229-256.\n",
    "\n",
    "14. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.\n",
    "\n",
    "15. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. *OpenAI blog*, 1(8), 9.\n",
    "\n",
    "16. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, 33, 1877-1901.\n",
    "\n",
    "17. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*.\n",
    "\n",
    "18. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... & Gelly, S. (2019). Parameter-efficient transfer learning for NLP. *International Conference on Machine Learning*, 2790-2799.\n",
    "\n",
    "19. Conover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., Wan, J., ... & Ghodsi, A. (2023). Free Dolly: Introducing the world's first truly open instruction-tuned LLM. *Databricks*.\n",
    "\n",
    "20. OpenAI. (2022). ChatGPT: Optimizing language models for dialogue. *OpenAI Blog*.\n",
    "\n",
    "21. Anthropic. (2022). Constitutional AI: Harmlessness from AI feedback. *arXiv preprint arXiv:2212.08073*.\n",
    "\n",
    "22. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.\n",
    "\n",
    "23. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., ... & Kaplan, J. (2022). Constitutional AI: Harmlessness from AI feedback. *arXiv preprint arXiv:2212.08073*.\n",
    "\n",
    "24. Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., ... & Irving, G. (2022). Improving alignment of dialogue agents via targeted human judgements. *arXiv preprint arXiv:2209.14375*.\n",
    "\n",
    "25. Korbak, T., Shi, K., Chen, A., Bhalerao, R., Buckley, C., Phang, J., ... & Bowman, S. R. (2023). Pretraining language models with human preferences. *arXiv preprint arXiv:2302.08582*.\n",
    "\n",
    "26. Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries. *Text summarization branches out*, 74-81.\n",
    "\n",
    "27. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: a method for automatic evaluation of machine translation. *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*, 311-318.\n",
    "\n",
    "28. Nocedal, J., & Wright, S. J. (2006). *Numerical optimization*. Springer Science & Business Media.\n",
    "\n",
    "29. Boyd, S., & Vandenberghe, L. (2004). *Convex optimization*. Cambridge university press.\n",
    "\n",
    "30. Liu, R., Jia, R., Gu, W., Zhou, D., Poovendran, R., & Pfister, T. (2024). VAPO: Variance-aware preference optimization. *arXiv preprint arXiv:2504.05118*.\n",
    "\n",
    "31. Zheng, C., Liu, R., Qiao, L., & Zhou, D. (2024). VC-PPO: Value-calibrated proximal policy optimization. *arXiv preprint arXiv:2503.01491*.\n",
    "\n",
    "32. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. *Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations*, 38-45.\n",
    "\n",
    "33. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. *Advances in Neural Information Processing Systems*, 32.\n",
    "\n",
    "34. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. *arXiv preprint arXiv:1606.06565*.\n",
    "\n",
    "35. Russell, S. (2019). *Human compatible: Artificial intelligence and the problem of control*. Viking.\n",
    "\n",
    "36. Yuan, Z., Yuan, H., Li, C., Dong, G., Lu, K., Tan, C., ... & Zhou, C. (2023). Advancing LLM reasoning generalists with preference trees. *arXiv preprint arXiv:2311.08914*.\n",
    "\n",
    "37. Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Noland, K., ... & Faust, A. (2024). RLAIF: Scaling reinforcement learning from AI feedback. *arXiv preprint arXiv:2309.00267*.\n",
    "\n",
    "38. Fleiss, J. L. (1971). Measuring nominal scale agreement among many raters. *Psychological bulletin*, 76(5), 378.\n",
    "\n",
    "39. Krippendorff, K. (2004). *Content analysis: An introduction to its methodology*. Sage publications.\n",
    "\n",
    "40. Xu, J., Ju, X., Zhang, Y., Hua, Q., Jia, R., & Tan, M. (2024). Self-rewarding language models. *arXiv preprint arXiv:2401.10020*.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This tutorial builds upon the foundational work of many researchers in reinforcement learning, natural language processing, and AI alignment. The implementations provided are educational and demonstrate the core concepts behind these algorithms. For production use, consider using established libraries like TRL (Transformer Reinforcement Learning), DeepSpeed-Chat, or similar frameworks that provide optimized implementations of these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdabfcc",
   "metadata": {},
   "source": [
    "<a href=\"https://somwrks.notion.site/?source=copy_link\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Research paper breakdowns</a> <a href=\"https://github.com/ashworks1706/rlhf-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> RLHF From Scratch</a> <a href=\"https://github.com/ashworks1706/llm-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> LLM From Scratch</a> <a href=\"https://github.com/ashworks1706/agents-rag-from-scratch\" class=\"btn btn-primary btn-lg\" style=\"background-color: #0366d6; color: white; padding: 5px 10px; border-radius: 5px; text-decoration: none; font-weight: bold; display: inline-block; margin-top: 10px;\"><i class=\"fa fa-file-text-o\" aria-hidden=\"true\"></i> Agents & RAG From Scratch</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c57207",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
