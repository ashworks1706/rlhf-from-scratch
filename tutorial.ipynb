{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffd4d83",
   "metadata": {},
   "source": [
    "# RLHF From Scratch on LLMs\n",
    "\n",
    "In this notebook, I will start with history of RLHF, the importance of RLHF in LLMs, then go into the architectures TRPO, PPO, GRPO and DPO. Each of the technique's explanation will have the math, code and explanations on how it's done, finally in the end we'll experiment these techniques on one of prebuilt LLMs (the llms are not built from scratch, since i've already done that in my [llm-from-scratch repository](https://github.com/ashworks1706/llm-from-scratch)). If you're new to RL, check out [dqn-from-scratch](https://github.com/ashworks1706/dqn-from-scratch) where i've explained RL indepth from the core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a4fea",
   "metadata": {},
   "source": [
    "## Brief History of RLHF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d9008",
   "metadata": {},
   "source": [
    "\n",
    "RLHF emerged around 2017-2018 when researchers at OpenAI developed techniques to incorporate human preferences into reinforcement learning systems. The seminal paper \"Deep Reinforcement Learning from Human Preferences\" by Christiano et al. (2017) introduced the core concept of using human comparisons between pairs of outputs to train a reward model that could guide RL agents toward preferred behaviors. While initially applied to simpler tasks and robotics, the technique remained relatively specialized until recent years. The technique gained mainstream attention in 2022 when OpenAI used it to create ChatGPT from GPT-3.5, dramatically improving output quality by aligning the model with human preferences. This breakthrough demonstrated RLHF's potential to transform raw language model capabilities into systems that better align with human intent and values. Since then, RLHF has become a standard component in developing advanced language models like GPT-4, Claude, and Llama 2, with each iteration refining the techniques to achieve better alignment.\n",
    "\n",
    "#### Why RLHF Matters for LLMs\n",
    "\n",
    "<img src=\"assets/rlhf-vs-finetune.png\" width=300>\n",
    "\n",
    "Large Language Models trained solely on next-token prediction are just models with knowledge, they don't know how to answer properly. You have model trained on shakespear work, great! but how do you make it to answer questions in the way we want (in the way humans talk)? These models optimize for predicting the next token based on training data distribution, which doesn't necessarily correlate with producing helpful, harmless, or honest responses. Traditional LLMs may generate toxic, harmful, or misleading content because they're simply trying to produce statistically likely continuations without understanding human values or preferences. They lack an inherent mechanism to distinguish between content that is statistically probable and content that is actually desirable according to human standards. RLHF addresses these issues by creating a feedback loop where human preferences explicitly guide the model's learning process, steering it toward outputs that humans find more helpful, honest, and aligned with their intent. This alignment process transforms a powerful but directionless prediction engine into a system that can better understand and respect nuanced human values and follow complex instructions in ways that maximize utility for users.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5616c",
   "metadata": {},
   "source": [
    "## The Birds Eye View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4df866",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow.png\" >\n",
    "\n",
    "\n",
    "Before delving into the complexities of RLHF, it's essential to understand the overal workflow of what actually happens in a typical RLHF based projects. When a large language model is initially trained on vast internet text corpora, it develops remarkable capabilities to predict text and acquire factual knowledge, but this training alone doesn't prepare it to be helpful in specific contexts or respond appropriately to human instructions. Consider a language model trained on extensive educational materials, including university canvas modules, academic papers, and textbooks. This model would possess substantial knowledge about various academic subjects, pedagogical approaches, and educational concepts. However, if asked to \"Explain the concept of photosynthesis to a 10-year-old,\" it might produce a technically accurate but overly complex explanation filled with academic jargon that would confuse rather than enlighten a young student. The model hasn't been optimized to serve as an effective tutor - it simply predicts what text might follow in educational materials. \n",
    "\n",
    "The Supervised Fine-Tuning stage addresses this gap by training the model on demonstrations of desired behavior. For our hypothetical educational assistant, SFT would involve collecting thousands of examples showing how skilled human tutors respond to student questions: simplifying complex concepts, using age-appropriate language, providing relevant examples, checking for understanding, and offering encouragement. These demonstrations are formatted as input-output pairs (prompt and ideal response), and the model is fine-tuned to minimize the difference between its outputs and these human-generated \"gold standard\" responses. Through this process, the model learns the patterns that characterize helpful tutoring: breaking down complex concepts into simpler components, using analogies relevant to younger audiences, avoiding unnecessary technical terms, and adopting a supportive tone. After SFT, when asked to explain photosynthesis to a 10-year-old, the model is much more likely to respond with an explanation involving plants \"eating sunlight\" and \"breathing in carbon dioxide to make food,\" rather than discussing electron transport chains and ATP synthesis. The model hasn't gained new knowledge, but it has learned a new way to present its existing knowledge that better aligns with the specific goal of being an effective tutor for younger students. However, SFT alone has significant limitations. First, it can only learn from the specific examples it's shown, leaving gaps in how to handle the infinite variety of possible user requests. Second, the demonstrations might not cover the full range of desirable behaviors or edge cases where special handling is needed. Third, the quality of the SFT model depends entirely on the quality and consistency of the demonstration data. Finally, there's no mechanism for the model to understand why certain responses are better than others - it simply learns to mimic patterns without a deeper understanding of the preferences that make one response superior. These limitations are precisely what RLHF is designed to address in the subsequent stages of the alignment process.\n",
    "\n",
    "Following Supervised Fine-Tuning, the RLHF workflow progresses to Human Preference Collection - a crucial stage that fundamentally changes how model improvement occurs. In this phase, rather than providing gold-standard demonstrations, human evaluators compare and rank different model responses to the same prompt. For our educational assistant, this might involve presenting evaluators with pairs of explanations for the same scientific concept and asking them which better achieves the goal of teaching a young student. One explanation might be more engaging and use more appropriate analogies, while another might be technically accurate but still too complex. By explicitly choosing the better response, humans provide preference signals that capture nuanced quality distinctions beyond what demonstration data alone can convey. These comparisons generate valuable datasets where each entry contains a prompt and two responses, with a label indicating which response humans preferred. The collection process typically gathers thousands or even millions of such comparative judgments, creating a rich dataset that embodies human preferences about what constitutes a high-quality response across diverse scenarios.\n",
    "\n",
    "The third stage, Reward Model Training, transforms these human preferences into a quantifiable reward function that can guide further optimization. This reward model takes a prompt and response as input and outputs a scalar score representing how well the response aligns with human preferences. Technically, it's trained to predict which of two responses humans would prefer by maximizing the likelihood of the observed preference data. For our educational tutor, the reward model learns to assign higher scores to explanations that successfully simplify complex concepts without sacrificing accuracy, use age-appropriate analogies, maintain an encouraging tone, and check for understanding. This model becomes a computational proxy for human judgment, capable of evaluating millions of potential responses far beyond what human evaluators could manually assess. The quality of this reward model is critical, as it effectively defines what \"good\" means for all subsequent optimization.\n",
    "\n",
    "With a trained reward model in place, the final stage applies Reinforcement Learning techniques to optimize the language model toward maximizing the predicted reward. The most common approach is Proximal Policy Optimization (PPO), which iteratively improves the model by adjusting its parameters to generate responses that receive higher reward scores. However, simply maximizing reward can lead to degenerate outputs that exploit loopholes in the reward model or diverge too far from natural language patterns. To prevent this, the optimization includes a \"KL divergence\" penalty that constrains how much the optimized model can deviate from the SFT model, preserving fluency and knowledge while improving alignment. For our educational tutor, this process might result in a model that maintains scientific accuracy while consistently finding creative, age-appropriate analogies and explanations across a much broader range of topics than were covered in the original demonstration data. The entire RLHF pipeline is often iterative, with new preference data collected from the improved model, leading to refined reward models and further optimization cycles. This continuous feedback loop progressively aligns the language model with human values and preferences, addressing the fundamental limitations of training on prediction alone or even on demonstration data without comparative preference signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df33e7",
   "metadata": {},
   "source": [
    "## 1. Getting a pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776fee7",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55f9c3",
   "metadata": {},
   "source": [
    "Now the first step is to have a fresh pretrained LLM right off the top. We'll be using huggingface library transformers library for our transformer components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers peft datasets tqdm wandb rouge-score\n",
    "# PEFT is a technique to fine tune LLMs without modifying all of their parameters. it's efficient for our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from library and setup the model class \n",
    " \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "class PretrainedLLM:\n",
    "    def __init__(self, model_name=\"facebook/opt-350m\", device=None):\n",
    "        \"\"\"\n",
    "        Initializing a (No SFT RLHF) pretrained language model for RLHF experiment\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier (default: OPT-350M, a relatively small but capable model)\n",
    "            device: Computing device (will auto-detect if None)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # this code is just for detecting if you have Nvidia CUDA driver or not\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Loading {model_name} on {self.device}...\")\n",
    "        \n",
    "        # Load model and tokenizer (for full guide on implementing llm from scratch check out https://github.com/ashworks1706/llm-from-scratch\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # distributed training for better GPU utilization\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded successfully with {sum(p.numel() for p in self.model.parameters())/1e6:.1f}M parameters\")\n",
    "        \n",
    "    def generate(self, prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate text from the model given a prompt (no RLHF)\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input text to generate from\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature (lower = more deterministic)\n",
    "            top_p: Nucleus sampling parameter (lower = more focused)\n",
    "            \n",
    "        Returns:\n",
    "            Generated text as string\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate with sampling, no fancy tuning required\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode and remove the prompt from the generated text\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_text = full_text[len(self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):]\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def save_checkpoint(self, path):\n",
    "        \"\"\"Save model checkpoint to the specified path\"\"\"\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "        \n",
    "    def load_adapter(self, adapter_path):\n",
    "        \"\"\"Load a PEFT adapter for efficient fine-tuning\"\"\"\n",
    "        self.model = PeftModel.from_pretrained(\n",
    "            self.model,\n",
    "            adapter_path,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        print(f\"Loaded adapter from {adapter_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with a small model for experimentation\n",
    "    llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = \"Explain quantum computing to a 10-year-old:\"\n",
    "    print(f\"(No SFT RLHF) Prompt: {prompt}\")\n",
    "    print(f\"(NO SFT RLHF) Response: {llm.generate(prompt, max_new_tokens=150)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e849",
   "metadata": {},
   "source": [
    "---\n",
    "(No SFT RLHF) *Prompt*: Explain quantum computing to a 10-year-old:\n",
    "\n",
    "(No SFT RLHF) *Response*: Quantum computing is a type of computation that harnesses the collective properties of quantum states, such as superposition, interference, and entanglement, to perform calculations. The basic unit of quantum computation is the quantum bit or qubit. Unlike classical bits, which can only be in states of 0 or 1, qubits can exist in a superposition of states, meaning they can represent both 0 and 1 simultaneously. This property allows quantum computers to process a vast number of possibilities simultaneously.\n",
    "\n",
    "The field emerged from quantum mechanics, which is a fundamental theory in physics that provides a description of the physical properties of nature at the atomic and subatomic scales. Quantum computing leverages quantum mechanical phenomena to perform operations on data. This can potentially solve certain problems much faster than classical computers.\n",
    "\n",
    "For instance, Shor's algorithm demonstrates that a quantum computer can factor large integers exponentially faster than the best known algorithm running on a classical computer. This has significant implications for cryptography systems that rely on the difficulty of factoring large numbers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650836aa",
   "metadata": {},
   "source": [
    "In this example, the user anticipates the model to give explanation releavant to a 10 year old. Instead, the model produces a very technical response with an academic tone without any examples. A better response could have been (with SFT):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948df58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "(No RLHF) *Prompt*: Explain quantum computing to a 10-year-old:\n",
    "\n",
    "(No RLHF) *Response*: Imagine your regular computer is like playing a board game where you can only be on one space at a time. You have to move step by step to get to the end.\n",
    "\n",
    "Quantum computers are like magical game pieces that can be in lots of different spaces at the same time! They use special bits called \"qubits\" instead of regular computer bits.\n",
    "\n",
    "Regular computer bits are like light switches - they can only be ON or OFF. But qubits are special because they can be ON and OFF at the same time, kind of like how a spinning coin is neither heads nor tails until it stops.\n",
    "\n",
    "This special power lets quantum computers solve some really tricky puzzles much faster than regular computers. Scientists are excited about them because they might help us discover new medicines, understand weather better, and solve problems that would take regular computers thousands of years!\n",
    "\n",
    "The quantum computers we have today are still small and make mistakes, but engineers are working hard to make them bigger and better.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36869ed0",
   "metadata": {},
   "source": [
    "Here as you can see that the SFT model has learned to use simple language and analogies, breakdown complex concepts, avoid technical jargon like superposition and slightly adatpive to the age level of the audience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241dde7a",
   "metadata": {},
   "source": [
    "This is our goal response with SFT and RLHF :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a685a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "(SFT RLHF) *Prompt*: Explain quantum computing to a 10-year-old:\n",
    "\n",
    "(SFT RLHF) *Response*: Hey there! Imagine you have a super cool magic coin. When you flip a normal coin, it lands on either heads or tails, right? But this magic coin can spin and be a little bit heads and a little bit tails at the same time while it's spinning!\n",
    "\n",
    "Quantum computers use special things called \"qubits\" that work like these magic coins. While normal computers use regular bits that can only be 0 OR 1 (like a coin that's only heads or tails), quantum computers use qubits that can be 0 AND 1 at the same time (like our spinning magic coin).\n",
    "\n",
    "Why is this so awesome? Well, imagine you lost your favorite toy somewhere in your house. A normal computer would have to look in each room one by one. But a quantum computer is like having a bunch of friends who can check all the rooms at the exact same time! That makes solving certain problems MUCH faster.\n",
    "\n",
    "Scientists are still learning how to build better quantum computers because they're tricky to make. The qubits are super sensitive - kind of like trying to keep a hundred coins spinning without letting any of them fall over!\n",
    "\n",
    "Does that make sense? What do you think would be the coolest thing to use a quantum computer for?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db3321",
   "metadata": {},
   "source": [
    "in this example, we can clearly see the difference, it's more conversational, user friendly and uses analogies to make explanation easier like a human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0a8fa",
   "metadata": {},
   "source": [
    "## 2. Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ba3d2",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow2.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69635b",
   "metadata": {},
   "source": [
    "### Understanding the difference between SFT and RLHF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126eac5b",
   "metadata": {},
   "source": [
    "But what if I just keep model with SFT and not RLHF? Or what if I just skip to RLHF instead of SFT?\n",
    "\n",
    "These are excellent questions that get to the heart of why the complete RLHF pipeline exists. Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) serve different but complementary roles in aligning language models with human expectations and preferences.\n",
    "\n",
    "If you only implement SFT without RLHF, you'll have a model that can follow basic patterns demonstrated in your training examples, but it will struggle to generalize beyond them. As we saw in our quantum computing example, SFT can teach a model to use simpler language and appropriate analogies, but it's limited by the specific demonstrations provided. The model learns to mimic patterns without developing a deeper understanding of why certain responses are better than others. When faced with novel queries or edge cases not covered in the training data, an SFT-only model often fails to maintain the same quality of responses. Additionally, SFT can only optimize for whatever patterns exist in your demonstration data - if that data contains subtle biases or inconsistencies, those will be faithfully reproduced by the model.\n",
    "\n",
    "Conversely, if you attempt to skip SFT and go directly to RLHF, you're likely to encounter significant challenges. RLHF works by refining an already somewhat aligned model through preference optimization. Starting with a raw pretrained model would make this process extremely inefficient and potentially unstable. The preference learning and reinforcement stages need a reasonable starting point where the model can already produce somewhat appropriate responses that humans can meaningfully compare and rank. Without SFT, the initial responses might be so far from helpful that the preference signals become too noisy or the optimization process becomes prohibitively difficult. It would be like trying to teach advanced painting techniques to someone who hasn't yet learned to hold a brush - the feedback would be overwhelming and difficult to incorporate.\n",
    "\n",
    "The full RLHF pipeline with SFT followed by preference learning and reinforcement creates a progressively refined alignment. SFT provides the foundation by teaching the model basic response patterns and formats through demonstration. RLHF then builds on this foundation by teaching the model to distinguish between good and better responses through comparative feedback, allowing it to generalize beyond specific examples to broader human preferences. As we observed in our examples, the SFT model improved basic comprehensibility and appropriateness, while the RLHF model further enhanced engagement, conversational tone, and subtle aspects of helpfulness that are difficult to capture through demonstrations alone. This complementary relationship explains why major AI systems like ChatGPT and Claude use both techniques in sequence rather than choosing one over the other. The complete alignment process transforms raw predictive power into carefully balanced helpful assistance that respects complex human values and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa793cf",
   "metadata": {},
   "source": [
    "### Components of SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b7d30",
   "metadata": {},
   "source": [
    "To perform Supervised Fine-Tuning (SFT) on our pretrained LLM, we need high-quality demonstration data consisting of prompt-response pairs showing the desired behavior, typically thousands of examples created by experts. We also need a data preprocessing pipeline to format this data consistently, including tokenization and special tokens to distinguish between prompts and responses. SFT requires careful configuration of hyperparameters like learning rate, batch size, and optimization methods, with techniques such as warmup and decay schedules for training stability. Rather than fine-tuning all parameters, we'll use PEFT methods like LoRA that add small trainable modules while keeping most of the model frozen, making training more efficient. We'll implement a training loop for forward passes, loss calculation, backpropagation, and parameter updates, along with evaluation metrics such as perplexity and ROUGE scores to assess performance. Finally, our existing PretrainedLLM class already supports checkpointing and adapter saving, which we'll use to periodically save the model state during training. This SFT process will transform our raw model into one that can follow instructions and communicate appropriately, serving as the foundation for subsequent RLHF stages.\n",
    "\n",
    "High-quality demonstration data: Thousands of prompt-response pairs created by experts\n",
    "\n",
    "Data preprocessing pipeline: Consistent formatting, tokenization, and special tokens\n",
    "\n",
    "Fine-tuning configuration: Learning rate, batch size, warmup/decay schedules\n",
    "\n",
    "PEFT implementation: Using LoRA to add trainable modules while freezing most parameters\n",
    "\n",
    "Training loop: Forward passes, loss calculation, backpropagation, parameter updates\n",
    "\n",
    "Evaluation metrics: Perplexity, ROUGE scores to assess performance\n",
    "\n",
    "Checkpointing: Saving model state during training using our existing functionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283179e",
   "metadata": {},
   "source": [
    "For dataset, we're using the Databricks Dolly-15k dataset, which is a high-quality instruction-following dataset specifically designed for fine-tuning language models. This dataset contains 15,000 human-generated prompt/response pairs across various instruction categories including creative writing, classification, information extraction, open QA, brainstorming, and summarization. \n",
    "\n",
    "The Dolly dataset was created by Databricks employees who manually wrote both the prompts and high-quality responses, making it particularly valuable for instruction-tuning. Unlike some other datasets which may be generated or filtered from existing sources, Dolly's samples are purpose-built for teaching models to follow instructions in a helpful manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50206b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SupervisedFineTuner:\n",
    "    def __init__(self, base_model, dataset_name=\"databricks/dolly-15k\", max_seq_length=512):\n",
    "        \"\"\"\n",
    "        Initializing SFT framework\n",
    "        \n",
    "        Args:\n",
    "            base_model: The PretrainedLLM instance to fine-tune\n",
    "            dataset_name: HuggingFace dataset identifier containing instruction/response pairs\n",
    "            max_seq_length: Maximum sequence length for inputs\n",
    "        \"\"\"\n",
    "        self.llm = base_model\n",
    "        self.tokenizer = base_model.tokenizer\n",
    "        self.model = base_model.model\n",
    "        self.device = base_model.device\n",
    "        self.max_seq_length = max_seq_length # max length of sequences that model will process\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        # If tokenizer doesn't have padding token, set it to eos token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "        print(f\"Loading dataset {dataset_name}...\")\n",
    "        self.raw_dataset = load_dataset(dataset_name)\n",
    "        print(f\"Dataset loaded with {len(self.raw_dataset['train'])} training examples\")\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Process the dataset into the format needed for instruction fine-tuning\"\"\"\n",
    "        \n",
    "        def format_instruction(example):\n",
    "            \"\"\"Format an example into a prompt-response pair with special tokens\"\"\"\n",
    "            # Different datasets have different column names, they might call different call different labels\n",
    "            if 'instruction' in example and 'response' in example:\n",
    "                prompt = example['instruction']\n",
    "                response = example['response']\n",
    "            elif 'prompt' in example and 'completion' in example:\n",
    "                prompt = example['prompt']\n",
    "                response = example['completion']\n",
    "            else:\n",
    "                # Fallback for other dataset formats\n",
    "                prompt = str(example['input']) if 'input' in example else \"\"\n",
    "                response = str(example['output']) if 'output' in example else \"\"\n",
    "            \n",
    "            # Format with special tokens\n",
    "            formatted_text = f\"User: {prompt.strip()}\\n\\nAssistant: {response.strip()}\"\n",
    "            return {\"formatted_text\": formatted_text}\n",
    "        \n",
    "        print(\"Formatting dataset...\")\n",
    "        self.processed_dataset = self.raw_dataset.map(format_instruction)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"Tokenize the examples and prepare for training\"\"\"\n",
    "            texts = examples[\"formatted_text\"]\n",
    "            \n",
    "            # Tokenize with padding and truncation\n",
    "            tokenized = self.tokenizer(\n",
    "                texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Create labels (for causal LM, labels are the same as input_ids)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "            \n",
    "            # Mask padding tokens in the labels to -100 so they're not included in loss\n",
    "            tokenized[\"labels\"][tokenized[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "            \n",
    "            return tokenized\n",
    "        \n",
    "        print(\"Tokenizing dataset...\")\n",
    "        self.tokenized_dataset = self.processed_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.processed_dataset[\"train\"].column_names\n",
    "        )\n",
    "        \n",
    "        return self.tokenized_dataset\n",
    "    \n",
    "    def setup_peft(self, r=16, lora_alpha=32, lora_dropout=0.05):\n",
    "        \"\"\"Set up Parameter-Efficient Fine-Tuning using LoRA\"\"\"\n",
    "        \n",
    "        print(\"Setting up LoRA for efficient fine-tuning...\")\n",
    "        # Configure LoRA\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=r,  # Rank of the update matrices\n",
    "            lora_alpha=lora_alpha,  # Scaling factor\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],  # Which modules to apply LoRA to\n",
    "            bias=\"none\",\n",
    "            inference_mode=False\n",
    "        )\n",
    "        \n",
    "        # Prepare model for training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        # Apply LoRA\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        \n",
    "        # Display trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self, output_dir=\"sft_model\", num_epochs=3, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train the model using the prepared dataset\"\"\"\n",
    "        \n",
    "        print(\"Setting up training arguments...\")\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,                           # Directory to save model checkpoints\n",
    "            num_train_epochs=num_epochs,                     # Number of times to iterate through the dataset\n",
    "            per_device_train_batch_size=batch_size,          # Batch size per GPU/CPU for training\n",
    "            gradient_accumulation_steps=4,                   # Number of updates steps to accumulate gradients for\n",
    "            warmup_ratio=0.1,                               # Percentage of steps for learning rate warmup\n",
    "            weight_decay=0.01,                              # L2 regularization weight\n",
    "            learning_rate=learning_rate,                     # Initial learning rate\n",
    "            logging_steps=10,                               # How often to log training metrics\n",
    "            save_steps=200,                                 # How often to save model checkpoints\n",
    "            save_total_limit=3,                             # Maximum number of checkpoints to keep\n",
    "            fp16=True if self.device == \"cuda\" else False,   # Whether to use 16-bit floating point precision\n",
    "            report_to=\"none\"                                # Disable external reporting services\n",
    "        )\n",
    "        \n",
    "        # Create data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        print(\"Creating trainer...\")\n",
    "        # Initialize the trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.tokenized_dataset[\"train\"],\n",
    "            eval_dataset=self.tokenized_dataset[\"test\"] if \"test\" in self.tokenized_dataset else None,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        # Finally Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the adapter\n",
    "        adapter_path = os.path.join(output_dir, \"adapter\")\n",
    "        self.model.save_pretrained(adapter_path) # save the model to the path\n",
    "        self.tokenizer.save_pretrained(adapter_path)\n",
    "        print(f\"Saved LoRA adapter to {adapter_path}\")\n",
    "        \n",
    "        return adapter_path\n",
    "    \n",
    "    def evaluate(self, evaluation_prompts):\n",
    "        \"\"\"Evaluate the model on a list of prompts\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        for prompt in evaluation_prompts:\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            \n",
    "            # Generate with the original model\n",
    "            base_response = self.llm.generate(prompt, max_new_tokens=200)\n",
    "            print(f\"Base model response: {base_response}\\n\")\n",
    "            \n",
    "            # Format prompt for the fine-tuned model\n",
    "            formatted_prompt = f\"User: {prompt}\\n\\nAssistant: \"\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Generate with the fine-tuned model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=200,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "                \n",
    "            # Decode and display\n",
    "            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            sft_response = full_text[len(formatted_prompt):]\n",
    "            print(f\"SFT model response: {sft_response}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c49e95",
   "metadata": {},
   "source": [
    "As we can see, we have first a base model (our pretrained LLM), a dataset name, and a maximum sequence length parameter that controls how much text the model processes at once. It handles tokenizer configuration by ensuring the padding token is properly set, which is crucial for consistent batch processing during training. Next, the prepare_data method loads the Dolly-15k dataset and transforms each example by extracting the prompt-response pairs and formatting them with special tokens that help the model distinguish between user input and expected output. The formatting includes adding \"User:\" and \"Assistant:\" prefixes that teach the model the proper conversation structure. After formatting, it tokenizes all examples, handling padding and truncation to ensure consistent lengths, and sets up special label handling where padding tokens are masked from loss calculations.\n",
    "\n",
    "The setup_peft method is particularly innovative, implementing Parameter-Efficient Fine-Tuning using Low-Rank Adaptation (LoRA). Rather than updating all model weights—which would be computationally expensive—LoRA adds small trainable matrices to key attention components while keeping most parameters frozen. The method configures LoRA with appropriate rank and scaling parameters, then applies it to the query and value projection matrices of the transformer architecture. This approach dramatically reduces the number of trainable parameters to often less than 1% of the total, making fine-tuning feasible on hardware. At the core of LoRA's efficiency is its mathematical insight about weight updates. It first decomposes weight updates into products of two smaller matrices by leveraging the observation that during fine-tuning, the weight updates often have a low \"intrinsic rank\" - meaning they can be approximated by low-rank matrices without significant loss of information. For example, in a transformer model where a weight matrix W might be `768×768` (containing `589,824` parameters), LoRA replaces the full update with two matrices B `(768×16)` and A `(16×768)`, requiring only `24,576` parameters - a 96% reduction. These matrices are initialized with careful scaling: B starts with random Gaussian values while A begins at zero, ensuring training begins from the original model's behavior. The implementation in the code uses `r=16` for the rank hyperparameter, which determines this compression ratio. The `lora_alpha=32` parameter controls scaling during inference, effectively determining how strongly the adaptation affects the original weights. The `target_modules=[\"q_proj\", \"v_proj\"]` parameter specifically targets the query and value projection matrices in the attention mechanism, which are particularly influential for language understanding and generation while leaving other components untouched.\n",
    "\n",
    "The train method handles the actual training process by configuring optimization parameters including learning rate, batch size, and gradient accumulation steps. It sets up a training pipeline with appropriate arguments for supervised learning, including warmup schedules and weight decay for regularization. After training completes, it saves just the LoRA adapter rather than the full model, making the fine-tuned version extremely portable at just a fraction of the full model size. Finally, the evaluation method provides a convenient way to compare the base model against the fine-tuned version using the same prompts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Initialize base model\n",
    "    base_llm = PretrainedLLM(model_name=\"facebook/opt-350m\")\n",
    "    \n",
    "    # Create SFT trainer\n",
    "    sft = SupervisedFineTuner(base_llm, dataset_name=\"databricks/dolly-15k\")\n",
    "    \n",
    "    # Prepare data\n",
    "    processed_data = sft.prepare_data()\n",
    "    \n",
    "    # Setup PEFT\n",
    "    peft_model = sft.setup_peft()\n",
    "    \n",
    "    # Train the model\n",
    "    adapter_path = sft.train(output_dir=\"sft_model\", num_epochs=1)  # Reduced for demo\n",
    "    \n",
    "    # Load the adapter into the base model\n",
    "    base_llm.load_adapter(adapter_path)\n",
    "    \n",
    "    # Test the model\n",
    "    evaluation_prompts = [\n",
    "        \"Explain quantum computing to a 10-year-old:\",\n",
    "        \"Write a short story about a robot learning to feel emotions:\",\n",
    "        \"How do I bake a chocolate cake?\"\n",
    "    ]\n",
    "    \n",
    "    sft.evaluate(evaluation_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23542e",
   "metadata": {},
   "source": [
    "EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ea9ed",
   "metadata": {},
   "source": [
    "and there we go! we have a successfully supervised fine tuned llm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a89dba",
   "metadata": {},
   "source": [
    "## 3. Reinforcement Learning with Human Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dc63b",
   "metadata": {},
   "source": [
    "#### Understand the relation between LLMs and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3fd7b",
   "metadata": {},
   "source": [
    "Before diving into the human feedback mechanisms, it's crucial to understand how Reinforcement Learning fundamentals apply to language models. This connection isn't immediately obvious, as LLMs seem quite different from traditional RL scenarios like game-playing agents or robotic control systems.\n",
    "\n",
    "Reinforcement Learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. The core components of any RL system include:\n",
    "\n",
    "- **Agent**: The decision-maker (our language model) that observes the environment and takes actions\n",
    "- **Environment**: The context in which the agent operates (the conversation or text generation task)\n",
    "- **State (s)**: The current situation or context that the agent observes (the prompt and any previously generated text)\n",
    "- **Action (a)**: The decisions the agent can make (choosing the next token/word to generate)\n",
    "- **Policy (π)**: The strategy that maps states to actions (how the model decides what token to generate next)\n",
    "- **Reward (r)**: The feedback signal indicating how good an action was (human preference scores or task-specific metrics)\n",
    "- **Value Function (V)**: Estimates the expected cumulative reward from a given state (how \"good\" a particular context is)\n",
    "- **Trajectory**: A sequence of states, actions, and rewards over time (the complete generation process from prompt to final response)\n",
    "\n",
    "Let's return to our quantum computing explanation scenario to see how these RL concepts apply to language models. Imagine our model is explaining quantum computing to a 10-year-old:\n",
    "\n",
    "- **Environment**: The conversation context where a child has asked \"Explain quantum computing to a 10-year-old\"\n",
    "- **Initial State**: The prompt \"Explain quantum computing to a 10-year-old:\" (this is what the model \"observes\")\n",
    "- **Agent**: Our language model that must generate an appropriate response\n",
    "- **Action Space**: The model's vocabulary (approximately 50,000 possible tokens it could choose from at each step)\n",
    "- **Policy**: The model's current strategy for choosing words - initially just next-token prediction, later optimized for human preferences\n",
    "\n",
    "At each generation step, the model is in a specific state (the prompt plus all previously generated tokens) and must choose an action (the next token). For example:\n",
    "\n",
    "- **State 1**: \"Explain quantum computing to a 10-year-old:\"\n",
    "- **Action 1**: Choose token \"Hey\" (starting conversationally)\n",
    "- **State 2**: \"Explain quantum computing to a 10-year-old: Hey\"\n",
    "- **Action 2**: Choose token \"there!\" (continuing the friendly tone)\n",
    "- **State 3**: \"Explain quantum computing to a 10-year-old: Hey there!\"\n",
    "- **Action 3**: Choose token \"Imagine\" (starting an analogy)\n",
    "\n",
    "And so forth, until the model generates an end-of-sequence token, completing the trajectory.\n",
    "\n",
    "\n",
    "The genius of applying RL to language models lies in reframing text generation as a sequential decision-making process. Instead of just predicting the statistically most likely next token (as in standard pretraining), we can now optimize for much more sophisticated objectives:\n",
    "\n",
    "**Traditional Approach**: \"What word typically comes next in internet text?\"\n",
    "\n",
    "**RL Approach**: \"What word choice will lead to a response that humans find helpful, harmless, and honest?\"\n",
    "\n",
    "In our quantum explanation example, the traditional model might continue with technical terms because they frequently appear after \"quantum computing\" in training data. But an RL-optimized model learns that choosing words like \"imagine\" or \"think of it like\" leads to better human ratings for child-appropriate explanations.\n",
    "\n",
    "**State Representation in Language Models**: Unlike board games where the state is clearly defined (piece positions), in language models the state is the entire context window - the prompt plus all previously generated tokens. This creates a vast and complex state space where the same prompt can lead to exponentially many possible conversation paths.\n",
    "\n",
    "**Action Space Complexity**: At each step, the model chooses from thousands of possible tokens. The sequence of these choices determines whether we get a technical academic explanation or a child-friendly analogy with magic coins and spinning objects.\n",
    "\n",
    "**Reward Signal**: This is where human feedback becomes crucial. Instead of immediate rewards (like points in a game), language model rewards typically come at the end of generation when humans evaluate the complete response. This creates what's called a \"sparse reward\" problem - the model must learn which early token choices led to the eventual positive feedback.\n",
    "\n",
    "**Policy Evolution**: The model's policy starts as simple next-token prediction but evolves through RLHF to incorporate human preferences. It learns that certain patterns (like using analogies, asking engaging questions, maintaining appropriate tone) consistently lead to higher rewards.\n",
    "\n",
    "This reframing transforms language model training from pattern matching to goal-oriented behavior, enabling models to optimize for nuanced human values rather than just statistical likelihood. The subsequent RLHF stages (preference collection, reward modeling, and policy optimization) all build on this fundamental RL foundation to create AI systems that are not just knowledgeable, but genuinely helpful and aligned with human intentions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce026f20",
   "metadata": {},
   "source": [
    "### Human Preference Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b4b3e",
   "metadata": {},
   "source": [
    "#### But Why Do We Even Need Human Preference Collection?\n",
    "\n",
    "You might wonder: \"We already have an SFT model that can explain quantum computing nicely to children. Why do we need this additional step?\" This is a fundamental question that gets to the heart of what makes RLHF so powerful.\n",
    "\n",
    "The limitation of SFT becomes clear when we consider what it actually teaches the model. SFT is essentially sophisticated pattern matching - it learns to mimic the style and structure of the demonstration data, but it doesn't develop a deeper understanding of *why* certain responses are better than others. It's like teaching someone to paint by having them copy masterpieces stroke by stroke - they might reproduce the paintings accurately, but they haven't learned the principles that make great art.\n",
    "\n",
    "Consider our quantum computing example. An SFT model might learn that responses for children should use simple language and analogies, but it lacks nuanced understanding of what makes one analogy better than another. It might consistently use the \"spinning coin\" analogy because it appeared in training data, but it won't know that sometimes a \"magic treasure hunt\" analogy might be more engaging for a particular child, or that asking follow-up questions creates better learning experiences.\n",
    "\n",
    "Human Preference Collection addresses these limitations by teaching the model to understand *relative quality* - not just how to generate appropriate responses, but how to distinguish between good, better, and best responses. This comparative learning is fundamentally different from demonstration learning and unlocks much more sophisticated behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883ebdc",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow3.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ed1ea",
   "metadata": {},
   "source": [
    "\n",
    "Human Preference Collection is essentially a massive, systematic comparison exercise where human evaluators help the model learn what \"better\" means in countless different scenarios. Here's how it works in practice:\n",
    "\n",
    "#### Step 1: Response Generation\n",
    "First, we use our SFT model to generate multiple responses to the same prompt. For our quantum computing example, we might generate several different explanations:\n",
    "\n",
    "**Prompt**: \"Explain quantum computing to a 10-year-old\"\n",
    "\n",
    "**Response A**: \"Quantum computers use qubits instead of regular bits. Think of regular bits like light switches that can only be on or off. Qubits are special because they can be on and off at the same time, like a spinning coin that's both heads and tails until it stops spinning.\"\n",
    "\n",
    "**Response B**: \"Hey there! Imagine you have a magic coin that can be heads and tails at the same time while it's spinning! Quantum computers use these special 'qubits' that work just like our magic coin. This lets them solve puzzles much faster than regular computers. What's your favorite kind of puzzle?\"\n",
    "\n",
    "**Response C**: \"Quantum computing is like having a super-fast helper that can check every room in your house for your lost toy at the same time, instead of checking one room after another like a regular computer would do.\"\n",
    "\n",
    "#### Step 2: Human Evaluation\n",
    "Human evaluators (often experts in education, communication, or the relevant domain) are presented with pairs of these responses and asked to choose which one better fulfills the criteria. The evaluation interface might look like:\n",
    "\n",
    "```\n",
    "Prompt: Explain quantum computing to a 10-year-old\n",
    "\n",
    "Response A: [Response A text]\n",
    "Response B: [Response B text]\n",
    "\n",
    "Which response better explains quantum computing to a 10-year-old?\n",
    "□ Response A is significantly better\n",
    "□ Response A is slightly better  \n",
    "□ Response B is slightly better\n",
    "□ Response B is significantly better\n",
    "□ They're about the same quality\n",
    "```\n",
    "\n",
    "The evaluators consider multiple factors:\n",
    "- **Age-appropriateness**: Does it use language a 10-year-old would understand?\n",
    "- **Engagement**: Would this capture and maintain a child's interest?\n",
    "- **Accuracy**: Is the explanation scientifically sound (even if simplified)?\n",
    "- **Clarity**: Would a child actually understand this after reading it?\n",
    "- **Interactivity**: Does it encourage questions or further learning?\n",
    "\n",
    "#### Step 3: Preference Data Creation\n",
    "Each comparison creates a preference data point. If evaluators consistently prefer Response B over Response A, this creates a training signal that Response B's approach (conversational tone, asking questions, using engaging analogies) should be valued more highly than Response A's approach (more technical, less interactive).\n",
    "\n",
    "\n",
    "Let's extend our quantum computing analogy to understand preference collection itself:\n",
    "\n",
    "Imagine you're teaching a young student not just about quantum computers, but about how to *evaluate* different explanations of quantum computers. Instead of just showing them one \"correct\" way to explain it (like SFT), you show them pairs of explanations and ask: \"Which of these would help you understand better?\"\n",
    "\n",
    "**Comparison 1**: Technical explanation vs. Magic coin analogy\n",
    "→ Student prefers: Magic coin analogy\n",
    "\n",
    "**Comparison 2**: Magic coin analogy vs. Treasure hunt analogy  \n",
    "→ Student prefers: Treasure hunt analogy (more engaging)\n",
    "\n",
    "**Comparison 3**: Treasure hunt analogy vs. Interactive treasure hunt with questions\n",
    "→ Student prefers: Interactive version (encourages participation)\n",
    "\n",
    "Through hundreds of these comparisons, the student (our model) learns not just individual explanations, but the *principles* that make explanations effective: engagement beats technicality, interactivity beats passivity, relatable analogies beat abstract concepts.\n",
    "\n",
    "\n",
    "In real RLHF implementations, this process happens at massive scale:\n",
    "\n",
    "- **Thousands of prompts** across diverse topics and scenarios\n",
    "- **Multiple responses** generated for each prompt (typically 2-4)\n",
    "- **Multiple evaluators** rating each comparison (to ensure reliability)\n",
    "- **Hundreds of thousands** of preference comparisons collected\n",
    "- **Quality control** measures to ensure consistent evaluation standards\n",
    "\n",
    "For our quantum explanation example, the preference data might reveal patterns like:\n",
    "- Responses with questions score higher than statements\n",
    "- Analogies to familiar objects (coins, toys) beat abstract concepts\n",
    "- Conversational tone (\"Hey there!\") beats formal tone\n",
    "- Explanations that acknowledge the topic's complexity score higher than oversimplifications\n",
    "\n",
    "#### Why This Works Better Than More SFT Data\n",
    "\n",
    "You might think: \"Why not just collect more demonstration data instead?\" The key insight is that preference collection captures information that demonstration data cannot:\n",
    "\n",
    "**Demonstration data tells us**: \"This is a good response\"\n",
    "\n",
    "**Preference data tells us**: \"This response is better than that response *because*...\"\n",
    "\n",
    "The \"because\" part is crucial. Through comparative evaluation, the model learns the underlying principles that make responses effective, not just specific examples of effective responses. This enables much better generalization to new scenarios and more nuanced quality judgments.\n",
    "\n",
    "In our quantum computing scenario, instead of learning \"use the magic coin analogy,\" the model learns \"use analogies that relate to a child's everyday experience, and frame them in an engaging, interactive way.\" This principle can then be applied to explaining any complex topic to children, not just quantum computing.\n",
    "\n",
    "This preference data becomes the foundation for the next stage: training a reward model that can automatically evaluate response quality and guide the final optimization process. The human preferences, collected at scale, teach the AI system not just what to say, but what makes some ways of saying things better than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd636c7b",
   "metadata": {},
   "source": [
    "### Reward Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f9112",
   "metadata": {},
   "source": [
    "<img src=\"assets/workflow4.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3115b",
   "metadata": {},
   "source": [
    "dive into reward model and how they play a role in this why is it needed and how it works, if possible also dive into math of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576dea8c",
   "metadata": {},
   "source": [
    "### Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a308803",
   "metadata": {},
   "source": [
    "#### Why these algorithms were needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06588d",
   "metadata": {},
   "source": [
    "start with basic policy optimization introduciton and about, how gradient policy optimzation works generally and state its limitations briefly, off policy and on policy learning intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b0ede",
   "metadata": {},
   "source": [
    "##### Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5afbf",
   "metadata": {},
   "source": [
    "<img src=\"assets/trpo.png\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316d5c6",
   "metadata": {},
   "source": [
    "Now in this one, explain a lot, what it is, use our quantum 5 year old analogies how it works laydown the components and how it was groundbreaking, dive into the math side of it a lot we wont be doing code here since we're mainly focusing on PPO and DPO for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29c2a2",
   "metadata": {},
   "source": [
    "In policy gradient algorithms, update steps computed at any specific policy $\\pi_{\\theta}$ are only really \"predictive\" in the neighborhood of $\\theta_t$. That is, it is probable that updates outside of this neighborhood may not contain any predictive value at all. Intuitively, you may then think of constraining updates so that they do stay in the vicinity of our current policy.\n",
    "\n",
    "Since the policy is a probability distribution over (state, action) pairs, we refer to it as the \"policy space\". Trust region methods (Kakade, 2001; Schulman et al., 2015a; 2017) aim to restrict how far successive policies are allowed to deviate. \n",
    "\n",
    "In Trust Region Policy Optimization(Schulman et al., 2015a), this is achieved by minimizing the KL divergence between successive policies on the optimization trajectory. Let $\\hat{A_{\\pi}}(s_t, a_t)$ be the estimated advantage function where the agent picks action $a$ at time $t$ given he is at state $s$ while following a policy $\\pi_{\\theta}$. Let $\\pi(a_t|s_t)$ be the old policy where we take action $a$ given we are in state $s$ at time $t$. $\\pi_{\\theta}(a_t|s_t)$ is our current (parameterized) policy which we seek to update.\n",
    "\n",
    "Define the optimization problem as:\n",
    "$$\\underset{\\theta}{\\max}\\ \\mathbb{E}_{(s_t, a_t)\\sim \\pi} \\left[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi(a_t|s_t)}\\hat{A_{\\pi}}(s_t, a_t)\\right]\\\\ \\\\\n",
    "\\text{subject to } D_{KL}(\\pi_{\\theta}(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))\\leq\\delta, \\ \\forall s\n",
    "$$\n",
    "The \"subject to\" line is essentially an assumption that we will have to improve. It is saying that our optimization problem will abide by the goal of having the KL divergence between the two policies becoming less than some small $\\delta$.\n",
    "\n",
    "This theoretical update is not easy to compute. Thus, TRPO makes approximations by reformulating both the loss function $\\mathcal{L}(\\theta_k, \\theta)$ and the KL divergence $D_KL(\\theta||\\theta_k)$ to give an easier-to-compute approximation of the objective. Furthermore, this approximate objective is then able to be solved using Lagrangian duality to yield the update:\n",
    "$$\\theta_{k+1}=\\theta_k+\\alpha^j\\sqrt{\\frac{2\\delta}{g^TH^{-1}g}}H^{-1}g$$\n",
    "Since the Hessian inverse $H^{-1}$ is expensive to compute, TRPO utilizes the conjugate gradient algorithm to solve $Hx=g$ (or $x=H^{-1}g$) which requires a function for computing $Hx$ instead of computing and storing the entire matrix $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bd4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0700002b",
   "metadata": {},
   "source": [
    "##### Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8976cc0",
   "metadata": {},
   "source": [
    "explain what PPO is how it was better than TRPOm define its components, explain it with the math and how it works one by one indepth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769240b",
   "metadata": {},
   "source": [
    "<img src=\"assets/ppo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb4727",
   "metadata": {},
   "source": [
    "Let's start off by making a simple `PPOModel` class such that we can perform a `forward()` pass. \n",
    "\n",
    "In PPO, we refer to the policy model as the \"actor\" and the value model as the \"critic\".\n",
    "\n",
    "The value model outputs scalar values (scores) just like the reward model. Meanwhile the policy model outputs probability distributions (take the log and you get logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be87083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "from transformers import Trainer\n",
    "\n",
    "class PPOModel(nn.Module):\n",
    "    def __init__(self, actor_model, critic_model):\n",
    "        super().__init__()\n",
    "        self.actor_model = actor_model\n",
    "        self.critic_model = critic_model\n",
    "\n",
    "    def forward(self, sequences, extra_inputs=None):\n",
    "        # fetch logits from actor, fetch scalar reward from reference\n",
    "        actor_logits = self.actor_model(**sequences, return_dict=True).logits\n",
    "        critic_values = self.critic_model(**sequences)[-1]\n",
    "        \n",
    "        if extra_inputs is not None:\n",
    "            extra_loss = self.actor_model(**extra_inputs, return_dict=True).loss\n",
    "        else:\n",
    "            extra_loss = 0.0\n",
    "        return actor_logits, critic_values, extra_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3fd11",
   "metadata": {},
   "source": [
    "Now, navigate to the `src/ppo` folder to find `ppo_trainer.py`. This will contain the main code for our PPO algorithm.\n",
    "\n",
    "We will now go over each component of the `PPOTrainer` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b591fd6",
   "metadata": {},
   "source": [
    "The KL divergence cannot be computed in its original form. \n",
    "\n",
    "The action space (token space) is far too vast for us to sum/integrate over all $x$.\n",
    "Furthermore, when we train, we don't store full probability distributions but rather log-probabilities of the tokens.\n",
    "Thus, it doesn't make sense to waste GPU memory on something we can avoid. \n",
    "So, it is necessary to estimate it (with Monte Carlo, for example). However, we have a plethora of KL estimators to choose from.\n",
    "\n",
    "Choosing the most \"optimal\" estimator is out of the scope of this notebook. We only implement the most popular estimators which have been tried and tested. \n",
    "\n",
    "Suppose $r=\\frac{\\pi (\\theta)}{\\pi_{\\text{old}}(\\theta)}$ is the ratio between the current policy and the old policy.\n",
    "Let $k$ denote an ambiguous KL estimator. Then, we have the following:\n",
    "\n",
    "$$k_3 = (r - 1) - \\log r$$\n",
    "$$k_{\\text{abs}}=|\\log r|$$\n",
    "$$k_{\\text{MSE}}=\\frac{1}{2}(\\log r)^2$$\n",
    "\n",
    "This might be up for debate, but the best KL estimator is $k_{\\text{MSE}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards_with_kl_penalty(self, ref_values, actor_log_probs, ref_log_probs, responses_mask):\n",
    "    \"\"\"\n",
    "    Computes rewards with the KL divergence penalty.\n",
    "    Includes implementations of KL estimators since we can't compute it exactly.\n",
    "    - k_3 is a popular KL estimator proposed here: http://joschu.net/blog/kl-approx.html\n",
    "\n",
    "    Args:\n",
    "        ref_values\n",
    "\n",
    "        actor_log_probs: torch.Tensor\n",
    "            log probabilities from our actor model\n",
    "\n",
    "        ref_log_probs: torch.Tensor\n",
    "            log probabilities from our reference model\n",
    "\n",
    "        responses_mask: torch.Tensor\n",
    "    \"\"\"\n",
    "    masks = responses_mask[:, 1:] \n",
    "    rewards_score = self.get_last_reward_score(ref_values, responses_mask)\n",
    "    \n",
    "    batch_size = rewards_score.shape[0]\n",
    "    rewards_with_kl_penalty, kl_penalty_all = [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        mask = masks[i]\n",
    "        lp_a = actor_log_probs[i][mask] # masked actor logprobs\n",
    "        lp_r = ref_log_probs[i][mask] # masked reference logprobs\n",
    "\n",
    "\n",
    "        # in my equations below, r is simply the ratio: pi(y) / pi_ref(y)\n",
    "        if self.args.kl_penalty_method == 'k_3': # equation: (r - 1) - log r\n",
    "            lp_diff = lp_a - lp_r\n",
    "            ratio = torch.exp(lp_diff)\n",
    "            kl_est = (ratio - 1.0) - lp_diff\n",
    "\n",
    "        elif self.args.kl_penalty_method == 'abs': # equation: |log r|\n",
    "            kl_est = torch.abs(lp_a - lp_r)\n",
    "\n",
    "        elif self.args.kl_penalty_method == 'mse': # equation: 1/2 * (log r)^2\n",
    "            kl_est = 0.5 * (lp_a - lp_r) ** 2 \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kl_penalty_method: {self.args.kl_penalty_method}\")\n",
    "\n",
    "            \n",
    "        kl_penalty = - self.args.kl_penalty_beta * kl_est\n",
    "        kl_penalty_all.append(kl_penalty)\n",
    "\n",
    "        if self.args.reward_score_clip is not None:\n",
    "            rewards_score[i] = torch.clamp(rewards_score[i], -self.args.reward_score_clip, self.args.reward_score_clip)\n",
    "        \n",
    "        end_index = mask.nonzero()[-1].detach().item()\n",
    "        kl_penalty[end_index] += rewards_score[i]\n",
    "\n",
    "        rewards_with_kl_penalty.append(kl_penalty)\n",
    "    return torch.stack(rewards_with_kl_penalty), torch.stack(kl_penalty_all), rewards_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cf3f9",
   "metadata": {},
   "source": [
    "Now, we cannot compute the advantage function $A_t$ exactly, so we estimate it. \n",
    "\n",
    "The algorithm used in PPO to compute the estimate of the advantage $\\hat{A}_t$ is Generalized Advantage Estimation (GAE).\n",
    "One key thing here is that we do not just naively estimate $A_t$; we have to do it carefully to mitigate a large variance.\n",
    "\n",
    "If $T$ is our trajectory length, then GAE computes this estimate by:\n",
    "\n",
    "$$\\hat{A}_t=\\sum^{T-t-1}_{l=0}(\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\gamma$ is the discount factor, $\\lambda\\in[0,1]$ is the GAE parameter, and $\\delta_t=R(s_t,a_t)+\\gamma V(s_{t+1})-V(s_t)$ which is the Temporal-Difference (TD) error. Here, $R(s_t, a_t)$ is the reward at time $t$, and $V(s_t)$ is the value function at time $t$. \n",
    "\n",
    "Variance reduction is a common theme in reinforcement learning as long-horizon estimates are prone to deviating from our expected value.\n",
    "What GAE does is it performs multi-step \"bootstrapping\" to reduce the variance. Bootstrapping is a term which refers to updating an estimate value using (one or more) of the **same kind of estimate values**.\n",
    "\n",
    "Without bootstrapping, our variance would explode, leading to longer episode trajectories (which we don't want). We want to keep these trajectories as short as possible; that is, to converge as fast as possible. In GAE, this bootstrapping can be seen in the TD error $\\delta_t=R(s_t,a_t)+\\gamma V(s_{t+1})-V(s_t)$.\n",
    "\n",
    "However, we also need to consider bias. Remember the bias-variance tradeoff? Here, as we progress in the trajectory, our estimates accumulate positive bias at each step. This is the price to be paid for variance reduction at each step. There are proposed methods (such as VAPO https://arxiv.org/pdf/2504.05118) which aim to achieve both low variance while mitigating high bias. This is a bit more advanced, though.\n",
    "\n",
    "In the context of applying this to language models, $T$ can be viewed as the maximum sequence length of a model's output. In GAE, the $t$-th step would be the $t$-th token that is currently being sampled from our model. The difference $T-t$ is just how far away this token is from the end of the sentence (EOS). The only nonzero reward $R(s_T)$ is calculated at the `<EOS>` token. Thus, all prior tokens are just propagating it backwards (with discounting). \n",
    "\n",
    "Given this discounting, as we increase the difference $T-t=2, T-t=3,..., T-t=k$, the reward signal grows weaker. Eventually, the last token to obtain this initial reward might obtain a zero value! Yikes. Thankfully, there are methods which attempt to address this decaying reward problem (such as VC-PPO https://arxiv.org/pdf/2503.01491)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f87186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae_advantage_return(self, rewards, values, mask, gamma, lam):\n",
    "    \"\"\"\n",
    "    Computes the Generalized Advantage Estimation via Temporal-Difference with parameter lambda.\n",
    "\n",
    "    Args:\n",
    "        rewards: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        mask: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        gamma: float\n",
    "            discount factor\n",
    "        lam: float\n",
    "            lambda parameter for GAE algorithm\n",
    "    \"\"\"\n",
    "    B, T = rewards.shape # B is batch size, T is response_length\n",
    "\n",
    "    # here, we bootstrap the value model updates with Temporal-Difference (parameter \\lambda)\n",
    "    with torch.no_grad():\n",
    "        advantages_reversed = []\n",
    "        lastgaelam = 0\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            # for long sequences with T - t >> 1, discounting reduces the reward signal to near zero\n",
    "            next_values = values[:, t + 1] if t < T - 1 else 0.0\n",
    "            delta = rewards[:, t] + gamma * next_values - values[:, t]\n",
    "            lastgaelam = (delta + gamma * lam * lastgaelam) * mask[:, t] \n",
    "            advantages_reversed.append(lastgaelam)\n",
    "        advantages = torch.stack(advantages_reversed[::-1], dim=1)\n",
    "\n",
    "        returns = advantages + values\n",
    "        # \n",
    "        if use_advantage_norm:\n",
    "            advantages = masked_whiten(advantages, eos_mask)\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf330d7",
   "metadata": {},
   "source": [
    "Implementing PPO (which is an Actor-Critic algorithm) requires a lot of design choices, where you can easily lose yourself trying to read them all. However, the topmost choice deals with the actor and critic themselves. We have two choices: \n",
    "\n",
    "1. Learn two largely separate models. One for the actor and another for the critic.\n",
    "2. Learn one large model, and apply two shallow heads at the end to format our output (one for the actor, another for the critic)\n",
    "\n",
    "The first approach is usually more popular because the singular model we learn is usually a deep transformer which contains rich, hidden representations. As such, our two heads only need to be shallow because they serve as \"mappings\" from our representations to scalar values. The large transformer will have already done the grunt work during its intense training runs (and perhaps, any additional supervised fine-tuning).\n",
    "\n",
    "In usual implementations of PPO (which is an Actor-Critic algorithm), we train a single large model with two heads on top. One is the actor (policy) head and the other is the critic (value) head.\n",
    "We optimize this single model by minimizing a three-part loss function. \n",
    "\n",
    "That is, we add the $L^{CLIP}$ clipped surrogate loss as proposed in the original PPO paper. Next, we subtract an MSE error loss, $L^{VF}$ for computing the value function. Finally, we add the entropy of the policy, $H(\\pi_{\\theta}(\\cdot | s))$ which is multiplied by some constant term, $c_1$. Usually, we also multiply $L^{VF}$ by a constant $c_2$ as well. So we have:\n",
    "$$L=L^{CLIP}-c_2 L^{VF} + c_1 H(\\pi_{\\theta}(\\cdot | s))$$\n",
    "\n",
    "In the following sections, we will go over each term and their theoretical significances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b855d64",
   "metadata": {},
   "source": [
    "# *=================================BREAK================================*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747d43d",
   "metadata": {},
   "source": [
    "The actor has an entropy term which is effectively a regularizer that promotes exploration. Entropy in our case is (very loosely) a measure of our uncertainty in the policy. As such, it can tell us how confident a policy is at choosing an action given that they are in some state. When this entropy $H(\\pi)$ is low, we have a high confidence, whereas a high $H(\\pi)$ implies low confidence. \n",
    "\n",
    "We want our policy to diversify itself instead of picking the \"best choice\" each time. Thus, if we want to push the policy to \"explore more\", we need to keep some amount of uncertainty. We would effectively be \"spreading\" the probability distribution over multiple actions rather than letting a high confidence confine it to only a few actions. So, natually, we'd want to maximize the entropy $H(\\pi)$. \n",
    "\n",
    "Another thing is that we don't always know how to compute $H(\\pi)$ exactly, so we estimate it by taking the log-probability of our policy and averaging over the batch. If $\\mathcal{B}$ is our batch of state, action pairs, then we have:\n",
    "$$H(\\pi)\\approx -\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} (-\\log \\pi(a|s))=\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} (\\log \\pi(a|s))$$\n",
    "Here, the negative in the front is by the definition of entropy $H(x)=-\\sum_x p(x)\\log(p(x))$, but since we are estimating it we can omit the $p(x)$ term so we have something like $H(x)\\approx -\\sum_x \\log(p(x))$. The negative in front of $\\log \\pi(a|s)$ is because we are collecting negative log-probabilities. It is usually easier to compute log-probabilities than the full probabilities, hence this estimation. \n",
    "\n",
    "So, our entropy $H(\\pi)$ comes out as positive. Furthermore, we add a constant $c_1$ which we can tweak to regularize the effect of $H(\\pi)$ on the overall loss $L$.\n",
    "$$c_1 H(\\pi(\\cdot|s))$$\n",
    "However, we can afford computing the entropy in its entire form (which is what the code does below):\n",
    "$$H(\\pi)= -\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} (-\\pi(a|s)\\log \\pi(a|s))=\\frac{1}{\\mathcal{|B|}}\\sum_{a,s\\in\\mathcal{B}} \\pi(a|s)\\log \\pi(a|s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(self, logits, mask):\n",
    "        \"\"\"\n",
    "        Computes the entropy of the policy, which incentivizes exploration.\n",
    "\n",
    "        Args:\n",
    "            logits: torch.Tensor\n",
    "                unnormalized log-probabilities from a model\n",
    "            mask: torch.Tensor\n",
    "                mask to be applied to the computed entropy\n",
    "        \"\"\"\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        entropy = self.masked_mean(-torch.sum(probs * log_probs, dim=-1), mask)\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c259d1",
   "metadata": {},
   "source": [
    "The PPO-Clip surrogate objective is defined as:\n",
    "$$ L^{\\text{CLIP}} = \\min\\left(\\frac{\\pi(\\theta)}{\\pi_{\\text{old}}(\\theta)}A_t, \\text{clip}\\left( \\frac{\\pi(\\theta)}{\\pi_{\\text{old}}(\\theta)} , 1-\\epsilon, 1+\\epsilon \\right)A_t \\right) $$\n",
    "\n",
    "Notice how we do not have a `min` operation here? Notice how we actually use `torch.max()`? That seems counterintuitive. However, further note that we added a negative sign to `loss1` and `loss2`, before taking their `max`. \n",
    "\n",
    "This is effectively equivalent to the `min()` operation but done in reverse to ensure numerical stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_loss(self, old_log_prob, log_prob, advantages, eos_mask, epsilon):\n",
    "        \"\"\"\n",
    "        Computes the policy gradient loss function for PPO.\n",
    "\n",
    "        Args:\n",
    "            old_log_prob: torch.Tensor\n",
    "                log probabilities from the old policy\n",
    "            log_prob: torch.Tensor\n",
    "                log probabilities from the current policy \n",
    "            advantages: torch.Tensor\n",
    "                Computed advantages via advantage estimation\n",
    "            eos_mask: torch.Tensor\n",
    "            \n",
    "            epsilon: float\n",
    "        \"\"\"\n",
    "\n",
    "        # from log domain -> real domain\n",
    "        ratio = torch.exp(log_prob - old_log_prob)\n",
    "        loss1 = -advantages * ratio\n",
    "        loss2 = -advantages * torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "\n",
    "        loss = masked_mean(torch.max(loss1, loss2), eos_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b898b7",
   "metadata": {},
   "source": [
    "As detailed in PPO-Clip algorithm, we compute the value function by regressing on an MSE loss. \n",
    "\n",
    "For some reason this is not explicitly mentioned in the original PPO paper. It is, however, mentioned in OpenAI's Spinning Up documentation entry on PPO. For the answer, we need to do some digging back to the old Barto & Sutton book (you can not escape RL!!!)\n",
    "\n",
    "Recall that the on-policy value function is defined as the expected discounted return that the agent can get given he starts in some state $s$ and acts according to the policy $\\pi$:\n",
    "$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\cdots |S_t=s]$$\n",
    "Now, define the discounted return as $G_t=\\sum_{t'=t}^T \\gamma^{t'-t}r_{t'}$. If we reindex with $k=t'-t$ then this becomes $G_t=\\sum_{k=0}^T \\gamma^{k}r_{t}$.\n",
    "So, the value function becomes:\n",
    "$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]=\\mathbb{E}_{\\pi}[\\sum_{k=0}^T \\gamma^{k}r_{t}|S_t=s]$$\n",
    "which makes a lot more sense now! We see that $v_{\\pi}(s)$ is just the expected discounted return! \n",
    "\n",
    "\n",
    "It is found in Chapter 9.2 of the book. Here, the \"natural objective function\" for value-based algorithms is to minimize the MSE between the true value and observed returns.\n",
    "That is, our value error is defined as:\n",
    "$$\\overline{VE}(\\mathbf{w})=\\mathbb{E}[v_{\\pi}(s)-\\hat{v}(s,\\mathbf{w})]^2$$\n",
    "where the expectation $\\mathbb{E}[\\cdot]$ is taken over the state distribution $\\mu(s)$ such that $\\mu(s)\\geq 0$ and $\\sum_s \\mu(s)=1$.\n",
    "\n",
    "Dissecting this equation, we can see that the expected discounted return $v_{\\pi}(s)$ can be interpreted as the empirical return $G_t$. However, this is not the exactly the $G_t$ we defined earlier. In practice, we do not compute the full return, so we usually use something like a Monte Carlo estimate. It can be shown that this Monte Carlo estimate is an unbiased sample of $v_{\\pi}(s)$, so they are not quite equal (but we can get close enough).\n",
    "\n",
    "Next, $\\hat{v}(s, \\mathbf{w})$ is defined as a joint distribution with states $s$ and weights $\\mathbf{w}$ (which we learn). This is effectively our learned value function, $V(s_t)$ which can be computed using a multi-layer neural network. \n",
    "\n",
    "Thus, in our PPO-Clip algorithm we have the MSE loss:\n",
    "$$L_v = \\sum_{t=1}^T \\frac{1}{2}(G_t - V(S_t))^2$$\n",
    "which tells us how far our observed returns (value estimates), $V(s_t)$, deviate from the \"true\" empirical returns, $G_t$.\n",
    "\n",
    "For a further dissection of On-Policy Prediction with Approximation, I recommend reading Chapter 9 of Sutton & Barto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_loss(self, value_preds, returns, values, eos_mask, epsilon):\n",
    "    \"\"\"\n",
    "    Fits the value function by regression on the MSE loss. \n",
    "\n",
    "    Args:\n",
    "        value_preds: torch.Tensor\n",
    "            the predictions from our value model\n",
    "        returns: torch.Tensor\n",
    "            shape: (bs, response_length)\n",
    "        values: torch.Tensor\n",
    "            \n",
    "        eos_mask: torch.Tensor\n",
    "\n",
    "        epsilon: torch.Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # this keeps our value predictions within some epsilon-distance\n",
    "    # mostly for numerical stability before performing regression\n",
    "    clip_value_preds = torch.clamp(value_preds, values - epsilon, values + epsilon)   \n",
    "\n",
    "    # thus, we have two errors, but it doesn't matter because we take the maximum (one)\n",
    "    values_error = (value_preds - returns) ** 2\n",
    "    clip_values_error = (clip_value_preds - returns) ** 2\n",
    "    \n",
    "    # this is essentially the inner sum for one trajectory t \\in D_k where D_k is set of trajectories\n",
    "    loss = 0.5 * masked_mean(torch.max(values_error, clip_values_error), eos_mask)\n",
    "    return loss, values_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0650a",
   "metadata": {},
   "source": [
    "Notice that in the above code cell, we called `masked_mean()`? What is this? Well, we're now getting into implementation-specific \"tricks\" associated with PPO. Stuff you might not find in the original literature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e8f16",
   "metadata": {},
   "source": [
    "#### Group Relative Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f06bb",
   "metadata": {},
   "source": [
    "<img src=\"assets/grpo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf95a38",
   "metadata": {},
   "source": [
    "Now in this one, explain a lot, what it is, use our quantum 5 year old analogies how it works laydown the components and how it was groundbreaking, dive into the math side of it a lot we wont be doing code here since we're mainly focusing on PPO and DPO for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe022c1",
   "metadata": {},
   "source": [
    "#### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2998b",
   "metadata": {},
   "source": [
    "<img src=\"assets/dpo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325c6c4",
   "metadata": {},
   "source": [
    "Now in this one, explain a lot, what it is, use our quantum 5 year old analogies how it works laydown the components and how it was groundbreaking, dive into the math side of it a lot, since we're focusing on dpo and ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747b25c",
   "metadata": {},
   "source": [
    "#### (Bonus) Test Time Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3f6d8",
   "metadata": {},
   "source": [
    "Now in this one, explain a lot, what it is, use our quantum 5 year old analogies how it works laydown the components and how it was groundbreaking, dive into the math side of it a lot we wont be doing code here since we're mainly focusing on PPO and DPO for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26218b",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540787ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
